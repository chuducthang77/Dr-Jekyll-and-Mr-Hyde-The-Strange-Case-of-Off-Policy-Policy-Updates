Number of states: 10
Number of actions: 2
Info about the experiment: The chain domain is designed to have multiple states where each state has two actions: Action 1 leads to an immediate reward, while action 2 leads to no reward but a better long-term reward. The optimal policy is always choosing action 2.
Run #1
Iteration 0
Discounting theory with old adv calculation: [0.0082, -0.0082, 0.00772, -0.00772, 0.00674, -0.00674, 0.00475, -0.00475, 0.00075, -0.00075, -0.00734, 0.00734, -0.02368, 0.02368, -0.0567, 0.0567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 0 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0082, -0.0082, 0.00772, -0.00772, 0.00674, -0.00674, 0.00475, -0.00475, 0.00075, -0.00075, -0.00734, 0.00734, -0.02368, 0.02368, -0.0567, 0.0567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 0 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 500
Discounting theory with old adv calculation: [0.0, -0.00907, 0.0003, -0.01015, 0.0017, -0.00549, -0.00159, 0.00194, -0.01242, 0.00874, -0.03023, 0.01454, -0.05408, 0.02096, -0.08466, 0.02873, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.33788, -11.34052, 2.19459, -7.09015, -0.00159, 0.00194, -17.84767, 12.56873, -43.45739, 20.90815, -77.94528, 30.21379, -122.27568, 41.50256, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 1000
Discounting theory with old adv calculation: [0.0, -0.00906, 0.0003, -0.01015, 0.0017, -0.00549, -0.00159, 0.00194, -0.01242, 0.00875, -0.03024, 0.01454, -0.05409, 0.02096, -0.08466, 0.02873, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.33678, -11.33847, 2.19319, -7.08882, -0.00159, 0.00194, -17.85692, 12.57265, -43.47202, 20.91038, -77.96365, 30.21374, -122.29552, 41.4993, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 1500
Discounting theory with old adv calculation: [0.0, -0.00906, 0.0003, -0.01015, 0.0017, -0.00549, -0.00159, 0.00194, -0.01243, 0.00875, -0.03024, 0.01454, -0.0541, 0.02096, -0.08467, 0.02872, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.33569, -11.33642, 2.1918, -7.0875, -0.00159, 0.00194, -17.86615, 12.57655, -43.4866, 20.9126, -77.98194, 30.21369, -122.31526, 41.49605, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 2000
Discounting theory with old adv calculation: [0.0, -0.00906, 0.0003, -0.01015, 0.0017, -0.00549, -0.0016, 0.00195, -0.01243, 0.00875, -0.03025, 0.01454, -0.0541, 0.02096, -0.08467, 0.02872, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.3346, -11.33437, 2.19041, -7.08618, -0.0016, 0.00195, -17.87534, 12.58043, -43.50113, 20.91481, -78.00015, 30.21363, -122.3349, 41.49278, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 2500
Discounting theory with old adv calculation: [0.0, -0.00906, 0.0003, -0.01015, 0.0017, -0.00549, -0.0016, 0.00195, -0.01244, 0.00875, -0.03026, 0.01454, -0.05411, 0.02096, -0.08468, 0.02871, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.33352, -11.33232, 2.18903, -7.08486, -0.0016, 0.00195, -17.88451, 12.5843, -43.5156, 20.917, -78.01829, 30.21356, -122.35445, 41.48952, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 3000
Discounting theory with old adv calculation: [0.0, -0.00906, 0.0003, -0.01015, 0.00169, -0.00549, -0.0016, 0.00195, -0.01244, 0.00875, -0.03027, 0.01454, -0.05412, 0.02095, -0.08468, 0.02871, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.33245, -11.33027, 2.18765, -7.08354, -0.0016, 0.00195, -17.89364, 12.58816, -43.53001, 20.91918, -78.03635, 30.21348, -122.3739, 41.48626, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 3500
Discounting theory with old adv calculation: [0.0, -0.00906, 0.0003, -0.01015, 0.00169, -0.00549, -0.0016, 0.00195, -0.01245, 0.00876, -0.03027, 0.01454, -0.05413, 0.02095, -0.08469, 0.0287, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.33139, -11.32822, 2.18627, -7.08222, -0.0016, 0.00195, -17.90274, 12.59199, -43.54438, 20.92134, -78.05433, 30.21339, -122.39326, 41.48299, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 4000
Discounting theory with old adv calculation: [0.0, -0.00906, 0.0003, -0.01015, 0.00169, -0.00548, -0.00161, 0.00196, -0.01245, 0.00876, -0.03028, 0.01454, -0.05413, 0.02095, -0.08469, 0.0287, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.33033, -11.32617, 2.1849, -7.08089, -0.00161, 0.00196, -17.91181, 12.59582, -43.55868, 20.92349, -78.07224, 30.21329, -122.41252, 41.47972, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 4500
Discounting theory with old adv calculation: [0.0, -0.00906, 0.0003, -0.01015, 0.00169, -0.00548, -0.00161, 0.00196, -0.01246, 0.00876, -0.03029, 0.01455, -0.05414, 0.02095, -0.0847, 0.02869, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.32927, -11.32411, 2.18354, -7.07957, -0.00161, 0.00196, -17.92086, 12.59962, -43.57294, 20.92563, -78.09008, 30.21318, -122.4317, 41.47644, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 5000
Discounting theory with old adv calculation: [0.0, -0.00906, 0.00029, -0.01015, 0.00169, -0.00548, -0.00161, 0.00196, -0.01246, 0.00876, -0.03029, 0.01455, -0.05415, 0.02094, -0.0847, 0.02869, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.32823, -11.32206, 2.18217, -7.07825, -0.00161, 0.00196, -17.92987, 12.60341, -43.58714, 20.92775, -78.10783, 30.21306, -122.45078, 41.47317, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 5500
Discounting theory with old adv calculation: [0.0, -0.00906, 0.00029, -0.01015, 0.00169, -0.00548, -0.00161, 0.00197, -0.01247, 0.00876, -0.0303, 0.01455, -0.05415, 0.02094, -0.08471, 0.02868, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.32719, -11.32, 2.18081, -7.07693, -0.00161, 0.00197, -17.93885, 12.60719, -43.60129, 20.92986, -78.12552, 30.21293, -122.46977, 41.46989, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 6000
Discounting theory with old adv calculation: [0.0, -0.00906, 0.00029, -0.01014, 0.00169, -0.00548, -0.00161, 0.00197, -0.01247, 0.00876, -0.03031, 0.01455, -0.05416, 0.02094, -0.08471, 0.02868, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.32616, -11.31795, 2.17946, -7.07561, -0.00161, 0.00197, -17.94781, 12.61095, -43.61539, 20.93196, -78.14313, 30.2128, -122.48867, 41.46661, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 6500
Discounting theory with old adv calculation: [0.0, -0.00905, 0.00029, -0.01014, 0.00169, -0.00548, -0.00162, 0.00197, -0.01248, 0.00877, -0.03031, 0.01455, -0.05417, 0.02094, -0.08472, 0.02867, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.32513, -11.3159, 2.17811, -7.07429, -0.00162, 0.00197, -17.95674, 12.61469, -43.62944, 20.93404, -78.16067, 30.21265, -122.50748, 41.46333, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 7000
Discounting theory with old adv calculation: [0.0, -0.00905, 0.00029, -0.01014, 0.00169, -0.00548, -0.00162, 0.00197, -0.01248, 0.00877, -0.03032, 0.01455, -0.05417, 0.02094, -0.08472, 0.02867, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.32411, -11.31384, 2.17676, -7.07297, -0.00162, 0.00197, -17.96563, 12.61842, -43.64343, 20.93611, -78.17814, 30.2125, -122.52621, 41.46005, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 7500
Discounting theory with old adv calculation: [0.0, -0.00905, 0.00029, -0.01014, 0.00168, -0.00548, -0.00162, 0.00198, -0.01249, 0.00877, -0.03033, 0.01455, -0.05418, 0.02093, -0.08473, 0.02866, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.32309, -11.31179, 2.17542, -7.07164, -0.00162, 0.00198, -17.9745, 12.62213, -43.65737, 20.93817, -78.19553, 30.21234, -122.54484, 41.45677, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 8000
Discounting theory with old adv calculation: [0.0, -0.00905, 0.00029, -0.01014, 0.00168, -0.00548, -0.00162, 0.00198, -0.01249, 0.00877, -0.03033, 0.01455, -0.05419, 0.02093, -0.08473, 0.02866, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.32208, -11.30973, 2.17408, -7.07032, -0.00162, 0.00198, -17.98334, 12.62583, -43.67126, 20.94022, -78.21286, 30.21217, -122.56339, 41.45348, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 8500
Discounting theory with old adv calculation: [0.0, -0.00905, 0.00029, -0.01014, 0.00168, -0.00547, -0.00163, 0.00198, -0.0125, 0.00877, -0.03034, 0.01455, -0.0542, 0.02093, -0.08474, 0.02865, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.32108, -11.30768, 2.17274, -7.069, -0.00163, 0.00198, -17.99215, 12.62952, -43.68511, 20.94225, -78.23011, 30.21199, -122.58185, 41.4502, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 9000
Discounting theory with old adv calculation: [0.0, -0.00905, 0.00029, -0.01014, 0.00168, -0.00547, -0.00163, 0.00198, -0.0125, 0.00878, -0.03035, 0.01455, -0.0542, 0.02093, -0.08474, 0.02865, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.32008, -11.30562, 2.17141, -7.06768, -0.00163, 0.00198, -18.00094, 12.63319, -43.6989, 20.94427, -78.24729, 30.2118, -122.60022, 41.44691, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 9500
Discounting theory with old adv calculation: [0.0, -0.00905, 0.00029, -0.01014, 0.00168, -0.00547, -0.00163, 0.00199, -0.01251, 0.00878, -0.03036, 0.01455, -0.05421, 0.02093, -0.08475, 0.02864, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.31909, -11.30356, 2.17009, -7.06636, -0.00163, 0.00199, -18.00969, 12.63684, -43.71264, 20.94628, -78.26441, 30.21161, -122.61851, 41.44362, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 10000
Discounting theory with old adv calculation: [0.0, -0.00905, 0.00029, -0.01014, 0.00168, -0.00547, -0.00163, 0.00199, -0.01251, 0.00878, -0.03036, 0.01455, -0.05422, 0.02092, -0.08475, 0.02864, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.31811, -11.30151, 2.16876, -7.06504, -0.00163, 0.00199, -18.01842, 12.64049, -43.72633, 20.94827, -78.28145, 30.21141, -122.63672, 41.44033, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 10500
Discounting theory with old adv calculation: [0.0, -0.00905, 0.00028, -0.01014, 0.00168, -0.00547, -0.00163, 0.00199, -0.01252, 0.00878, -0.03037, 0.01455, -0.05422, 0.02092, -0.08476, 0.02863, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.31713, -11.29945, 2.16744, -7.06372, -0.00163, 0.00199, -18.02712, 12.64411, -43.73998, 20.95026, -78.29843, 30.21119, -122.65484, 41.43704, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 11000
Discounting theory with old adv calculation: [0.0, -0.00905, 0.00028, -0.01014, 0.00168, -0.00547, -0.00164, 0.002, -0.01252, 0.00878, -0.03038, 0.01455, -0.05423, 0.02092, -0.08476, 0.02863, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.31615, -11.2974, 2.16613, -7.0624, -0.00164, 0.002, -18.0358, 12.64772, -43.75357, 20.95223, -78.31534, 30.21098, -122.67287, 41.43375, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 11500
Discounting theory with old adv calculation: [0.0, -0.00905, 0.00028, -0.01014, 0.00168, -0.00547, -0.00164, 0.002, -0.01253, 0.00878, -0.03038, 0.01455, -0.05424, 0.02092, -0.08477, 0.02862, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.31519, -11.29534, 2.16482, -7.06108, -0.00164, 0.002, -18.04444, 12.65132, -43.76712, 20.95419, -78.33218, 30.21075, -122.69083, 41.43045, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 12000
Discounting theory with old adv calculation: [0.0, -0.00905, 0.00028, -0.01014, 0.00168, -0.00547, -0.00164, 0.002, -0.01253, 0.00879, -0.03039, 0.01455, -0.05424, 0.02092, -0.08477, 0.02862, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.31422, -11.29329, 2.16351, -7.05977, -0.00164, 0.002, -18.05306, 12.65491, -43.78062, 20.95614, -78.34895, 30.21051, -122.7087, 41.42716, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 12500
Discounting theory with old adv calculation: [0.0, -0.00904, 0.00028, -0.01014, 0.00167, -0.00547, -0.00164, 0.002, -0.01254, 0.00879, -0.0304, 0.01455, -0.05425, 0.02091, -0.08478, 0.02861, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.31327, -11.29123, 2.1622, -7.05845, -0.00164, 0.002, -18.06165, 12.65848, -43.79407, 20.95808, -78.36566, 30.21027, -122.72649, 41.42386, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 13000
Discounting theory with old adv calculation: [0.0, -0.00904, 0.00028, -0.01014, 0.00167, -0.00546, -0.00165, 0.00201, -0.01254, 0.00879, -0.0304, 0.01455, -0.05426, 0.02091, -0.08478, 0.02861, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.31232, -11.28918, 2.1609, -7.05713, -0.00165, 0.00201, -18.07022, 12.66203, -43.80747, 20.96, -78.3823, 30.21002, -122.7442, 41.42057, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 13500
Discounting theory with old adv calculation: [0.0, -0.00904, 0.00028, -0.01014, 0.00167, -0.00546, -0.00165, 0.00201, -0.01255, 0.00879, -0.03041, 0.01455, -0.05426, 0.02091, -0.08479, 0.02861, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.31137, -11.28713, 2.15961, -7.05581, -0.00165, 0.00201, -18.07876, 12.66558, -43.82083, 20.96191, -78.39888, 30.20977, -122.76183, 41.41727, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 14000
Discounting theory with old adv calculation: [0.0, -0.00904, 0.00028, -0.01014, 0.00167, -0.00546, -0.00165, 0.00201, -0.01255, 0.00879, -0.03042, 0.01455, -0.05427, 0.02091, -0.08479, 0.0286, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.31043, -11.28507, 2.15831, -7.05449, -0.00165, 0.00201, -18.08727, 12.66911, -43.83414, 20.96382, -78.41539, 30.2095, -122.77938, 41.41397, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 14500
Discounting theory with old adv calculation: [0.0, -0.00904, 0.00028, -0.01014, 0.00167, -0.00546, -0.00165, 0.00201, -0.01256, 0.00879, -0.03042, 0.01455, -0.05428, 0.02091, -0.0848, 0.0286, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.30949, -11.28302, 2.15703, -7.05318, -0.00165, 0.00201, -18.09576, 12.67262, -43.8474, 20.96571, -78.43183, 30.20923, -122.79685, 41.41068, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 15000
Discounting theory with old adv calculation: [0.0, -0.00904, 0.00028, -0.01014, 0.00167, -0.00546, -0.00165, 0.00202, -0.01256, 0.0088, -0.03043, 0.01455, -0.05428, 0.0209, -0.0848, 0.02859, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.30857, -11.28097, 2.15574, -7.05186, -0.00165, 0.00202, -18.10422, 12.67612, -43.86062, 20.96759, -78.44821, 30.20895, -122.81424, 41.40738, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 15500
Discounting theory with old adv calculation: [0.0, -0.00904, 0.00028, -0.01014, 0.00167, -0.00546, -0.00166, 0.00202, -0.01257, 0.0088, -0.03043, 0.01455, -0.05429, 0.0209, -0.08481, 0.02859, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.30764, -11.27891, 2.15446, -7.05054, -0.00166, 0.00202, -18.11265, 12.67961, -43.87379, 20.96945, -78.46453, 30.20867, -122.83156, 41.40408, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 16000
Discounting theory with old adv calculation: [0.0, -0.00904, 0.00028, -0.01014, 0.00167, -0.00546, -0.00166, 0.00202, -0.01257, 0.0088, -0.03044, 0.01455, -0.0543, 0.0209, -0.08481, 0.02858, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.30672, -11.27686, 2.15318, -7.04923, -0.00166, 0.00202, -18.12106, 12.68309, -43.88691, 20.97131, -78.48078, 30.20838, -122.8488, 41.40078, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 16500
Discounting theory with old adv calculation: [0.0, -0.00904, 0.00028, -0.01014, 0.00167, -0.00546, -0.00166, 0.00202, -0.01258, 0.0088, -0.03045, 0.01455, -0.0543, 0.0209, -0.08482, 0.02858, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.30581, -11.27481, 2.15191, -7.04791, -0.00166, 0.00202, -18.12944, 12.68655, -43.89999, 20.97316, -78.49697, 30.20808, -122.86596, 41.39748, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 17000
Discounting theory with old adv calculation: [0.0, -0.00904, 0.00027, -0.01014, 0.00167, -0.00546, -0.00166, 0.00203, -0.01258, 0.0088, -0.03045, 0.01455, -0.05431, 0.0209, -0.08482, 0.02857, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.3049, -11.27276, 2.15063, -7.0466, -0.00166, 0.00203, -18.1378, 12.69, -43.91303, 20.97499, -78.5131, 30.20777, -122.88304, 41.39418, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 17500
Discounting theory with old adv calculation: [0.0, -0.00904, 0.00027, -0.01014, 0.00166, -0.00545, -0.00166, 0.00203, -0.01259, 0.0088, -0.03046, 0.01455, -0.05432, 0.02089, -0.08482, 0.02857, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.304, -11.27071, 2.14937, -7.04528, -0.00166, 0.00203, -18.14613, 12.69344, -43.92602, 20.97682, -78.52917, 30.20746, -122.90005, 41.39088, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 18000
Discounting theory with old adv calculation: [0.0, -0.00904, 0.00027, -0.01014, 0.00166, -0.00545, -0.00167, 0.00203, -0.01259, 0.00881, -0.03047, 0.01455, -0.05432, 0.02089, -0.08483, 0.02856, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.3031, -11.26866, 2.1481, -7.04397, -0.00167, 0.00203, -18.15444, 12.69686, -43.93896, 20.97863, -78.54517, 30.20714, -122.91698, 41.38758, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 18500
Discounting theory with old adv calculation: [0.0, -0.00904, 0.00027, -0.01014, 0.00166, -0.00545, -0.00167, 0.00203, -0.0126, 0.00881, -0.03047, 0.01455, -0.05433, 0.02089, -0.08483, 0.02856, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.30221, -11.26661, 2.14684, -7.04266, -0.00167, 0.00203, -18.16272, 12.70027, -43.95186, 20.98043, -78.56111, 30.20681, -122.93384, 41.38427, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 19000
Discounting theory with old adv calculation: [0.0, -0.00903, 0.00027, -0.01014, 0.00166, -0.00545, -0.00167, 0.00204, -0.0126, 0.00881, -0.03048, 0.01455, -0.05434, 0.02089, -0.08484, 0.02855, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.30132, -11.26456, 2.14559, -7.04134, -0.00167, 0.00204, -18.17098, 12.70367, -43.96472, 20.98223, -78.577, 30.20648, -122.95063, 41.38097, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 19500
Discounting theory with old adv calculation: [0.0, -0.00903, 0.00027, -0.01014, 0.00166, -0.00545, -0.00167, 0.00204, -0.0126, 0.00881, -0.03049, 0.01455, -0.05434, 0.02089, -0.08484, 0.02855, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.30044, -11.26251, 2.14433, -7.04003, -0.00167, 0.00204, -18.17921, 12.70705, -43.97753, 20.98401, -78.59282, 30.20614, -122.96734, 41.37767, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 20000
Discounting theory with old adv calculation: [0.0, -0.00903, 0.00027, -0.01014, 0.00166, -0.00545, -0.00167, 0.00204, -0.01261, 0.00881, -0.03049, 0.01455, -0.05435, 0.02088, -0.08485, 0.02854, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.29956, -11.26046, 2.14308, -7.03872, -0.00167, 0.00204, -18.18742, 12.71042, -43.9903, 20.98578, -78.60858, 30.20579, -122.98398, 41.37437, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 20500
Discounting theory with old adv calculation: [0.0, -0.00903, 0.00027, -0.01014, 0.00166, -0.00545, -0.00168, 0.00205, -0.01261, 0.00881, -0.0305, 0.01455, -0.05436, 0.02088, -0.08485, 0.02854, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.29868, -11.25841, 2.14184, -7.03741, -0.00168, 0.00205, -18.1956, 12.71378, -44.00303, 20.98754, -78.62428, 30.20544, -123.00054, 41.37107, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 21000
Discounting theory with old adv calculation: [0.0, -0.00903, 0.00027, -0.01014, 0.00166, -0.00545, -0.00168, 0.00205, -0.01262, 0.00882, -0.03051, 0.01455, -0.05436, 0.02088, -0.08486, 0.02854, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.29782, -11.25636, 2.1406, -7.0361, -0.00168, 0.00205, -18.20376, 12.71713, -44.01572, 20.98929, -78.63992, 30.20508, -123.01704, 41.36777, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 21500
Discounting theory with old adv calculation: [0.0, -0.00903, 0.00027, -0.01014, 0.00166, -0.00545, -0.00168, 0.00205, -0.01262, 0.00882, -0.03051, 0.01455, -0.05437, 0.02088, -0.08486, 0.02853, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.29695, -11.25432, 2.13936, -7.03479, -0.00168, 0.00205, -18.2119, 12.72046, -44.02836, 20.99103, -78.65551, 30.20472, -123.03346, 41.36447, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 22000
Discounting theory with old adv calculation: [0.0, -0.00903, 0.00027, -0.01014, 0.00166, -0.00544, -0.00168, 0.00205, -0.01263, 0.00882, -0.03052, 0.01455, -0.05438, 0.02088, -0.08487, 0.02853, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.29609, -11.25227, 2.13812, -7.03348, -0.00168, 0.00205, -18.22001, 12.72379, -44.04096, 20.99276, -78.67103, 30.20435, -123.04981, 41.36116, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 22500
Discounting theory with old adv calculation: [0.0, -0.00903, 0.00027, -0.01014, 0.00165, -0.00544, -0.00169, 0.00206, -0.01263, 0.00882, -0.03052, 0.01455, -0.05438, 0.02087, -0.08487, 0.02852, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.29524, -11.25023, 2.13689, -7.03217, -0.00169, 0.00206, -18.22809, 12.7271, -44.05352, 20.99449, -78.6865, 30.20397, -123.06609, 41.35786, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 23000
Discounting theory with old adv calculation: [0.0, -0.00903, 0.00027, -0.01014, 0.00165, -0.00544, -0.00169, 0.00206, -0.01264, 0.00882, -0.03053, 0.01455, -0.05439, 0.02087, -0.08488, 0.02852, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.29439, -11.24819, 2.13566, -7.03086, -0.00169, 0.00206, -18.23616, 12.73039, -44.06603, 20.9962, -78.70191, 30.20359, -123.08231, 41.35456, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 23500
Discounting theory with old adv calculation: [0.0, -0.00903, 0.00026, -0.01014, 0.00165, -0.00544, -0.00169, 0.00206, -0.01264, 0.00882, -0.03054, 0.01455, -0.05439, 0.02087, -0.08488, 0.02851, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.29355, -11.24614, 2.13443, -7.02955, -0.00169, 0.00206, -18.2442, 12.73368, -44.07851, 20.9979, -78.71726, 30.2032, -123.09845, 41.35126, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 24000
Discounting theory with old adv calculation: [0.0, -0.00903, 0.00026, -0.01014, 0.00165, -0.00544, -0.00169, 0.00206, -0.01265, 0.00883, -0.03054, 0.01455, -0.0544, 0.02087, -0.08488, 0.02851, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.29271, -11.2441, 2.13321, -7.02824, -0.00169, 0.00206, -18.25221, 12.73695, -44.09094, 20.99959, -78.73256, 30.2028, -123.11452, 41.34796, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 24500
Discounting theory with old adv calculation: [0.0, -0.00903, 0.00026, -0.01014, 0.00165, -0.00544, -0.00169, 0.00207, -0.01265, 0.00883, -0.03055, 0.01455, -0.05441, 0.02087, -0.08489, 0.0285, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.29187, -11.24206, 2.13199, -7.02694, -0.00169, 0.00207, -18.26021, 12.74022, -44.10333, 21.00127, -78.7478, 30.2024, -123.13053, 41.34466, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 25000
Discounting theory with old adv calculation: [0.0, -0.00903, 0.00026, -0.01014, 0.00165, -0.00544, -0.0017, 0.00207, -0.01266, 0.00883, -0.03056, 0.01455, -0.05441, 0.02086, -0.08489, 0.0285, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.29104, -11.24002, 2.13078, -7.02563, -0.0017, 0.00207, -18.26818, 12.74347, -44.11569, 21.00294, -78.76298, 30.202, -123.14647, 41.34137, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 25500
Discounting theory with old adv calculation: [0.0, -0.00903, 0.00026, -0.01014, 0.00165, -0.00544, -0.0017, 0.00207, -0.01266, 0.00883, -0.03056, 0.01455, -0.05442, 0.02086, -0.0849, 0.02849, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.29022, -11.23798, 2.12957, -7.02432, -0.0017, 0.00207, -18.27613, 12.74671, -44.128, 21.00461, -78.77811, 30.20158, -123.16234, 41.33807, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 26000
Discounting theory with old adv calculation: [0.0, -0.00902, 0.00026, -0.01014, 0.00165, -0.00544, -0.0017, 0.00207, -0.01266, 0.00883, -0.03057, 0.01455, -0.05443, 0.02086, -0.0849, 0.02849, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.28939, -11.23594, 2.12836, -7.02302, -0.0017, 0.00207, -18.28405, 12.74993, -44.14027, 21.00626, -78.79318, 30.20117, -123.17814, 41.33477, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 26500
Discounting theory with old adv calculation: [0.0, -0.00902, 0.00026, -0.01014, 0.00165, -0.00544, -0.0017, 0.00208, -0.01267, 0.00883, -0.03057, 0.01455, -0.05443, 0.02086, -0.08491, 0.02849, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.28858, -11.2339, 2.12715, -7.02172, -0.0017, 0.00208, -18.29195, 12.75315, -44.1525, 21.0079, -78.8082, 30.20074, -123.19388, 41.33147, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 27000
Discounting theory with old adv calculation: [0.0, -0.00902, 0.00026, -0.01014, 0.00165, -0.00543, -0.0017, 0.00208, -0.01267, 0.00883, -0.03058, 0.01455, -0.05444, 0.02086, -0.08491, 0.02848, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.28776, -11.23187, 2.12595, -7.02041, -0.0017, 0.00208, -18.29983, 12.75635, -44.16469, 21.00954, -78.82316, 30.20032, -123.20955, 41.32817, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 27500
Discounting theory with old adv calculation: [0.0, -0.00902, 0.00026, -0.01014, 0.00164, -0.00543, -0.00171, 0.00208, -0.01268, 0.00884, -0.03059, 0.01455, -0.05444, 0.02086, -0.08492, 0.02848, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.28696, -11.22983, 2.12475, -7.01911, -0.00171, 0.00208, -18.30769, 12.75954, -44.17684, 21.01116, -78.83807, 30.19988, -123.22516, 41.32488, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 28000
Discounting theory with old adv calculation: [0.0, -0.00902, 0.00026, -0.01014, 0.00164, -0.00543, -0.00171, 0.00208, -0.01268, 0.00884, -0.03059, 0.01455, -0.05445, 0.02085, -0.08492, 0.02847, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.28615, -11.2278, 2.12356, -7.01781, -0.00171, 0.00208, -18.31552, 12.76272, -44.18896, 21.01278, -78.85292, 30.19944, -123.2407, 41.32158, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 28500
Discounting theory with old adv calculation: [0.0, -0.00902, 0.00026, -0.01013, 0.00164, -0.00543, -0.00171, 0.00208, -0.01269, 0.00884, -0.0306, 0.01455, -0.05446, 0.02085, -0.08492, 0.02847, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.28535, -11.22576, 2.12236, -7.0165, -0.00171, 0.00208, -18.32333, 12.76589, -44.20103, 21.01439, -78.86772, 30.199, -123.25618, 41.31829, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 29000
Discounting theory with old adv calculation: [0.0, -0.00902, 0.00026, -0.01013, 0.00164, -0.00543, -0.00171, 0.00209, -0.01269, 0.00884, -0.0306, 0.01455, -0.05446, 0.02085, -0.08493, 0.02846, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.28456, -11.22373, 2.12117, -7.0152, -0.00171, 0.00209, -18.33112, 12.76905, -44.21307, 21.01598, -78.88247, 30.19855, -123.27159, 41.31499, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 29500
Discounting theory with old adv calculation: [0.0, -0.00902, 0.00026, -0.01013, 0.00164, -0.00543, -0.00171, 0.00209, -0.0127, 0.00884, -0.03061, 0.01455, -0.05447, 0.02085, -0.08493, 0.02846, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.28376, -11.2217, 2.11999, -7.0139, -0.00171, 0.00209, -18.33889, 12.7722, -44.22506, 21.01757, -78.89716, 30.1981, -123.28694, 41.3117, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 30000
Discounting theory with old adv calculation: [0.0, -0.00902, 0.00026, -0.01013, 0.00164, -0.00543, -0.00172, 0.00209, -0.0127, 0.00884, -0.03062, 0.01455, -0.05448, 0.02085, -0.08494, 0.02846, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.28298, -11.21967, 2.1188, -7.0126, -0.00172, 0.00209, -18.34663, 12.77534, -44.23702, 21.01915, -78.9118, 30.19764, -123.30222, 41.30841, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 30500
Discounting theory with old adv calculation: [0.0, -0.00902, 0.00025, -0.01013, 0.00164, -0.00543, -0.00172, 0.00209, -0.01271, 0.00885, -0.03062, 0.01455, -0.05448, 0.02084, -0.08494, 0.02845, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.2822, -11.21764, 2.11762, -7.0113, -0.00172, 0.00209, -18.35436, 12.77846, -44.24894, 21.02072, -78.92639, 30.19717, -123.31745, 41.30512, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 31000
Discounting theory with old adv calculation: [0.0, -0.00902, 0.00025, -0.01013, 0.00164, -0.00543, -0.00172, 0.0021, -0.01271, 0.00885, -0.03063, 0.01455, -0.05449, 0.02084, -0.08495, 0.02845, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.28142, -11.21561, 2.11645, -7.01001, -0.00172, 0.0021, -18.36206, 12.78158, -44.26082, 21.02229, -78.94093, 30.1967, -123.33261, 41.30183, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 31500
Discounting theory with old adv calculation: [0.0, -0.00902, 0.00025, -0.01013, 0.00164, -0.00542, -0.00172, 0.0021, -0.01271, 0.00885, -0.03063, 0.01455, -0.05449, 0.02084, -0.08495, 0.02844, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.28064, -11.21358, 2.11527, -7.00871, -0.00172, 0.0021, -18.36974, 12.78468, -44.27267, 21.02384, -78.95541, 30.19623, -123.34771, 41.29854, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 32000
Discounting theory with old adv calculation: [0.0, -0.00902, 0.00025, -0.01013, 0.00164, -0.00542, -0.00172, 0.0021, -0.01272, 0.00885, -0.03064, 0.01455, -0.0545, 0.02084, -0.08495, 0.02844, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.27987, -11.21156, 2.1141, -7.00741, -0.00172, 0.0021, -18.3774, 12.78777, -44.28448, 21.02539, -78.96985, 30.19575, -123.36275, 41.29525, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 32500
Discounting theory with old adv calculation: [0.0, -0.00902, 0.00025, -0.01013, 0.00164, -0.00542, -0.00173, 0.0021, -0.01272, 0.00885, -0.03065, 0.01455, -0.05451, 0.02084, -0.08496, 0.02843, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.27911, -11.20953, 2.11294, -7.00612, -0.00173, 0.0021, -18.38504, 12.79085, -44.29625, 21.02692, -78.98423, 30.19526, -123.37772, 41.29196, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 33000
Discounting theory with old adv calculation: [0.0, -0.00902, 0.00025, -0.01013, 0.00163, -0.00542, -0.00173, 0.00211, -0.01273, 0.00885, -0.03065, 0.01455, -0.05451, 0.02084, -0.08496, 0.02843, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.27834, -11.20751, 2.11177, -7.00482, -0.00173, 0.00211, -18.39265, 12.79392, -44.30798, 21.02845, -78.99856, 30.19478, -123.39264, 41.28868, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 33500
Discounting theory with old adv calculation: [0.0, -0.00901, 0.00025, -0.01013, 0.00163, -0.00542, -0.00173, 0.00211, -0.01273, 0.00885, -0.03066, 0.01455, -0.05452, 0.02083, -0.08497, 0.02843, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.27758, -11.20549, 2.11061, -7.00353, -0.00173, 0.00211, -18.40025, 12.79698, -44.31968, 21.02997, -79.01285, 30.19428, -123.40749, 41.28539, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 34000
Discounting theory with old adv calculation: [0.0, -0.00901, 0.00025, -0.01013, 0.00163, -0.00542, -0.00173, 0.00211, -0.01274, 0.00886, -0.03066, 0.01455, -0.05452, 0.02083, -0.08497, 0.02842, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.27683, -11.20347, 2.10945, -7.00223, -0.00173, 0.00211, -18.40782, 12.80003, -44.33134, 21.03148, -79.02708, 30.19378, -123.42229, 41.28211, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 34500
Discounting theory with old adv calculation: [0.0, -0.00901, 0.00025, -0.01013, 0.00163, -0.00542, -0.00173, 0.00211, -0.01274, 0.00886, -0.03067, 0.01455, -0.05453, 0.02083, -0.08498, 0.02842, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.27608, -11.20145, 2.1083, -7.00094, -0.00173, 0.00211, -18.41538, 12.80307, -44.34296, 21.03299, -79.04126, 30.19328, -123.43702, 41.27882, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 35000
Discounting theory with old adv calculation: [0.0, -0.00901, 0.00025, -0.01013, 0.00163, -0.00542, -0.00174, 0.00212, -0.01274, 0.00886, -0.03068, 0.01455, -0.05454, 0.02083, -0.08498, 0.02841, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.27533, -11.19943, 2.10715, -6.99965, -0.00174, 0.00212, -18.42291, 12.8061, -44.35455, 21.03448, -79.0554, 30.19277, -123.4517, 41.27554, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 35500
Discounting theory with old adv calculation: [0.0, -0.00901, 0.00025, -0.01013, 0.00163, -0.00542, -0.00174, 0.00212, -0.01275, 0.00886, -0.03068, 0.01455, -0.05454, 0.02083, -0.08498, 0.02841, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.27459, -11.19741, 2.106, -6.99836, -0.00174, 0.00212, -18.43042, 12.80912, -44.3661, 21.03597, -79.06948, 30.19226, -123.46632, 41.27226, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 36000
Discounting theory with old adv calculation: [0.0, -0.00901, 0.00025, -0.01013, 0.00163, -0.00542, -0.00174, 0.00212, -0.01275, 0.00886, -0.03069, 0.01455, -0.05455, 0.02082, -0.08499, 0.0284, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.27385, -11.1954, 2.10485, -6.99707, -0.00174, 0.00212, -18.43792, 12.81213, -44.37762, 21.03745, -79.08352, 30.19174, -123.48088, 41.26898, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 36500
Discounting theory with old adv calculation: [0.0, -0.00901, 0.00025, -0.01013, 0.00163, -0.00541, -0.00174, 0.00212, -0.01276, 0.00886, -0.03069, 0.01455, -0.05455, 0.02082, -0.08499, 0.0284, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.27312, -11.19338, 2.10371, -6.99578, -0.00174, 0.00212, -18.44539, 12.81513, -44.3891, 21.03892, -79.09751, 30.19122, -123.49538, 41.26571, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 37000
Discounting theory with old adv calculation: [0.0, -0.00901, 0.00025, -0.01013, 0.00163, -0.00541, -0.00174, 0.00213, -0.01276, 0.00886, -0.0307, 0.01455, -0.05456, 0.02082, -0.085, 0.0284, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.27239, -11.19137, 2.10257, -6.99449, -0.00174, 0.00213, -18.45284, 12.81811, -44.40055, 21.04038, -79.11145, 30.1907, -123.50983, 41.26243, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 37500
Discounting theory with old adv calculation: [0.0, -0.00901, 0.00025, -0.01013, 0.00163, -0.00541, -0.00174, 0.00213, -0.01277, 0.00887, -0.03071, 0.01455, -0.05457, 0.02082, -0.085, 0.02839, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.27166, -11.18935, 2.10143, -6.9932, -0.00174, 0.00213, -18.46027, 12.82109, -44.41196, 21.04184, -79.12534, 30.19017, -123.52422, 41.25915, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 38000
Discounting theory with old adv calculation: [0.0, -0.00901, 0.00025, -0.01013, 0.00163, -0.00541, -0.00175, 0.00213, -0.01277, 0.00887, -0.03071, 0.01455, -0.05457, 0.02082, -0.085, 0.02839, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.27093, -11.18734, 2.1003, -6.99191, -0.00175, 0.00213, -18.46768, 12.82406, -44.42334, 21.04328, -79.13918, 30.18963, -123.53855, 41.25588, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 38500
Discounting theory with old adv calculation: [0.0, -0.00901, 0.00024, -0.01013, 0.00162, -0.00541, -0.00175, 0.00213, -0.01277, 0.00887, -0.03072, 0.01455, -0.05458, 0.02082, -0.08501, 0.02838, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.27021, -11.18533, 2.09917, -6.99062, -0.00175, 0.00213, -18.47507, 12.82702, -44.43468, 21.04472, -79.15298, 30.1891, -123.55282, 41.25261, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 39000
Discounting theory with old adv calculation: [0.0, -0.00901, 0.00024, -0.01013, 0.00162, -0.00541, -0.00175, 0.00213, -0.01278, 0.00887, -0.03072, 0.01455, -0.05458, 0.02081, -0.08501, 0.02838, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.2695, -11.18333, 2.09804, -6.98934, -0.00175, 0.00213, -18.48244, 12.82996, -44.44599, 21.04615, -79.16673, 30.18855, -123.56704, 41.24934, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 39500
Discounting theory with old adv calculation: [0.0, -0.00901, 0.00024, -0.01013, 0.00162, -0.00541, -0.00175, 0.00214, -0.01278, 0.00887, -0.03073, 0.01455, -0.05459, 0.02081, -0.08502, 0.02838, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.26879, -11.18132, 2.09691, -6.98805, -0.00175, 0.00214, -18.4898, 12.8329, -44.45726, 21.04758, -79.18043, 30.18801, -123.58121, 41.24607, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 40000
Discounting theory with old adv calculation: [0.0, -0.00901, 0.00024, -0.01013, 0.00162, -0.00541, -0.00175, 0.00214, -0.01279, 0.00887, -0.03073, 0.01455, -0.05459, 0.02081, -0.08502, 0.02837, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.26808, -11.17931, 2.09579, -6.98677, -0.00175, 0.00214, -18.49713, 12.83583, -44.4685, 21.04899, -79.19409, 30.18746, -123.59532, 41.2428, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 40500
Discounting theory with old adv calculation: [0.0, -0.00901, 0.00024, -0.01013, 0.00162, -0.00541, -0.00176, 0.00214, -0.01279, 0.00888, -0.03074, 0.01455, -0.0546, 0.02081, -0.08503, 0.02837, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.26737, -11.17731, 2.09467, -6.98549, -0.00176, 0.00214, -18.50444, 12.83875, -44.4797, 21.0504, -79.2077, 30.1869, -123.60937, 41.23953, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 41000
Discounting theory with old adv calculation: [0.0, -0.00901, 0.00024, -0.01013, 0.00162, -0.00541, -0.00176, 0.00214, -0.0128, 0.00888, -0.03075, 0.01455, -0.05461, 0.02081, -0.08503, 0.02836, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.26667, -11.17531, 2.09356, -6.9842, -0.00176, 0.00214, -18.51173, 12.84166, -44.49087, 21.0518, -79.22127, 30.18635, -123.62337, 41.23627, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 41500
Discounting theory with old adv calculation: [0.0, -0.009, 0.00024, -0.01013, 0.00162, -0.0054, -0.00176, 0.00215, -0.0128, 0.00888, -0.03075, 0.01455, -0.05461, 0.02081, -0.08503, 0.02836, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.26597, -11.17331, 2.09244, -6.98292, -0.00176, 0.00215, -18.51901, 12.84456, -44.50201, 21.0532, -79.23478, 30.18578, -123.63731, 41.233, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 42000
Discounting theory with old adv calculation: [0.0, -0.009, 0.00024, -0.01013, 0.00162, -0.0054, -0.00176, 0.00215, -0.0128, 0.00888, -0.03076, 0.01455, -0.05462, 0.0208, -0.08504, 0.02835, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.26528, -11.17131, 2.09133, -6.98164, -0.00176, 0.00215, -18.52626, 12.84744, -44.51312, 21.05458, -79.24826, 30.18522, -123.65121, 41.22974, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 42500
Discounting theory with old adv calculation: [0.0, -0.009, 0.00024, -0.01013, 0.00162, -0.0054, -0.00176, 0.00215, -0.01281, 0.00888, -0.03076, 0.01455, -0.05462, 0.0208, -0.08504, 0.02835, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.26458, -11.16931, 2.09022, -6.98036, -0.00176, 0.00215, -18.5335, 12.85032, -44.52419, 21.05596, -79.26169, 30.18465, -123.66504, 41.22648, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 43000
Discounting theory with old adv calculation: [0.0, -0.009, 0.00024, -0.01013, 0.00162, -0.0054, -0.00177, 0.00215, -0.01281, 0.00888, -0.03077, 0.01455, -0.05463, 0.0208, -0.08505, 0.02835, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.2639, -11.16731, 2.08912, -6.97908, -0.00177, 0.00215, -18.54071, 12.85319, -44.53523, 21.05733, -79.27507, 30.18407, -123.67883, 41.22322, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 43500
Discounting theory with old adv calculation: [0.0, -0.009, 0.00024, -0.01013, 0.00162, -0.0054, -0.00177, 0.00216, -0.01282, 0.00888, -0.03077, 0.01455, -0.05464, 0.0208, -0.08505, 0.02834, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.26321, -11.16532, 2.08802, -6.97781, -0.00177, 0.00216, -18.54791, 12.85605, -44.54624, 21.05869, -79.28841, 30.1835, -123.69256, 41.21997, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 44000
Discounting theory with old adv calculation: [0.0, -0.009, 0.00024, -0.01013, 0.00162, -0.0054, -0.00177, 0.00216, -0.01282, 0.00889, -0.03078, 0.01455, -0.05464, 0.0208, -0.08505, 0.02834, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.26253, -11.16332, 2.08692, -6.97653, -0.00177, 0.00216, -18.55509, 12.85891, -44.55721, 21.06005, -79.30171, 30.18291, -123.70624, 41.21671, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 44500
Discounting theory with old adv calculation: [0.0, -0.009, 0.00024, -0.01013, 0.00161, -0.0054, -0.00177, 0.00216, -0.01283, 0.00889, -0.03079, 0.01455, -0.05465, 0.0208, -0.08506, 0.02833, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.26185, -11.16133, 2.08582, -6.97525, -0.00177, 0.00216, -18.56225, 12.86175, -44.56815, 21.0614, -79.31496, 30.18233, -123.71987, 41.21346, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 45000
Discounting theory with old adv calculation: [0.0, -0.009, 0.00024, -0.01013, 0.00161, -0.0054, -0.00177, 0.00216, -0.01283, 0.00889, -0.03079, 0.01455, -0.05465, 0.02079, -0.08506, 0.02833, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.26118, -11.15934, 2.08473, -6.97398, -0.00177, 0.00216, -18.56939, 12.86458, -44.57906, 21.06274, -79.32817, 30.18174, -123.73344, 41.21021, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 45500
Discounting theory with old adv calculation: [0.0, -0.009, 0.00024, -0.01013, 0.00161, -0.0054, -0.00177, 0.00216, -0.01283, 0.00889, -0.0308, 0.01455, -0.05466, 0.02079, -0.08507, 0.02833, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.26051, -11.15735, 2.08363, -6.9727, -0.00177, 0.00216, -18.57652, 12.8674, -44.58994, 21.06408, -79.34134, 30.18115, -123.74697, 41.20696, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 46000
Discounting theory with old adv calculation: [0.0, -0.009, 0.00024, -0.01013, 0.00161, -0.0054, -0.00178, 0.00217, -0.01284, 0.00889, -0.0308, 0.01455, -0.05466, 0.02079, -0.08507, 0.02832, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.25984, -11.15536, 2.08255, -6.97143, -0.00178, 0.00217, -18.58362, 12.87022, -44.60078, 21.06541, -79.35446, 30.18055, -123.76044, 41.20371, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 46500
Discounting theory with old adv calculation: [0.0, -0.009, 0.00024, -0.01013, 0.00161, -0.00539, -0.00178, 0.00217, -0.01284, 0.00889, -0.03081, 0.01455, -0.05467, 0.02079, -0.08507, 0.02832, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.25918, -11.15338, 2.08146, -6.97016, -0.00178, 0.00217, -18.59071, 12.87302, -44.6116, 21.06673, -79.36754, 30.17995, -123.77387, 41.20046, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 47000
Discounting theory with old adv calculation: [0.0, -0.009, 0.00023, -0.01013, 0.00161, -0.00539, -0.00178, 0.00217, -0.01285, 0.00889, -0.03081, 0.01455, -0.05467, 0.02079, -0.08508, 0.02831, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.25851, -11.15139, 2.08038, -6.96888, -0.00178, 0.00217, -18.59777, 12.87582, -44.62238, 21.06804, -79.38058, 30.17935, -123.78724, 41.19722, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 47500
Discounting theory with old adv calculation: [0.0, -0.009, 0.00023, -0.01013, 0.00161, -0.00539, -0.00178, 0.00217, -0.01285, 0.00889, -0.03082, 0.01455, -0.05468, 0.02078, -0.08508, 0.02831, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.25786, -11.14941, 2.0793, -6.96761, -0.00178, 0.00217, -18.60482, 12.87861, -44.63313, 21.06935, -79.39357, 30.17875, -123.80056, 41.19397, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 48000
Discounting theory with old adv calculation: [0.0, -0.009, 0.00023, -0.01012, 0.00161, -0.00539, -0.00178, 0.00218, -0.01285, 0.0089, -0.03082, 0.01455, -0.05469, 0.02078, -0.08509, 0.02831, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.2572, -11.14743, 2.07822, -6.96634, -0.00178, 0.00218, -18.61186, 12.88139, -44.64385, 21.07065, -79.40652, 30.17814, -123.81383, 41.19073, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 48500
Discounting theory with old adv calculation: [0.0, -0.009, 0.00023, -0.01012, 0.00161, -0.00539, -0.00179, 0.00218, -0.01286, 0.0089, -0.03083, 0.01455, -0.05469, 0.02078, -0.08509, 0.0283, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.25655, -11.14545, 2.07714, -6.96507, -0.00179, 0.00218, -18.61887, 12.88415, -44.65454, 21.07195, -79.41943, 30.17752, -123.82706, 41.18749, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 49000
Discounting theory with old adv calculation: [0.0, -0.009, 0.00023, -0.01012, 0.00161, -0.00539, -0.00179, 0.00218, -0.01286, 0.0089, -0.03084, 0.01455, -0.0547, 0.02078, -0.08509, 0.0283, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.2559, -11.14347, 2.07607, -6.96381, -0.00179, 0.00218, -18.62586, 12.88692, -44.6652, 21.07323, -79.4323, 30.17691, -123.84023, 41.18426, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 49500
Discounting theory with old adv calculation: [0.0, -0.009, 0.00023, -0.01012, 0.00161, -0.00539, -0.00179, 0.00218, -0.01287, 0.0089, -0.03084, 0.01455, -0.0547, 0.02078, -0.0851, 0.02829, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.25526, -11.14149, 2.075, -6.96254, -0.00179, 0.00218, -18.63284, 12.88967, -44.67583, 21.07451, -79.44513, 30.17629, -123.85336, 41.18102, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 50000
Discounting theory with old adv calculation: [0.0, -0.009, 0.00023, -0.01012, 0.00161, -0.00539, -0.00179, 0.00218, -0.01287, 0.0089, -0.03085, 0.01455, -0.05471, 0.02078, -0.0851, 0.02829, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.25461, -11.13952, 2.07393, -6.96127, -0.00179, 0.00218, -18.6398, 12.89241, -44.68643, 21.07579, -79.45792, 30.17566, -123.86643, 41.17779, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 50500
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00023, -0.01012, 0.0016, -0.00539, -0.00179, 0.00219, -0.01287, 0.0089, -0.03085, 0.01455, -0.05471, 0.02077, -0.08511, 0.02829, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.25398, -11.13754, 2.07287, -6.96001, -0.00179, 0.00219, -18.64675, 12.89514, -44.69699, 21.07705, -79.47066, 30.17504, -123.87946, 41.17456, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 51000
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00023, -0.01012, 0.0016, -0.00539, -0.00179, 0.00219, -0.01288, 0.0089, -0.03086, 0.01455, -0.05472, 0.02077, -0.08511, 0.02828, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.25334, -11.13557, 2.07181, -6.95874, -0.00179, 0.00219, -18.65367, 12.89787, -44.70753, 21.07831, -79.48337, 30.17441, -123.89245, 41.17133, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 51500
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00023, -0.01012, 0.0016, -0.00538, -0.0018, 0.00219, -0.01288, 0.00891, -0.03086, 0.01455, -0.05472, 0.02077, -0.08511, 0.02828, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.25271, -11.1336, 2.07075, -6.95748, -0.0018, 0.00219, -18.66058, 12.90059, -44.71804, 21.07957, -79.49603, 30.17378, -123.90538, 41.1681, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 52000
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00023, -0.01012, 0.0016, -0.00538, -0.0018, 0.00219, -0.01289, 0.00891, -0.03087, 0.01455, -0.05473, 0.02077, -0.08512, 0.02828, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.25208, -11.13163, 2.06969, -6.95622, -0.0018, 0.00219, -18.66747, 12.9033, -44.72852, 21.08082, -79.50866, 30.17314, -123.91826, 41.16488, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 52500
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00023, -0.01012, 0.0016, -0.00538, -0.0018, 0.0022, -0.01289, 0.00891, -0.03087, 0.01455, -0.05474, 0.02077, -0.08512, 0.02827, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.25145, -11.12967, 2.06864, -6.95495, -0.0018, 0.0022, -18.67434, 12.906, -44.73896, 21.08206, -79.52124, 30.1725, -123.9311, 41.16165, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 53000
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00023, -0.01012, 0.0016, -0.00538, -0.0018, 0.0022, -0.01289, 0.00891, -0.03088, 0.01455, -0.05474, 0.02077, -0.08512, 0.02827, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.25083, -11.1277, 2.06759, -6.95369, -0.0018, 0.0022, -18.6812, 12.90869, -44.74938, 21.08329, -79.53378, 30.17186, -123.9439, 41.15843, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 53500
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00023, -0.01012, 0.0016, -0.00538, -0.0018, 0.0022, -0.0129, 0.00891, -0.03088, 0.01455, -0.05475, 0.02076, -0.08513, 0.02826, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.25021, -11.12574, 2.06654, -6.95243, -0.0018, 0.0022, -18.68804, 12.91137, -44.75977, 21.08452, -79.54629, 30.17122, -123.95664, 41.15521, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 54000
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00023, -0.01012, 0.0016, -0.00538, -0.00181, 0.0022, -0.0129, 0.00891, -0.03089, 0.01455, -0.05475, 0.02076, -0.08513, 0.02826, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.24959, -11.12377, 2.06549, -6.95118, -0.00181, 0.0022, -18.69486, 12.91404, -44.77013, 21.08574, -79.55876, 30.17057, -123.96934, 41.15199, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 54500
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00023, -0.01012, 0.0016, -0.00538, -0.00181, 0.0022, -0.01291, 0.00891, -0.0309, 0.01455, -0.05476, 0.02076, -0.08514, 0.02826, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.24897, -11.12181, 2.06445, -6.94992, -0.00181, 0.0022, -18.70167, 12.91671, -44.78046, 21.08696, -79.57118, 30.16992, -123.98199, 41.14878, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 55000
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00023, -0.01012, 0.0016, -0.00538, -0.00181, 0.00221, -0.01291, 0.00892, -0.0309, 0.01455, -0.05476, 0.02076, -0.08514, 0.02825, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.24836, -11.11986, 2.06341, -6.94866, -0.00181, 0.00221, -18.70845, 12.91937, -44.79077, 21.08817, -79.58357, 30.16927, -123.9946, 41.14557, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 55500
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00023, -0.01012, 0.0016, -0.00538, -0.00181, 0.00221, -0.01291, 0.00892, -0.03091, 0.01455, -0.05477, 0.02076, -0.08514, 0.02825, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.24775, -11.1179, 2.06237, -6.94741, -0.00181, 0.00221, -18.71523, 12.92202, -44.80104, 21.08937, -79.59592, 30.16861, -124.00717, 41.14236, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 56000
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00023, -0.01012, 0.0016, -0.00538, -0.00181, 0.00221, -0.01292, 0.00892, -0.03091, 0.01455, -0.05477, 0.02076, -0.08515, 0.02824, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.24715, -11.11594, 2.06133, -6.94615, -0.00181, 0.00221, -18.72198, 12.92466, -44.81129, 21.09057, -79.60823, 30.16795, -124.01968, 41.13915, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 56500
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00022, -0.01012, 0.00159, -0.00538, -0.00181, 0.00221, -0.01292, 0.00892, -0.03092, 0.01455, -0.05478, 0.02076, -0.08515, 0.02824, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.24654, -11.11399, 2.0603, -6.9449, -0.00181, 0.00221, -18.72872, 12.92729, -44.8215, 21.09176, -79.6205, 30.16729, -124.03216, 41.13594, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 57000
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00022, -0.01012, 0.00159, -0.00537, -0.00182, 0.00221, -0.01293, 0.00892, -0.03092, 0.01455, -0.05478, 0.02075, -0.08516, 0.02824, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.24594, -11.11204, 2.05927, -6.94364, -0.00182, 0.00221, -18.73544, 12.92992, -44.83169, 21.09295, -79.63274, 30.16663, -124.04459, 41.13274, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 57500
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00022, -0.01012, 0.00159, -0.00537, -0.00182, 0.00222, -0.01293, 0.00892, -0.03093, 0.01455, -0.05479, 0.02075, -0.08516, 0.02823, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.24534, -11.11009, 2.05824, -6.94239, -0.00182, 0.00222, -18.74215, 12.93254, -44.84185, 21.09413, -79.64494, 30.16596, -124.05697, 41.12953, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 58000
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00022, -0.01012, 0.00159, -0.00537, -0.00182, 0.00222, -0.01293, 0.00892, -0.03093, 0.01455, -0.05479, 0.02075, -0.08516, 0.02823, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.24475, -11.10814, 2.05721, -6.94114, -0.00182, 0.00222, -18.74884, 12.93515, -44.85199, 21.0953, -79.6571, 30.16529, -124.06931, 41.12633, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 58500
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00022, -0.01012, 0.00159, -0.00537, -0.00182, 0.00222, -0.01294, 0.00892, -0.03094, 0.01455, -0.0548, 0.02075, -0.08517, 0.02823, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.24416, -11.10619, 2.05619, -6.93989, -0.00182, 0.00222, -18.75551, 12.93775, -44.86209, 21.09647, -79.66922, 30.16462, -124.08161, 41.12314, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 59000
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00022, -0.01012, 0.00159, -0.00537, -0.00182, 0.00222, -0.01294, 0.00893, -0.03094, 0.01455, -0.05481, 0.02075, -0.08517, 0.02822, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.24357, -11.10425, 2.05517, -6.93864, -0.00182, 0.00222, -18.76216, 12.94034, -44.87217, 21.09763, -79.68131, 30.16394, -124.09386, 41.11994, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 59500
Discounting theory with old adv calculation: [0.0, -0.00899, 0.00022, -0.01012, 0.00159, -0.00537, -0.00182, 0.00223, -0.01295, 0.00893, -0.03095, 0.01455, -0.05481, 0.02075, -0.08517, 0.02822, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.24298, -11.1023, 2.05415, -6.93739, -0.00182, 0.00223, -18.76881, 12.94293, -44.88222, 21.09879, -79.69335, 30.16326, -124.10607, 41.11675, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 60000
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00022, -0.01012, 0.00159, -0.00537, -0.00183, 0.00223, -0.01295, 0.00893, -0.03095, 0.01455, -0.05482, 0.02074, -0.08518, 0.02821, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.2424, -11.10036, 2.05313, -6.93615, -0.00183, 0.00223, -18.77543, 12.94551, -44.89224, 21.09994, -79.70537, 30.16258, -124.11824, 41.11356, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 60500
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00022, -0.01012, 0.00159, -0.00537, -0.00183, 0.00223, -0.01295, 0.00893, -0.03096, 0.01455, -0.05482, 0.02074, -0.08518, 0.02821, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.24182, -11.09842, 2.05212, -6.9349, -0.00183, 0.00223, -18.78204, 12.94808, -44.90224, 21.10108, -79.71734, 30.1619, -124.13036, 41.11037, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 61000
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00022, -0.01012, 0.00159, -0.00537, -0.00183, 0.00223, -0.01296, 0.00893, -0.03096, 0.01455, -0.05483, 0.02074, -0.08519, 0.02821, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.24124, -11.09648, 2.0511, -6.93365, -0.00183, 0.00223, -18.78863, 12.95064, -44.91221, 21.10222, -79.72928, 30.16121, -124.14245, 41.10718, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 61500
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00022, -0.01012, 0.00159, -0.00537, -0.00183, 0.00223, -0.01296, 0.00893, -0.03097, 0.01455, -0.05483, 0.02074, -0.08519, 0.0282, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.24066, -11.09455, 2.0501, -6.93241, -0.00183, 0.00223, -18.79521, 12.95319, -44.92215, 21.10336, -79.74118, 30.16053, -124.15449, 41.104, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 62000
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00022, -0.01012, 0.00159, -0.00537, -0.00183, 0.00224, -0.01296, 0.00893, -0.03097, 0.01455, -0.05484, 0.02074, -0.08519, 0.0282, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.24009, -11.09261, 2.04909, -6.93117, -0.00183, 0.00224, -18.80177, 12.95574, -44.93206, 21.10448, -79.75305, 30.15984, -124.16648, 41.10082, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 62500
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00022, -0.01012, 0.00159, -0.00536, -0.00183, 0.00224, -0.01297, 0.00893, -0.03098, 0.01455, -0.05484, 0.02074, -0.0852, 0.0282, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23952, -11.09068, 2.04808, -6.92992, -0.00183, 0.00224, -18.80831, 12.95828, -44.94195, 21.10561, -79.76488, 30.15914, -124.17844, 41.09764, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 63000
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00022, -0.01012, 0.00158, -0.00536, -0.00184, 0.00224, -0.01297, 0.00894, -0.03098, 0.01455, -0.05485, 0.02073, -0.0852, 0.02819, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23895, -11.08875, 2.04708, -6.92868, -0.00184, 0.00224, -18.81484, 12.96081, -44.95181, 21.10672, -79.77668, 30.15845, -124.19036, 41.09446, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 63500
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00022, -0.01012, 0.00158, -0.00536, -0.00184, 0.00224, -0.01298, 0.00894, -0.03099, 0.01455, -0.05485, 0.02073, -0.0852, 0.02819, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.23839, -11.08682, 2.04608, -6.92744, -0.00184, 0.00224, -18.82136, 12.96333, -44.96165, 21.10783, -79.78844, 30.15775, -124.20223, 41.09129, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 64000
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00022, -0.01012, 0.00158, -0.00536, -0.00184, 0.00224, -0.01298, 0.00894, -0.03099, 0.01455, -0.05486, 0.02073, -0.08521, 0.02819, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23782, -11.08489, 2.04508, -6.9262, -0.00184, 0.00224, -18.82786, 12.96585, -44.97146, 21.10894, -79.80017, 30.15705, -124.21407, 41.08811, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 64500
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00022, -0.01012, 0.00158, -0.00536, -0.00184, 0.00225, -0.01298, 0.00894, -0.031, 0.01455, -0.05486, 0.02073, -0.08521, 0.02818, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.23726, -11.08297, 2.04409, -6.92497, -0.00184, 0.00225, -18.83434, 12.96836, -44.98124, 21.11004, -79.81186, 30.15635, -124.22586, 41.08494, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 65000
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00022, -0.01012, 0.00158, -0.00536, -0.00184, 0.00225, -0.01299, 0.00894, -0.03101, 0.01455, -0.05487, 0.02073, -0.08521, 0.02818, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23671, -11.08104, 2.0431, -6.92373, -0.00184, 0.00225, -18.84081, 12.97086, -44.991, 21.11113, -79.82351, 30.15564, -124.23761, 41.08178, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 65500
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00022, -0.01012, 0.00158, -0.00536, -0.00185, 0.00225, -0.01299, 0.00894, -0.03101, 0.01455, -0.05487, 0.02073, -0.08522, 0.02817, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23615, -11.07912, 2.0421, -6.92249, -0.00185, 0.00225, -18.84726, 12.97336, -45.00073, 21.11222, -79.83513, 30.15494, -124.24932, 41.07861, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 66000
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00022, -0.01012, 0.00158, -0.00536, -0.00185, 0.00225, -0.013, 0.00894, -0.03102, 0.01455, -0.05488, 0.02072, -0.08522, 0.02817, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.2356, -11.0772, 2.04112, -6.92126, -0.00185, 0.00225, -18.8537, 12.97584, -45.01044, 21.11331, -79.84672, 30.15423, -124.26099, 41.07545, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 66500
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00021, -0.01011, 0.00158, -0.00536, -0.00185, 0.00225, -0.013, 0.00895, -0.03102, 0.01455, -0.05488, 0.02072, -0.08523, 0.02817, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23505, -11.07528, 2.04013, -6.92002, -0.00185, 0.00225, -18.86013, 12.97832, -45.02012, 21.11438, -79.85827, 30.15351, -124.27263, 41.07229, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 67000
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00021, -0.01011, 0.00158, -0.00536, -0.00185, 0.00226, -0.013, 0.00895, -0.03103, 0.01455, -0.05489, 0.02072, -0.08523, 0.02816, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.2345, -11.07337, 2.03915, -6.91879, -0.00185, 0.00226, -18.86653, 12.9808, -45.02977, 21.11546, -79.86979, 30.1528, -124.28422, 41.06913, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 67500
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00021, -0.01011, 0.00158, -0.00535, -0.00185, 0.00226, -0.01301, 0.00895, -0.03103, 0.01455, -0.05489, 0.02072, -0.08523, 0.02816, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.23395, -11.07145, 2.03817, -6.91756, -0.00185, 0.00226, -18.87293, 12.98326, -45.0394, 21.11653, -79.88128, 30.15208, -124.29577, 41.06597, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 68000
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00021, -0.01011, 0.00158, -0.00535, -0.00185, 0.00226, -0.01301, 0.00895, -0.03104, 0.01455, -0.0549, 0.02072, -0.08524, 0.02816, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23341, -11.06954, 2.03719, -6.91633, -0.00185, 0.00226, -18.87931, 12.98572, -45.049, 21.11759, -79.89273, 30.15137, -124.30729, 41.06282, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 68500
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00021, -0.01011, 0.00158, -0.00535, -0.00186, 0.00226, -0.01301, 0.00895, -0.03104, 0.01455, -0.0549, 0.02072, -0.08524, 0.02815, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23287, -11.06763, 2.03621, -6.9151, -0.00186, 0.00226, -18.88567, 12.98817, -45.05858, 21.11865, -79.90415, 30.15065, -124.31876, 41.05967, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 69000
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00021, -0.01011, 0.00158, -0.00535, -0.00186, 0.00226, -0.01302, 0.00895, -0.03105, 0.01455, -0.05491, 0.02072, -0.08524, 0.02815, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23233, -11.06572, 2.03523, -6.91387, -0.00186, 0.00226, -18.89202, 12.99061, -45.06814, 21.1197, -79.91553, 30.14992, -124.3302, 41.05652, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 69500
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00021, -0.01011, 0.00157, -0.00535, -0.00186, 0.00227, -0.01302, 0.00895, -0.03105, 0.01455, -0.05491, 0.02071, -0.08525, 0.02815, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.2318, -11.06381, 2.03426, -6.91264, -0.00186, 0.00227, -18.89835, 12.99305, -45.07767, 21.12075, -79.92689, 30.1492, -124.3416, 41.05338, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 70000
Discounting theory with old adv calculation: [0.0, -0.00898, 0.00021, -0.01011, 0.00157, -0.00535, -0.00186, 0.00227, -0.01303, 0.00895, -0.03106, 0.01455, -0.05492, 0.02071, -0.08525, 0.02814, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23127, -11.0619, 2.03329, -6.91141, -0.00186, 0.00227, -18.90467, 12.99548, -45.08717, 21.12179, -79.93821, 30.14847, -124.35296, 41.05023, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 70500
Discounting theory with old adv calculation: [0.0, -0.00897, 0.00021, -0.01011, 0.00157, -0.00535, -0.00186, 0.00227, -0.01303, 0.00895, -0.03106, 0.01455, -0.05492, 0.02071, -0.08525, 0.02814, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.23073, -11.06, 2.03232, -6.91019, -0.00186, 0.00227, -18.91098, 12.9979, -45.09665, 21.12283, -79.94949, 30.14775, -124.36429, 41.04709, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 71000
Discounting theory with old adv calculation: [0.0, -0.00897, 0.00021, -0.01011, 0.00157, -0.00535, -0.00186, 0.00227, -0.01303, 0.00896, -0.03107, 0.01455, -0.05493, 0.02071, -0.08526, 0.02814, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23021, -11.0581, 2.03136, -6.90896, -0.00186, 0.00227, -18.91727, 13.00032, -45.10611, 21.12386, -79.96075, 30.14701, -124.37557, 41.04396, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 71500
Discounting theory with old adv calculation: [0.0, -0.00897, 0.00021, -0.01011, 0.00157, -0.00535, -0.00187, 0.00227, -0.01304, 0.00896, -0.03107, 0.01455, -0.05493, 0.02071, -0.08526, 0.02813, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.22968, -11.0562, 2.03039, -6.90774, -0.00187, 0.00227, -18.92354, 13.00273, -45.11554, 21.12488, -79.97197, 30.14628, -124.38682, 41.04082, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 72000
Discounting theory with old adv calculation: [0.0, -0.00897, 0.00021, -0.01011, 0.00157, -0.00535, -0.00187, 0.00228, -0.01304, 0.00896, -0.03108, 0.01455, -0.05494, 0.02071, -0.08526, 0.02813, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22916, -11.0543, 2.02943, -6.90651, -0.00187, 0.00228, -18.92981, 13.00513, -45.12495, 21.12591, -79.98316, 30.14555, -124.39803, 41.03769, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 72500
Discounting theory with old adv calculation: [0.0, -0.00897, 0.00021, -0.01011, 0.00157, -0.00535, -0.00187, 0.00228, -0.01304, 0.00896, -0.03108, 0.01455, -0.05494, 0.02071, -0.08527, 0.02812, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.22864, -11.0524, 2.02847, -6.90529, -0.00187, 0.00228, -18.93605, 13.00752, -45.13433, 21.12692, -79.99431, 30.14481, -124.4092, 41.03456, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 73000
Discounting theory with old adv calculation: [0.0, -0.00897, 0.00021, -0.01011, 0.00157, -0.00535, -0.00187, 0.00228, -0.01305, 0.00896, -0.03109, 0.01455, -0.05495, 0.0207, -0.08527, 0.02812, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22812, -11.05051, 2.02752, -6.90407, -0.00187, 0.00228, -18.94229, 13.00991, -45.14369, 21.12794, -80.00544, 30.14407, -124.42034, 41.03143, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 73500
Discounting theory with old adv calculation: [0.0, -0.00897, 0.00021, -0.01011, 0.00157, -0.00534, -0.00187, 0.00228, -0.01305, 0.00896, -0.03109, 0.01455, -0.05495, 0.0207, -0.08528, 0.02812, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.2276, -11.04862, 2.02656, -6.90285, -0.00187, 0.00228, -18.94851, 13.01229, -45.15303, 21.12894, -80.01653, 30.14333, -124.43144, 41.0283, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 74000
Discounting theory with old adv calculation: [0.0, -0.00897, 0.00021, -0.01011, 0.00157, -0.00534, -0.00187, 0.00228, -0.01305, 0.00896, -0.0311, 0.01455, -0.05496, 0.0207, -0.08528, 0.02811, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22708, -11.04672, 2.02561, -6.90163, -0.00187, 0.00228, -18.95471, 13.01467, -45.16234, 21.12995, -80.02759, 30.14259, -124.4425, 41.02518, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 74500
Discounting theory with old adv calculation: [0.0, -0.00897, 0.00021, -0.01011, 0.00157, -0.00534, -0.00187, 0.00229, -0.01306, 0.00896, -0.0311, 0.01455, -0.05496, 0.0207, -0.08528, 0.02811, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22657, -11.04484, 2.02466, -6.90042, -0.00187, 0.00229, -18.9609, 13.01704, -45.17163, 21.13095, -80.03863, 30.14185, -124.45353, 41.02206, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 75000
Discounting theory with old adv calculation: [0.0, -0.00897, 0.00021, -0.01011, 0.00157, -0.00534, -0.00188, 0.00229, -0.01306, 0.00897, -0.0311, 0.01455, -0.05497, 0.0207, -0.08529, 0.02811, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22606, -11.04295, 2.02371, -6.8992, -0.00188, 0.00229, -18.96708, 13.0194, -45.18089, 21.13194, -80.04963, 30.1411, -124.46452, 41.01894, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 75500
Discounting theory with old adv calculation: [0.0, -0.00897, 0.00021, -0.01011, 0.00157, -0.00534, -0.00188, 0.00229, -0.01307, 0.00897, -0.03111, 0.01455, -0.05497, 0.0207, -0.08529, 0.0281, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.22555, -11.04106, 2.02276, -6.89798, -0.00188, 0.00229, -18.97325, 13.02175, -45.19013, 21.13293, -80.06059, 30.14035, -124.47548, 41.01583, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 76000
Discounting theory with old adv calculation: [0.0, -0.00897, 0.00021, -0.01011, 0.00157, -0.00534, -0.00188, 0.00229, -0.01307, 0.00897, -0.03111, 0.01455, -0.05498, 0.02069, -0.08529, 0.0281, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22505, -11.03918, 2.02182, -6.89677, -0.00188, 0.00229, -18.9794, 13.0241, -45.19935, 21.13391, -80.07153, 30.1396, -124.4864, 41.01271, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 76500
Discounting theory with old adv calculation: [0.0, -0.00897, 0.00021, -0.01011, 0.00156, -0.00534, -0.00188, 0.00229, -0.01307, 0.00897, -0.03112, 0.01455, -0.05498, 0.02069, -0.0853, 0.0281, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22454, -11.0373, 2.02088, -6.89555, -0.00188, 0.00229, -18.98553, 13.02644, -45.20854, 21.13489, -80.08244, 30.13885, -124.49728, 41.0096, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 77000
Discounting theory with old adv calculation: [0.0, -0.00897, 0.00021, -0.01011, 0.00156, -0.00534, -0.00188, 0.0023, -0.01308, 0.00897, -0.03112, 0.01455, -0.05499, 0.02069, -0.0853, 0.02809, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.22404, -11.03542, 2.01994, -6.89434, -0.00188, 0.0023, -18.99165, 13.02878, -45.21772, 21.13586, -80.09332, 30.1381, -124.50813, 41.00649, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 77500
Discounting theory with old adv calculation: [0.0, -0.00897, 0.0002, -0.01011, 0.00156, -0.00534, -0.00188, 0.0023, -0.01308, 0.00897, -0.03113, 0.01455, -0.05499, 0.02069, -0.0853, 0.02809, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.22354, -11.03354, 2.019, -6.89313, -0.00188, 0.0023, -18.99776, 13.0311, -45.22687, 21.13683, -80.10416, 30.13735, -124.51895, 41.00339, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 78000
Discounting theory with old adv calculation: [0.0, -0.00897, 0.0002, -0.01011, 0.00156, -0.00534, -0.00189, 0.0023, -0.01308, 0.00897, -0.03113, 0.01455, -0.055, 0.02069, -0.08531, 0.02809, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.22305, -11.03166, 2.01806, -6.89192, -0.00189, 0.0023, -19.00386, 13.03343, -45.23599, 21.1378, -80.11498, 30.13659, -124.52973, 41.00029, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 78500
Discounting theory with old adv calculation: [0.0, -0.00897, 0.0002, -0.01011, 0.00156, -0.00534, -0.00189, 0.0023, -0.01309, 0.00897, -0.03114, 0.01455, -0.055, 0.02069, -0.08531, 0.02808, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.22255, -11.02979, 2.01713, -6.89071, -0.00189, 0.0023, -19.00994, 13.03574, -45.2451, 21.13876, -80.12576, 30.13583, -124.54047, 40.99719, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 79000
Discounting theory with old adv calculation: [0.0, -0.00897, 0.0002, -0.01011, 0.00156, -0.00533, -0.00189, 0.0023, -0.01309, 0.00898, -0.03114, 0.01455, -0.05501, 0.02069, -0.08531, 0.02808, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.22206, -11.02792, 2.0162, -6.8895, -0.00189, 0.0023, -19.01601, 13.03805, -45.25418, 21.13972, -80.13652, 30.13507, -124.55118, 40.99409, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 79500
Discounting theory with old adv calculation: [0.0, -0.00897, 0.0002, -0.01011, 0.00156, -0.00533, -0.00189, 0.00231, -0.01309, 0.00898, -0.03115, 0.01455, -0.05501, 0.02068, -0.08532, 0.02808, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22157, -11.02605, 2.01527, -6.88829, -0.00189, 0.00231, -19.02206, 13.04035, -45.26323, 21.14067, -80.14724, 30.13431, -124.56186, 40.991, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 80000
Discounting theory with old adv calculation: [0.0, -0.00897, 0.0002, -0.01011, 0.00156, -0.00533, -0.00189, 0.00231, -0.0131, 0.00898, -0.03115, 0.01455, -0.05502, 0.02068, -0.08532, 0.02807, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22108, -11.02418, 2.01434, -6.88709, -0.00189, 0.00231, -19.02811, 13.04265, -45.27227, 21.14162, -80.15794, 30.13355, -124.5725, 40.9879, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 80500
Discounting theory with old adv calculation: [0.0, -0.00897, 0.0002, -0.01011, 0.00156, -0.00533, -0.00189, 0.00231, -0.0131, 0.00898, -0.03116, 0.01455, -0.05502, 0.02068, -0.08532, 0.02807, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22059, -11.02231, 2.01341, -6.88588, -0.00189, 0.00231, -19.03413, 13.04494, -45.28128, 21.14256, -80.16861, 30.13278, -124.58311, 40.98481, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 81000
Discounting theory with old adv calculation: [0.0, -0.00897, 0.0002, -0.01011, 0.00156, -0.00533, -0.0019, 0.00231, -0.01311, 0.00898, -0.03116, 0.01455, -0.05503, 0.02068, -0.08533, 0.02807, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22011, -11.02045, 2.01249, -6.88468, -0.0019, 0.00231, -19.04015, 13.04722, -45.29028, 21.1435, -80.17925, 30.13202, -124.59368, 40.98173, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 81500
Discounting theory with old adv calculation: [0.0, -0.00897, 0.0002, -0.01011, 0.00156, -0.00533, -0.0019, 0.00231, -0.01311, 0.00898, -0.03117, 0.01455, -0.05503, 0.02068, -0.08533, 0.02806, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21963, -11.01858, 2.01157, -6.88348, -0.0019, 0.00231, -19.04615, 13.0495, -45.29925, 21.14443, -80.18985, 30.13125, -124.60422, 40.97864, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 82000
Discounting theory with old adv calculation: [0.0, -0.00897, 0.0002, -0.01011, 0.00156, -0.00533, -0.0019, 0.00232, -0.01311, 0.00898, -0.03117, 0.01455, -0.05504, 0.02068, -0.08533, 0.02806, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21915, -11.01672, 2.01065, -6.88227, -0.0019, 0.00232, -19.05214, 13.05177, -45.30819, 21.14536, -80.20043, 30.13048, -124.61473, 40.97556, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 82500
Discounting theory with old adv calculation: [0.0, -0.00896, 0.0002, -0.01011, 0.00156, -0.00533, -0.0019, 0.00232, -0.01312, 0.00898, -0.03118, 0.01455, -0.05504, 0.02068, -0.08534, 0.02806, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21867, -11.01486, 2.00973, -6.88107, -0.0019, 0.00232, -19.05812, 13.05403, -45.31712, 21.14629, -80.21098, 30.12971, -124.6252, 40.97248, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 83000
Discounting theory with old adv calculation: [0.0, -0.00896, 0.0002, -0.01011, 0.00156, -0.00533, -0.0019, 0.00232, -0.01312, 0.00898, -0.03118, 0.01455, -0.05505, 0.02067, -0.08534, 0.02805, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21819, -11.01301, 2.00882, -6.87987, -0.0019, 0.00232, -19.06408, 13.05629, -45.32602, 21.14721, -80.22151, 30.12894, -124.63565, 40.96941, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 83500
Discounting theory with old adv calculation: [0.0, -0.00896, 0.0002, -0.01011, 0.00155, -0.00533, -0.0019, 0.00232, -0.01312, 0.00899, -0.03119, 0.01455, -0.05505, 0.02067, -0.08534, 0.02805, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21772, -11.01115, 2.0079, -6.87867, -0.0019, 0.00232, -19.07003, 13.05854, -45.33491, 21.14812, -80.232, 30.12817, -124.64605, 40.96633, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 84000
Discounting theory with old adv calculation: [0.0, -0.00896, 0.0002, -0.01011, 0.00155, -0.00533, -0.00191, 0.00232, -0.01313, 0.00899, -0.03119, 0.01455, -0.05506, 0.02067, -0.08535, 0.02805, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21725, -11.0093, 2.00699, -6.87747, -0.00191, 0.00232, -19.07597, 13.06079, -45.34377, 21.14904, -80.24246, 30.12739, -124.65643, 40.96326, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 84500
Discounting theory with old adv calculation: [0.0, -0.00896, 0.0002, -0.0101, 0.00155, -0.00532, -0.00191, 0.00233, -0.01313, 0.00899, -0.0312, 0.01455, -0.05506, 0.02067, -0.08535, 0.02804, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21678, -11.00745, 2.00608, -6.87628, -0.00191, 0.00233, -19.08189, 13.06303, -45.35261, 21.14994, -80.2529, 30.12661, -124.66677, 40.9602, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 85000
Discounting theory with old adv calculation: [0.0, -0.00896, 0.0002, -0.0101, 0.00155, -0.00532, -0.00191, 0.00233, -0.01313, 0.00899, -0.0312, 0.01455, -0.05507, 0.02067, -0.08535, 0.02804, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21631, -11.0056, 2.00517, -6.87508, -0.00191, 0.00233, -19.0878, 13.06527, -45.36142, 21.15085, -80.26331, 30.12584, -124.67709, 40.95713, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 85500
Discounting theory with old adv calculation: [0.0, -0.00896, 0.0002, -0.0101, 0.00155, -0.00532, -0.00191, 0.00233, -0.01314, 0.00899, -0.03121, 0.01455, -0.05507, 0.02067, -0.08536, 0.02804, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21584, -11.00375, 2.00427, -6.87389, -0.00191, 0.00233, -19.0937, 13.06749, -45.37022, 21.15175, -80.27369, 30.12506, -124.68737, 40.95407, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 86000
Discounting theory with old adv calculation: [0.0, -0.00896, 0.0002, -0.0101, 0.00155, -0.00532, -0.00191, 0.00233, -0.01314, 0.00899, -0.03121, 0.01455, -0.05508, 0.02067, -0.08536, 0.02803, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21538, -11.0019, 2.00336, -6.87269, -0.00191, 0.00233, -19.09959, 13.06972, -45.379, 21.15264, -80.28404, 30.12428, -124.69761, 40.95101, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 86500
Discounting theory with old adv calculation: [0.0, -0.00896, 0.0002, -0.0101, 0.00155, -0.00532, -0.00191, 0.00233, -0.01314, 0.00899, -0.03121, 0.01455, -0.05508, 0.02066, -0.08536, 0.02803, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21491, -11.00006, 2.00246, -6.8715, -0.00191, 0.00233, -19.10546, 13.07193, -45.38775, 21.15353, -80.29436, 30.12349, -124.70783, 40.94795, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 87000
Discounting theory with old adv calculation: [0.0, -0.00896, 0.0002, -0.0101, 0.00155, -0.00532, -0.00191, 0.00233, -0.01315, 0.00899, -0.03122, 0.01455, -0.05508, 0.02066, -0.08537, 0.02803, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21445, -10.99822, 2.00156, -6.87031, -0.00191, 0.00233, -19.11133, 13.07414, -45.39648, 21.15442, -80.30466, 30.12271, -124.71801, 40.9449, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 87500
Discounting theory with old adv calculation: [0.0, -0.00896, 0.0002, -0.0101, 0.00155, -0.00532, -0.00192, 0.00234, -0.01315, 0.009, -0.03122, 0.01455, -0.05509, 0.02066, -0.08537, 0.02802, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21399, -10.99638, 2.00066, -6.86912, -0.00192, 0.00234, -19.11717, 13.07635, -45.4052, 21.15531, -80.31493, 30.12192, -124.72817, 40.94184, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 88000
Discounting theory with old adv calculation: [0.0, -0.00896, 0.0002, -0.0101, 0.00155, -0.00532, -0.00192, 0.00234, -0.01315, 0.009, -0.03123, 0.01455, -0.05509, 0.02066, -0.08537, 0.02802, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21354, -10.99454, 1.99977, -6.86793, -0.00192, 0.00234, -19.12301, 13.07854, -45.41389, 21.15618, -80.32517, 30.12114, -124.73829, 40.93879, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 88500
Discounting theory with old adv calculation: [0.0, -0.00896, 0.0002, -0.0101, 0.00155, -0.00532, -0.00192, 0.00234, -0.01316, 0.009, -0.03123, 0.01455, -0.0551, 0.02066, -0.08538, 0.02802, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21308, -10.9927, 1.99887, -6.86674, -0.00192, 0.00234, -19.12884, 13.08074, -45.42256, 21.15706, -80.33539, 30.12035, -124.74838, 40.93575, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 89000
Discounting theory with old adv calculation: [0.0, -0.00896, 0.0002, -0.0101, 0.00155, -0.00532, -0.00192, 0.00234, -0.01316, 0.009, -0.03124, 0.01455, -0.0551, 0.02066, -0.08538, 0.02801, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21263, -10.99087, 1.99798, -6.86555, -0.00192, 0.00234, -19.13465, 13.08292, -45.43121, 21.15793, -80.34558, 30.11956, -124.75844, 40.9327, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 89500
Discounting theory with old adv calculation: [0.0, -0.00896, 0.0002, -0.0101, 0.00155, -0.00532, -0.00192, 0.00234, -0.01316, 0.009, -0.03124, 0.01455, -0.05511, 0.02066, -0.08538, 0.02801, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21218, -10.98903, 1.99709, -6.86437, -0.00192, 0.00234, -19.14045, 13.0851, -45.43984, 21.1588, -80.35574, 30.11877, -124.76847, 40.92966, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 90000
Discounting theory with old adv calculation: [0.0, -0.00896, 0.00019, -0.0101, 0.00155, -0.00532, -0.00192, 0.00235, -0.01317, 0.009, -0.03125, 0.01455, -0.05511, 0.02065, -0.08539, 0.02801, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21173, -10.9872, 1.9962, -6.86318, -0.00192, 0.00235, -19.14623, 13.08728, -45.44845, 21.15966, -80.36587, 30.11798, -124.77846, 40.92662, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 90500
Discounting theory with old adv calculation: [0.0, -0.00896, 0.00019, -0.0101, 0.00155, -0.00531, -0.00193, 0.00235, -0.01317, 0.009, -0.03125, 0.01455, -0.05512, 0.02065, -0.08539, 0.028, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21128, -10.98537, 1.99532, -6.862, -0.00193, 0.00235, -19.15201, 13.08945, -45.45704, 21.16052, -80.37598, 30.11719, -124.78843, 40.92359, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 91000
Discounting theory with old adv calculation: [0.0, -0.00896, 0.00019, -0.0101, 0.00154, -0.00531, -0.00193, 0.00235, -0.01317, 0.009, -0.03126, 0.01455, -0.05512, 0.02065, -0.08539, 0.028, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21084, -10.98355, 1.99443, -6.86081, -0.00193, 0.00235, -19.15777, 13.09161, -45.46561, 21.16138, -80.38606, 30.11639, -124.79837, 40.92056, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 91500
Discounting theory with old adv calculation: [0.0, -0.00896, 0.00019, -0.0101, 0.00154, -0.00531, -0.00193, 0.00235, -0.01318, 0.009, -0.03126, 0.01455, -0.05513, 0.02065, -0.0854, 0.028, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21039, -10.98172, 1.99355, -6.85963, -0.00193, 0.00235, -19.16352, 13.09377, -45.47416, 21.16223, -80.39611, 30.1156, -124.80828, 40.91753, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 92000
Discounting theory with old adv calculation: [0.0, -0.00896, 0.00019, -0.0101, 0.00154, -0.00531, -0.00193, 0.00235, -0.01318, 0.00901, -0.03127, 0.01455, -0.05513, 0.02065, -0.0854, 0.02799, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.20995, -10.9799, 1.99267, -6.85845, -0.00193, 0.00235, -19.16926, 13.09592, -45.48269, 21.16307, -80.40614, 30.1148, -124.81815, 40.9145, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 92500
Discounting theory with old adv calculation: [0.0, -0.00896, 0.00019, -0.0101, 0.00154, -0.00531, -0.00193, 0.00236, -0.01318, 0.00901, -0.03127, 0.01455, -0.05514, 0.02065, -0.0854, 0.02799, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.20951, -10.97808, 1.99179, -6.85727, -0.00193, 0.00236, -19.17499, 13.09807, -45.4912, 21.16392, -80.41614, 30.114, -124.828, 40.91147, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 93000
Discounting theory with old adv calculation: [0.0, -0.00896, 0.00019, -0.0101, 0.00154, -0.00531, -0.00193, 0.00236, -0.01319, 0.00901, -0.03127, 0.01455, -0.05514, 0.02065, -0.08541, 0.02799, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.20907, -10.97626, 1.99091, -6.85609, -0.00193, 0.00236, -19.18071, 13.10021, -45.49969, 21.16476, -80.42612, 30.11321, -124.83782, 40.90845, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 93500
Discounting theory with old adv calculation: [0.0, -0.00896, 0.00019, -0.0101, 0.00154, -0.00531, -0.00193, 0.00236, -0.01319, 0.00901, -0.03128, 0.01455, -0.05514, 0.02064, -0.08541, 0.02798, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.20863, -10.97444, 1.99004, -6.85491, -0.00193, 0.00236, -19.18641, 13.10234, -45.50816, 21.16559, -80.43607, 30.11241, -124.84761, 40.90543, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 94000
Discounting theory with old adv calculation: [0.0, -0.00896, 0.00019, -0.0101, 0.00154, -0.00531, -0.00194, 0.00236, -0.0132, 0.00901, -0.03128, 0.01455, -0.05515, 0.02064, -0.08541, 0.02798, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.2082, -10.97262, 1.98916, -6.85373, -0.00194, 0.00236, -19.1921, 13.10447, -45.51661, 21.16643, -80.44599, 30.11161, -124.85737, 40.90241, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 94500
Discounting theory with old adv calculation: [0.0, -0.00896, 0.00019, -0.0101, 0.00154, -0.00531, -0.00194, 0.00236, -0.0132, 0.00901, -0.03129, 0.01455, -0.05515, 0.02064, -0.08542, 0.02798, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.20777, -10.97081, 1.98829, -6.85256, -0.00194, 0.00236, -19.19778, 13.1066, -45.52504, 21.16726, -80.45589, 30.1108, -124.8671, 40.8994, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 95000
Discounting theory with old adv calculation: [0.0, -0.00896, 0.00019, -0.0101, 0.00154, -0.00531, -0.00194, 0.00236, -0.0132, 0.00901, -0.03129, 0.01455, -0.05516, 0.02064, -0.08542, 0.02797, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.20734, -10.969, 1.98742, -6.85138, -0.00194, 0.00236, -19.20345, 13.10872, -45.53345, 21.16808, -80.46576, 30.11, -124.8768, 40.89639, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 95500
Discounting theory with old adv calculation: [0.0, -0.00895, 0.00019, -0.0101, 0.00154, -0.00531, -0.00194, 0.00237, -0.01321, 0.00901, -0.0313, 0.01455, -0.05516, 0.02064, -0.08542, 0.02797, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.20691, -10.96719, 1.98656, -6.85021, -0.00194, 0.00237, -19.20911, 13.11083, -45.54184, 21.1689, -80.47561, 30.1092, -124.88647, 40.89338, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 96000
Discounting theory with old adv calculation: [0.0, -0.00895, 0.00019, -0.0101, 0.00154, -0.00531, -0.00194, 0.00237, -0.01321, 0.00901, -0.0313, 0.01455, -0.05517, 0.02064, -0.08542, 0.02797, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.20648, -10.96538, 1.98569, -6.84904, -0.00194, 0.00237, -19.21475, 13.11294, -45.55021, 21.16972, -80.48543, 30.10839, -124.89611, 40.89038, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 96500
Discounting theory with old adv calculation: [0.0, -0.00895, 0.00019, -0.0101, 0.00154, -0.0053, -0.00194, 0.00237, -0.01321, 0.00902, -0.03131, 0.01455, -0.05517, 0.02064, -0.08543, 0.02796, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.20605, -10.96358, 1.98482, -6.84786, -0.00194, 0.00237, -19.22038, 13.11504, -45.55857, 21.17053, -80.49523, 30.10758, -124.90573, 40.88737, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 97000
Discounting theory with old adv calculation: [0.0, -0.00895, 0.00019, -0.0101, 0.00154, -0.0053, -0.00195, 0.00237, -0.01322, 0.00902, -0.03131, 0.01455, -0.05518, 0.02063, -0.08543, 0.02796, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.20563, -10.96177, 1.98396, -6.84669, -0.00195, 0.00237, -19.22601, 13.11714, -45.5669, 21.17134, -80.505, 30.10678, -124.91531, 40.88437, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 97500
Discounting theory with old adv calculation: [0.0, -0.00895, 0.00019, -0.0101, 0.00154, -0.0053, -0.00195, 0.00237, -0.01322, 0.00902, -0.03132, 0.01455, -0.05518, 0.02063, -0.08543, 0.02796, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.2052, -10.95997, 1.9831, -6.84552, -0.00195, 0.00237, -19.23162, 13.11923, -45.57522, 21.17215, -80.51475, 30.10597, -124.92487, 40.88138, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 98000
Discounting theory with old adv calculation: [0.0, -0.00895, 0.00019, -0.0101, 0.00154, -0.0053, -0.00195, 0.00238, -0.01322, 0.00902, -0.03132, 0.01455, -0.05519, 0.02063, -0.08544, 0.02796, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.20478, -10.95817, 1.98224, -6.84435, -0.00195, 0.00238, -19.23722, 13.12132, -45.58351, 21.17295, -80.52447, 30.10516, -124.9344, 40.87838, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 98500
Discounting theory with old adv calculation: [0.0, -0.00895, 0.00019, -0.0101, 0.00153, -0.0053, -0.00195, 0.00238, -0.01323, 0.00902, -0.03132, 0.01455, -0.05519, 0.02063, -0.08544, 0.02795, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.20436, -10.95637, 1.98139, -6.84319, -0.00195, 0.00238, -19.2428, 13.1234, -45.59179, 21.17375, -80.53417, 30.10435, -124.9439, 40.87539, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 99000
Discounting theory with old adv calculation: [0.0, -0.00895, 0.00019, -0.0101, 0.00153, -0.0053, -0.00195, 0.00238, -0.01323, 0.00902, -0.03133, 0.01455, -0.05519, 0.02063, -0.08544, 0.02795, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.20395, -10.95457, 1.98053, -6.84202, -0.00195, 0.00238, -19.24838, 13.12547, -45.60005, 21.17455, -80.54384, 30.10354, -124.95338, 40.8724, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 99500
Discounting theory with old adv calculation: [0.0, -0.00895, 0.00019, -0.0101, 0.00153, -0.0053, -0.00195, 0.00238, -0.01323, 0.00902, -0.03133, 0.01455, -0.0552, 0.02063, -0.08545, 0.02795, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 1 1 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.20353, -10.95278, 1.97968, -6.84085, -0.00195, 0.00238, -19.25395, 13.12754, -45.60829, 21.17534, -80.55349, 30.10272, -124.96282, 40.86941, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 0
Discounting theory with old adv calculation: [0.07752, 0.06111, 0.07703, 0.0616, 0.07605, 0.06258, 0.07407, 0.06456, 0.07006, 0.06856, 0.06198, 0.07665, 0.04563, 0.093, 0.01262, 0.12601, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [0 0 0 0 0 1 1 1 0 0]
Discounting theory with new adv calculation: [0.0082, -0.0082, 0.00772, -0.00772, 0.00674, -0.00674, 0.00475, -0.00475, 0.00075, -0.00075, -0.00734, 0.00734, -0.02368, 0.02368, -0.0567, 0.0567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 0 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 500
Discounting theory with old adv calculation: [0.0, 1.30893, 0.00324, 0.34413, 0.02867, 0.13874, 0.05829, 0.08167, 0.07596, 0.06206, 0.08221, 0.05382, 0.07345, 0.05372, 0.05262, 0.05796, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.33788, -11.34052, 2.19459, -7.09015, -0.00159, 0.00194, -17.84767, 12.56873, -43.45739, 20.90815, -77.94528, 30.21379, -122.27568, 41.50256, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 1000
Discounting theory with old adv calculation: [0.0, 1.30893, 0.00323, 0.34443, 0.02865, 0.13878, 0.05829, 0.08167, 0.07597, 0.06205, 0.08222, 0.05382, 0.07346, 0.05371, 0.05263, 0.05795, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.33678, -11.33847, 2.19319, -7.08882, -0.00159, 0.00194, -17.85692, 12.57265, -43.47202, 20.91038, -77.96365, 30.21374, -122.29552, 41.4993, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 1500
Discounting theory with old adv calculation: [0.0, 1.30893, 0.00322, 0.34473, 0.02864, 0.13882, 0.05829, 0.08167, 0.07597, 0.06204, 0.08223, 0.05381, 0.07347, 0.05371, 0.05264, 0.05794, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.33569, -11.33642, 2.1918, -7.0875, -0.00159, 0.00194, -17.86615, 12.57655, -43.4866, 20.9126, -77.98194, 30.21369, -122.31526, 41.49605, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 2000
Discounting theory with old adv calculation: [0.0, 1.30893, 0.00321, 0.34503, 0.02863, 0.13885, 0.05829, 0.08168, 0.07598, 0.06204, 0.08224, 0.0538, 0.07348, 0.0537, 0.05266, 0.05792, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.3346, -11.33437, 2.19041, -7.08618, -0.0016, 0.00195, -17.87534, 12.58043, -43.50113, 20.91481, -78.00015, 30.21363, -122.3349, 41.49278, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 2500
Discounting theory with old adv calculation: [0.0, 1.30893, 0.0032, 0.34532, 0.02862, 0.13889, 0.05828, 0.08168, 0.07599, 0.06203, 0.08225, 0.05379, 0.07349, 0.05369, 0.05267, 0.05791, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.33352, -11.33232, 2.18903, -7.08486, -0.0016, 0.00195, -17.88451, 12.5843, -43.5156, 20.917, -78.01829, 30.21356, -122.35445, 41.48952, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 3000
Discounting theory with old adv calculation: [0.0, 1.30893, 0.00319, 0.34562, 0.02861, 0.13892, 0.05828, 0.08168, 0.07599, 0.06202, 0.08226, 0.05379, 0.0735, 0.05368, 0.05268, 0.0579, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.33245, -11.33027, 2.18765, -7.08354, -0.0016, 0.00195, -17.89364, 12.58816, -43.53001, 20.91918, -78.03635, 30.21348, -122.3739, 41.48626, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 3500
Discounting theory with old adv calculation: [0.0, 1.30893, 0.00318, 0.34591, 0.0286, 0.13896, 0.05828, 0.08168, 0.076, 0.06202, 0.08226, 0.05378, 0.07351, 0.05367, 0.05269, 0.05789, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.33139, -11.32822, 2.18627, -7.08222, -0.0016, 0.00195, -17.90274, 12.59199, -43.54438, 20.92134, -78.05433, 30.21339, -122.39326, 41.48299, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 4000
Discounting theory with old adv calculation: [0.0, 1.30893, 0.00317, 0.34621, 0.02859, 0.13899, 0.05828, 0.08169, 0.07601, 0.06201, 0.08227, 0.05377, 0.07352, 0.05366, 0.05271, 0.05788, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.33033, -11.32617, 2.1849, -7.08089, -0.00161, 0.00196, -17.91181, 12.59582, -43.55868, 20.92349, -78.07224, 30.21329, -122.41252, 41.47972, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 4500
Discounting theory with old adv calculation: [0.0, 1.30893, 0.00316, 0.3465, 0.02857, 0.13903, 0.05828, 0.08169, 0.07601, 0.062, 0.08228, 0.05377, 0.07353, 0.05365, 0.05272, 0.05787, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.32927, -11.32411, 2.18354, -7.07957, -0.00161, 0.00196, -17.92086, 12.59962, -43.57294, 20.92563, -78.09008, 30.21318, -122.4317, 41.47644, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 5000
Discounting theory with old adv calculation: [0.0, 1.30894, 0.00315, 0.34679, 0.02856, 0.13906, 0.05827, 0.08169, 0.07602, 0.062, 0.08229, 0.05376, 0.07354, 0.05365, 0.05273, 0.05786, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.32823, -11.32206, 2.18217, -7.07825, -0.00161, 0.00196, -17.92987, 12.60341, -43.58714, 20.92775, -78.10783, 30.21306, -122.45078, 41.47317, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 5500
Discounting theory with old adv calculation: [0.0, 1.30894, 0.00314, 0.34708, 0.02855, 0.1391, 0.05827, 0.0817, 0.07603, 0.06199, 0.0823, 0.05375, 0.07355, 0.05364, 0.05274, 0.05785, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.32719, -11.32, 2.18081, -7.07693, -0.00161, 0.00197, -17.93885, 12.60719, -43.60129, 20.92986, -78.12552, 30.21293, -122.46977, 41.46989, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 6000
Discounting theory with old adv calculation: [0.0, 1.30894, 0.00313, 0.34737, 0.02854, 0.13913, 0.05827, 0.0817, 0.07603, 0.06198, 0.0823, 0.05374, 0.07356, 0.05363, 0.05276, 0.05784, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.32616, -11.31795, 2.17946, -7.07561, -0.00161, 0.00197, -17.94781, 12.61095, -43.61539, 20.93196, -78.14313, 30.2128, -122.48867, 41.46661, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 6500
Discounting theory with old adv calculation: [0.0, 1.30894, 0.00312, 0.34766, 0.02853, 0.13916, 0.05827, 0.0817, 0.07604, 0.06198, 0.08231, 0.05374, 0.07356, 0.05362, 0.05277, 0.05783, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.32513, -11.3159, 2.17811, -7.07429, -0.00162, 0.00197, -17.95674, 12.61469, -43.62944, 20.93404, -78.16067, 30.21265, -122.50748, 41.46333, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 7000
Discounting theory with old adv calculation: [0.0, 1.30894, 0.00312, 0.34795, 0.02852, 0.1392, 0.05826, 0.0817, 0.07605, 0.06197, 0.08232, 0.05373, 0.07357, 0.05361, 0.05278, 0.05781, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.32411, -11.31384, 2.17676, -7.07297, -0.00162, 0.00197, -17.96563, 12.61842, -43.64343, 20.93611, -78.17814, 30.2125, -122.52621, 41.46005, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 7500
Discounting theory with old adv calculation: [0.0, 1.30894, 0.00311, 0.34824, 0.02851, 0.13923, 0.05826, 0.08171, 0.07605, 0.06196, 0.08233, 0.05372, 0.07358, 0.0536, 0.05279, 0.0578, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.32309, -11.31179, 2.17542, -7.07164, -0.00162, 0.00198, -17.9745, 12.62213, -43.65737, 20.93817, -78.19553, 30.21234, -122.54484, 41.45677, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 8000
Discounting theory with old adv calculation: [0.0, 1.30894, 0.0031, 0.34853, 0.02849, 0.13927, 0.05826, 0.08171, 0.07606, 0.06196, 0.08234, 0.05372, 0.07359, 0.05359, 0.05281, 0.05779, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.32208, -11.30973, 2.17408, -7.07032, -0.00162, 0.00198, -17.98334, 12.62583, -43.67126, 20.94022, -78.21286, 30.21217, -122.56339, 41.45348, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 8500
Discounting theory with old adv calculation: [0.0, 1.30894, 0.00309, 0.34881, 0.02848, 0.1393, 0.05826, 0.08171, 0.07607, 0.06195, 0.08235, 0.05371, 0.0736, 0.05359, 0.05282, 0.05778, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.32108, -11.30768, 2.17274, -7.069, -0.00163, 0.00198, -17.99215, 12.62952, -43.68511, 20.94225, -78.23011, 30.21199, -122.58185, 41.4502, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 9000
Discounting theory with old adv calculation: [0.0, 1.30894, 0.00308, 0.3491, 0.02847, 0.13933, 0.05826, 0.08171, 0.07607, 0.06194, 0.08235, 0.0537, 0.07361, 0.05358, 0.05283, 0.05777, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.32008, -11.30562, 2.17141, -7.06768, -0.00163, 0.00198, -18.00094, 12.63319, -43.6989, 20.94427, -78.24729, 30.2118, -122.60022, 41.44691, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 9500
Discounting theory with old adv calculation: [0.0, 1.30894, 0.00307, 0.34938, 0.02846, 0.13937, 0.05825, 0.08172, 0.07608, 0.06194, 0.08236, 0.05369, 0.07362, 0.05357, 0.05284, 0.05776, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.31909, -11.30356, 2.17009, -7.06636, -0.00163, 0.00199, -18.00969, 12.63684, -43.71264, 20.94628, -78.26441, 30.21161, -122.61851, 41.44362, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 10000
Discounting theory with old adv calculation: [0.0, 1.30894, 0.00306, 0.34966, 0.02845, 0.1394, 0.05825, 0.08172, 0.07609, 0.06193, 0.08237, 0.05369, 0.07363, 0.05356, 0.05285, 0.05775, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.31811, -11.30151, 2.16876, -7.06504, -0.00163, 0.00199, -18.01842, 12.64049, -43.72633, 20.94827, -78.28145, 30.21141, -122.63672, 41.44033, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 10500
Discounting theory with old adv calculation: [0.0, 1.30894, 0.00305, 0.34995, 0.02844, 0.13943, 0.05825, 0.08172, 0.07609, 0.06193, 0.08238, 0.05368, 0.07364, 0.05355, 0.05287, 0.05774, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.31713, -11.29945, 2.16744, -7.06372, -0.00163, 0.00199, -18.02712, 12.64411, -43.73998, 20.95026, -78.29843, 30.21119, -122.65484, 41.43704, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 11000
Discounting theory with old adv calculation: [0.0, 1.30895, 0.00304, 0.35023, 0.02843, 0.13947, 0.05825, 0.08173, 0.0761, 0.06192, 0.08239, 0.05367, 0.07365, 0.05354, 0.05288, 0.05773, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.31615, -11.2974, 2.16613, -7.0624, -0.00164, 0.002, -18.0358, 12.64772, -43.75357, 20.95223, -78.31534, 30.21098, -122.67287, 41.43375, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 11500
Discounting theory with old adv calculation: [0.0, 1.30895, 0.00304, 0.35051, 0.02842, 0.1395, 0.05824, 0.08173, 0.07611, 0.06191, 0.08239, 0.05367, 0.07366, 0.05354, 0.05289, 0.05772, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.31519, -11.29534, 2.16482, -7.06108, -0.00164, 0.002, -18.04444, 12.65132, -43.76712, 20.95419, -78.33218, 30.21075, -122.69083, 41.43045, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 12000
Discounting theory with old adv calculation: [0.0, 1.30895, 0.00303, 0.35079, 0.02841, 0.13953, 0.05824, 0.08173, 0.07611, 0.06191, 0.0824, 0.05366, 0.07367, 0.05353, 0.0529, 0.05771, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.31422, -11.29329, 2.16351, -7.05977, -0.00164, 0.002, -18.05306, 12.65491, -43.78062, 20.95614, -78.34895, 30.21051, -122.7087, 41.42716, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 12500
Discounting theory with old adv calculation: [0.0, 1.30895, 0.00302, 0.35107, 0.0284, 0.13957, 0.05824, 0.08173, 0.07612, 0.0619, 0.08241, 0.05365, 0.07368, 0.05352, 0.05291, 0.0577, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.31327, -11.29123, 2.1622, -7.05845, -0.00164, 0.002, -18.06165, 12.65848, -43.79407, 20.95808, -78.36566, 30.21027, -122.72649, 41.42386, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 13000
Discounting theory with old adv calculation: [0.0, 1.30895, 0.00301, 0.35135, 0.02839, 0.1396, 0.05824, 0.08174, 0.07613, 0.06189, 0.08242, 0.05365, 0.07369, 0.05351, 0.05293, 0.05769, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.31232, -11.28918, 2.1609, -7.05713, -0.00165, 0.00201, -18.07022, 12.66203, -43.80747, 20.96, -78.3823, 30.21002, -122.7442, 41.42057, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 13500
Discounting theory with old adv calculation: [0.0, 1.30895, 0.003, 0.35162, 0.02837, 0.13963, 0.05824, 0.08174, 0.07613, 0.06189, 0.08242, 0.05364, 0.07369, 0.0535, 0.05294, 0.05768, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.31137, -11.28713, 2.15961, -7.05581, -0.00165, 0.00201, -18.07876, 12.66558, -43.82083, 20.96191, -78.39888, 30.20977, -122.76183, 41.41727, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 14000
Discounting theory with old adv calculation: [0.0, 1.30895, 0.00299, 0.3519, 0.02836, 0.13966, 0.05823, 0.08174, 0.07614, 0.06188, 0.08243, 0.05363, 0.0737, 0.0535, 0.05295, 0.05767, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.31043, -11.28507, 2.15831, -7.05449, -0.00165, 0.00201, -18.08727, 12.66911, -43.83414, 20.96382, -78.41539, 30.2095, -122.77938, 41.41397, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 14500
Discounting theory with old adv calculation: [0.0, 1.30895, 0.00298, 0.35218, 0.02835, 0.1397, 0.05823, 0.08174, 0.07615, 0.06188, 0.08244, 0.05363, 0.07371, 0.05349, 0.05296, 0.05766, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.30949, -11.28302, 2.15703, -7.05318, -0.00165, 0.00201, -18.09576, 12.67262, -43.8474, 20.96571, -78.43183, 30.20923, -122.79685, 41.41068, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 15000
Discounting theory with old adv calculation: [0.0, 1.30895, 0.00298, 0.35245, 0.02834, 0.13973, 0.05823, 0.08175, 0.07615, 0.06187, 0.08245, 0.05362, 0.07372, 0.05348, 0.05297, 0.05765, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.30857, -11.28097, 2.15574, -7.05186, -0.00165, 0.00202, -18.10422, 12.67612, -43.86062, 20.96759, -78.44821, 30.20895, -122.81424, 41.40738, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 15500
Discounting theory with old adv calculation: [0.0, 1.30895, 0.00297, 0.35273, 0.02833, 0.13976, 0.05823, 0.08175, 0.07616, 0.06186, 0.08246, 0.05361, 0.07373, 0.05347, 0.05299, 0.05764, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.30764, -11.27891, 2.15446, -7.05054, -0.00166, 0.00202, -18.11265, 12.67961, -43.87379, 20.96945, -78.46453, 30.20867, -122.83156, 41.40408, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 16000
Discounting theory with old adv calculation: [0.0, 1.30895, 0.00296, 0.353, 0.02832, 0.13979, 0.05823, 0.08175, 0.07617, 0.06186, 0.08246, 0.05361, 0.07374, 0.05346, 0.053, 0.05763, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.30672, -11.27686, 2.15318, -7.04923, -0.00166, 0.00202, -18.12106, 12.68309, -43.88691, 20.97131, -78.48078, 30.20838, -122.8488, 41.40078, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 16500
Discounting theory with old adv calculation: [0.0, 1.30895, 0.00295, 0.35327, 0.02831, 0.13983, 0.05822, 0.08175, 0.07617, 0.06185, 0.08247, 0.0536, 0.07375, 0.05346, 0.05301, 0.05761, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.30581, -11.27481, 2.15191, -7.04791, -0.00166, 0.00202, -18.12944, 12.68655, -43.89999, 20.97316, -78.49697, 30.20808, -122.86596, 41.39748, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 17000
Discounting theory with old adv calculation: [0.0, 1.30896, 0.00294, 0.35354, 0.0283, 0.13986, 0.05822, 0.08176, 0.07618, 0.06184, 0.08248, 0.05359, 0.07376, 0.05345, 0.05302, 0.0576, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.3049, -11.27276, 2.15063, -7.0466, -0.00166, 0.00203, -18.1378, 12.69, -43.91303, 20.97499, -78.5131, 30.20777, -122.88304, 41.39418, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 17500
Discounting theory with old adv calculation: [0.0, 1.30896, 0.00293, 0.35382, 0.02829, 0.13989, 0.05822, 0.08176, 0.07619, 0.06184, 0.08249, 0.05359, 0.07377, 0.05344, 0.05303, 0.05759, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.304, -11.27071, 2.14937, -7.04528, -0.00166, 0.00203, -18.14613, 12.69344, -43.92602, 20.97682, -78.52917, 30.20746, -122.90005, 41.39088, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 18000
Discounting theory with old adv calculation: [0.0, 1.30896, 0.00293, 0.35409, 0.02828, 0.13992, 0.05822, 0.08176, 0.07619, 0.06183, 0.08249, 0.05358, 0.07378, 0.05343, 0.05304, 0.05758, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.3031, -11.26866, 2.1481, -7.04397, -0.00167, 0.00203, -18.15444, 12.69686, -43.93896, 20.97863, -78.54517, 30.20714, -122.91698, 41.38758, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 18500
Discounting theory with old adv calculation: [0.0, 1.30896, 0.00292, 0.35436, 0.02827, 0.13995, 0.05821, 0.08176, 0.0762, 0.06183, 0.0825, 0.05357, 0.07379, 0.05342, 0.05306, 0.05757, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.30221, -11.26661, 2.14684, -7.04266, -0.00167, 0.00203, -18.16272, 12.70027, -43.95186, 20.98043, -78.56111, 30.20681, -122.93384, 41.38427, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 19000
Discounting theory with old adv calculation: [0.0, 1.30896, 0.00291, 0.35463, 0.02826, 0.13999, 0.05821, 0.08177, 0.0762, 0.06182, 0.08251, 0.05357, 0.07379, 0.05342, 0.05307, 0.05756, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.30132, -11.26456, 2.14559, -7.04134, -0.00167, 0.00204, -18.17098, 12.70367, -43.96472, 20.98223, -78.577, 30.20648, -122.95063, 41.38097, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 19500
Discounting theory with old adv calculation: [0.0, 1.30896, 0.0029, 0.35489, 0.02825, 0.14002, 0.05821, 0.08177, 0.07621, 0.06181, 0.08252, 0.05356, 0.0738, 0.05341, 0.05308, 0.05755, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.30044, -11.26251, 2.14433, -7.04003, -0.00167, 0.00204, -18.17921, 12.70705, -43.97753, 20.98401, -78.59282, 30.20614, -122.96734, 41.37767, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 20000
Discounting theory with old adv calculation: [0.0, 1.30896, 0.0029, 0.35516, 0.02824, 0.14005, 0.05821, 0.08177, 0.07622, 0.06181, 0.08252, 0.05355, 0.07381, 0.0534, 0.05309, 0.05754, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.29956, -11.26046, 2.14308, -7.03872, -0.00167, 0.00204, -18.18742, 12.71042, -43.9903, 20.98578, -78.60858, 30.20579, -122.98398, 41.37437, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 20500
Discounting theory with old adv calculation: [0.0, 1.30896, 0.00289, 0.35543, 0.02823, 0.14008, 0.05821, 0.08177, 0.07622, 0.0618, 0.08253, 0.05355, 0.07382, 0.05339, 0.0531, 0.05753, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.29868, -11.25841, 2.14184, -7.03741, -0.00168, 0.00205, -18.1956, 12.71378, -44.00303, 20.98754, -78.62428, 30.20544, -123.00054, 41.37107, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 21000
Discounting theory with old adv calculation: [0.0, 1.30896, 0.00288, 0.35569, 0.02822, 0.14011, 0.0582, 0.08178, 0.07623, 0.0618, 0.08254, 0.05354, 0.07383, 0.05338, 0.05311, 0.05752, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.29782, -11.25636, 2.1406, -7.0361, -0.00168, 0.00205, -18.20376, 12.71713, -44.01572, 20.98929, -78.63992, 30.20508, -123.01704, 41.36777, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 21500
Discounting theory with old adv calculation: [0.0, 1.30896, 0.00287, 0.35596, 0.02821, 0.14014, 0.0582, 0.08178, 0.07624, 0.06179, 0.08255, 0.05353, 0.07384, 0.05338, 0.05313, 0.05751, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.29695, -11.25432, 2.13936, -7.03479, -0.00168, 0.00205, -18.2119, 12.72046, -44.02836, 20.99103, -78.65551, 30.20472, -123.03346, 41.36447, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 22000
Discounting theory with old adv calculation: [0.0, 1.30896, 0.00286, 0.35622, 0.0282, 0.14017, 0.0582, 0.08178, 0.07624, 0.06178, 0.08255, 0.05353, 0.07385, 0.05337, 0.05314, 0.0575, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.29609, -11.25227, 2.13812, -7.03348, -0.00168, 0.00205, -18.22001, 12.72379, -44.04096, 20.99276, -78.67103, 30.20435, -123.04981, 41.36116, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 22500
Discounting theory with old adv calculation: [0.0, 1.30896, 0.00286, 0.35649, 0.02819, 0.1402, 0.0582, 0.08178, 0.07625, 0.06178, 0.08256, 0.05352, 0.07386, 0.05336, 0.05315, 0.05749, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.29524, -11.25023, 2.13689, -7.03217, -0.00169, 0.00206, -18.22809, 12.7271, -44.05352, 20.99449, -78.6865, 30.20397, -123.06609, 41.35786, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 23000
Discounting theory with old adv calculation: [0.0, 1.30896, 0.00285, 0.35675, 0.02818, 0.14024, 0.0582, 0.08179, 0.07625, 0.06177, 0.08257, 0.05351, 0.07386, 0.05335, 0.05316, 0.05748, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.29439, -11.24819, 2.13566, -7.03086, -0.00169, 0.00206, -18.23616, 12.73039, -44.06603, 20.9962, -78.70191, 30.20359, -123.08231, 41.35456, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 23500
Discounting theory with old adv calculation: [0.0, 1.30896, 0.00284, 0.35701, 0.02817, 0.14027, 0.05819, 0.08179, 0.07626, 0.06177, 0.08258, 0.05351, 0.07387, 0.05335, 0.05317, 0.05747, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.29355, -11.24614, 2.13443, -7.02955, -0.00169, 0.00206, -18.2442, 12.73368, -44.07851, 20.9979, -78.71726, 30.2032, -123.09845, 41.35126, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 24000
Discounting theory with old adv calculation: [0.0, 1.30897, 0.00283, 0.35728, 0.02816, 0.1403, 0.05819, 0.08179, 0.07627, 0.06176, 0.08258, 0.0535, 0.07388, 0.05334, 0.05318, 0.05747, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.29271, -11.2441, 2.13321, -7.02824, -0.00169, 0.00206, -18.25221, 12.73695, -44.09094, 20.99959, -78.73256, 30.2028, -123.11452, 41.34796, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 24500
Discounting theory with old adv calculation: [0.0, 1.30897, 0.00283, 0.35754, 0.02815, 0.14033, 0.05819, 0.08179, 0.07627, 0.06175, 0.08259, 0.05349, 0.07389, 0.05333, 0.05319, 0.05746, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.29187, -11.24206, 2.13199, -7.02694, -0.00169, 0.00207, -18.26021, 12.74022, -44.10333, 21.00127, -78.7478, 30.2024, -123.13053, 41.34466, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 25000
Discounting theory with old adv calculation: [0.0, 1.30897, 0.00282, 0.3578, 0.02814, 0.14036, 0.05819, 0.0818, 0.07628, 0.06175, 0.0826, 0.05349, 0.0739, 0.05332, 0.0532, 0.05745, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.29104, -11.24002, 2.13078, -7.02563, -0.0017, 0.00207, -18.26818, 12.74347, -44.11569, 21.00294, -78.76298, 30.202, -123.14647, 41.34137, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 25500
Discounting theory with old adv calculation: [0.0, 1.30897, 0.00281, 0.35806, 0.02813, 0.14039, 0.05819, 0.0818, 0.07628, 0.06174, 0.08261, 0.05348, 0.07391, 0.05332, 0.05322, 0.05744, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.29022, -11.23798, 2.12957, -7.02432, -0.0017, 0.00207, -18.27613, 12.74671, -44.128, 21.00461, -78.77811, 30.20158, -123.16234, 41.33807, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 26000
Discounting theory with old adv calculation: [0.0, 1.30897, 0.0028, 0.35832, 0.02812, 0.14042, 0.05818, 0.0818, 0.07629, 0.06174, 0.08261, 0.05347, 0.07392, 0.05331, 0.05323, 0.05743, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.28939, -11.23594, 2.12836, -7.02302, -0.0017, 0.00207, -18.28405, 12.74993, -44.14027, 21.00626, -78.79318, 30.20117, -123.17814, 41.33477, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 26500
Discounting theory with old adv calculation: [0.0, 1.30897, 0.0028, 0.35857, 0.02811, 0.14045, 0.05818, 0.0818, 0.0763, 0.06173, 0.08262, 0.05347, 0.07392, 0.0533, 0.05324, 0.05742, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.28858, -11.2339, 2.12715, -7.02172, -0.0017, 0.00208, -18.29195, 12.75315, -44.1525, 21.0079, -78.8082, 30.20074, -123.19388, 41.33147, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 27000
Discounting theory with old adv calculation: [0.0, 1.30897, 0.00279, 0.35883, 0.0281, 0.14048, 0.05818, 0.08181, 0.0763, 0.06173, 0.08263, 0.05346, 0.07393, 0.05329, 0.05325, 0.05741, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.28776, -11.23187, 2.12595, -7.02041, -0.0017, 0.00208, -18.29983, 12.75635, -44.16469, 21.00954, -78.82316, 30.20032, -123.20955, 41.32817, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 27500
Discounting theory with old adv calculation: [0.0, 1.30897, 0.00278, 0.35909, 0.02809, 0.14051, 0.05818, 0.08181, 0.07631, 0.06172, 0.08264, 0.05345, 0.07394, 0.05329, 0.05326, 0.0574, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.28696, -11.22983, 2.12475, -7.01911, -0.00171, 0.00208, -18.30769, 12.75954, -44.17684, 21.01116, -78.83807, 30.19988, -123.22516, 41.32488, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 28000
Discounting theory with old adv calculation: [0.0, 1.30897, 0.00277, 0.35934, 0.02808, 0.14054, 0.05818, 0.08181, 0.07632, 0.06171, 0.08264, 0.05345, 0.07395, 0.05328, 0.05327, 0.05739, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.28615, -11.2278, 2.12356, -7.01781, -0.00171, 0.00208, -18.31552, 12.76272, -44.18896, 21.01278, -78.85292, 30.19944, -123.2407, 41.32158, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 28500
Discounting theory with old adv calculation: [0.0, 1.30897, 0.00277, 0.3596, 0.02807, 0.14057, 0.05817, 0.08181, 0.07632, 0.06171, 0.08265, 0.05344, 0.07396, 0.05327, 0.05328, 0.05738, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.28535, -11.22576, 2.12236, -7.0165, -0.00171, 0.00208, -18.32333, 12.76589, -44.20103, 21.01439, -78.86772, 30.199, -123.25618, 41.31829, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 29000
Discounting theory with old adv calculation: [0.0, 1.30897, 0.00276, 0.35985, 0.02806, 0.1406, 0.05817, 0.08182, 0.07633, 0.0617, 0.08266, 0.05344, 0.07397, 0.05326, 0.05329, 0.05737, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.28456, -11.22373, 2.12117, -7.0152, -0.00171, 0.00209, -18.33112, 12.76905, -44.21307, 21.01598, -78.88247, 30.19855, -123.27159, 41.31499, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 29500
Discounting theory with old adv calculation: [0.0, 1.30897, 0.00275, 0.36011, 0.02805, 0.14063, 0.05817, 0.08182, 0.07633, 0.0617, 0.08266, 0.05343, 0.07398, 0.05326, 0.0533, 0.05736, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.28376, -11.2217, 2.11999, -7.0139, -0.00171, 0.00209, -18.33889, 12.7722, -44.22506, 21.01757, -78.89716, 30.1981, -123.28694, 41.3117, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 30000
Discounting theory with old adv calculation: [0.0, 1.30897, 0.00275, 0.36036, 0.02804, 0.14066, 0.05817, 0.08182, 0.07634, 0.06169, 0.08267, 0.05342, 0.07398, 0.05325, 0.05331, 0.05735, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.28298, -11.21967, 2.1188, -7.0126, -0.00172, 0.00209, -18.34663, 12.77534, -44.23702, 21.01915, -78.9118, 30.19764, -123.30222, 41.30841, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 30500
Discounting theory with old adv calculation: [0.0, 1.30897, 0.00274, 0.36062, 0.02803, 0.14069, 0.05817, 0.08182, 0.07634, 0.06169, 0.08268, 0.05342, 0.07399, 0.05324, 0.05333, 0.05734, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.2822, -11.21764, 2.11762, -7.0113, -0.00172, 0.00209, -18.35436, 12.77846, -44.24894, 21.02072, -78.92639, 30.19717, -123.31745, 41.30512, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 31000
Discounting theory with old adv calculation: [0.0, 1.30898, 0.00273, 0.36087, 0.02802, 0.14072, 0.05816, 0.08183, 0.07635, 0.06168, 0.08269, 0.05341, 0.074, 0.05323, 0.05334, 0.05733, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.28142, -11.21561, 2.11645, -7.01001, -0.00172, 0.0021, -18.36206, 12.78158, -44.26082, 21.02229, -78.94093, 30.1967, -123.33261, 41.30183, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 31500
Discounting theory with old adv calculation: [0.0, 1.30898, 0.00273, 0.36112, 0.02801, 0.14075, 0.05816, 0.08183, 0.07636, 0.06167, 0.08269, 0.0534, 0.07401, 0.05323, 0.05335, 0.05732, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.28064, -11.21358, 2.11527, -7.00871, -0.00172, 0.0021, -18.36974, 12.78468, -44.27267, 21.02384, -78.95541, 30.19623, -123.34771, 41.29854, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 32000
Discounting theory with old adv calculation: [0.0, 1.30898, 0.00272, 0.36137, 0.028, 0.14078, 0.05816, 0.08183, 0.07636, 0.06167, 0.0827, 0.0534, 0.07402, 0.05322, 0.05336, 0.05731, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.27987, -11.21156, 2.1141, -7.00741, -0.00172, 0.0021, -18.3774, 12.78777, -44.28448, 21.02539, -78.96985, 30.19575, -123.36275, 41.29525, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 32500
Discounting theory with old adv calculation: [0.0, 1.30898, 0.00271, 0.36162, 0.02799, 0.14081, 0.05816, 0.08183, 0.07637, 0.06166, 0.08271, 0.05339, 0.07403, 0.05321, 0.05337, 0.0573, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.27911, -11.20953, 2.11294, -7.00612, -0.00173, 0.0021, -18.38504, 12.79085, -44.29625, 21.02692, -78.98423, 30.19526, -123.37772, 41.29196, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 33000
Discounting theory with old adv calculation: [0.0, 1.30898, 0.0027, 0.36187, 0.02798, 0.14084, 0.05816, 0.08184, 0.07637, 0.06166, 0.08271, 0.05339, 0.07403, 0.0532, 0.05338, 0.05729, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.27834, -11.20751, 2.11177, -7.00482, -0.00173, 0.00211, -18.39265, 12.79392, -44.30798, 21.02845, -78.99856, 30.19478, -123.39264, 41.28868, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 33500
Discounting theory with old adv calculation: [0.0, 1.30898, 0.0027, 0.36212, 0.02797, 0.14086, 0.05815, 0.08184, 0.07638, 0.06165, 0.08272, 0.05338, 0.07404, 0.0532, 0.05339, 0.05728, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.27758, -11.20549, 2.11061, -7.00353, -0.00173, 0.00211, -18.40025, 12.79698, -44.31968, 21.02997, -79.01285, 30.19428, -123.40749, 41.28539, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 34000
Discounting theory with old adv calculation: [0.0, 1.30898, 0.00269, 0.36237, 0.02797, 0.14089, 0.05815, 0.08184, 0.07639, 0.06165, 0.08273, 0.05337, 0.07405, 0.05319, 0.0534, 0.05728, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.27683, -11.20347, 2.10945, -7.00223, -0.00173, 0.00211, -18.40782, 12.80003, -44.33134, 21.03148, -79.02708, 30.19378, -123.42229, 41.28211, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 34500
Discounting theory with old adv calculation: [0.0, 1.30898, 0.00268, 0.36261, 0.02796, 0.14092, 0.05815, 0.08184, 0.07639, 0.06164, 0.08274, 0.05337, 0.07406, 0.05318, 0.05341, 0.05727, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.27608, -11.20145, 2.1083, -7.00094, -0.00173, 0.00211, -18.41538, 12.80307, -44.34296, 21.03299, -79.04126, 30.19328, -123.43702, 41.27882, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 35000
Discounting theory with old adv calculation: [0.0, 1.30898, 0.00268, 0.36286, 0.02795, 0.14095, 0.05815, 0.08185, 0.0764, 0.06163, 0.08274, 0.05336, 0.07407, 0.05318, 0.05342, 0.05726, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.27533, -11.19943, 2.10715, -6.99965, -0.00174, 0.00212, -18.42291, 12.8061, -44.35455, 21.03448, -79.0554, 30.19277, -123.4517, 41.27554, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 35500
Discounting theory with old adv calculation: [0.0, 1.30898, 0.00267, 0.36311, 0.02794, 0.14098, 0.05815, 0.08185, 0.0764, 0.06163, 0.08275, 0.05336, 0.07407, 0.05317, 0.05343, 0.05725, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.27459, -11.19741, 2.106, -6.99836, -0.00174, 0.00212, -18.43042, 12.80912, -44.3661, 21.03597, -79.06948, 30.19226, -123.46632, 41.27226, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 36000
Discounting theory with old adv calculation: [0.0, 1.30898, 0.00266, 0.36335, 0.02793, 0.14101, 0.05814, 0.08185, 0.07641, 0.06162, 0.08276, 0.05335, 0.07408, 0.05316, 0.05344, 0.05724, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.27385, -11.1954, 2.10485, -6.99707, -0.00174, 0.00212, -18.43792, 12.81213, -44.37762, 21.03745, -79.08352, 30.19174, -123.48088, 41.26898, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 36500
Discounting theory with old adv calculation: [0.0, 1.30898, 0.00266, 0.3636, 0.02792, 0.14104, 0.05814, 0.08185, 0.07642, 0.06162, 0.08276, 0.05334, 0.07409, 0.05315, 0.05345, 0.05723, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.27312, -11.19338, 2.10371, -6.99578, -0.00174, 0.00212, -18.44539, 12.81513, -44.3891, 21.03892, -79.09751, 30.19122, -123.49538, 41.26571, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 37000
Discounting theory with old adv calculation: [0.0, 1.30898, 0.00265, 0.36384, 0.02791, 0.14107, 0.05814, 0.08185, 0.07642, 0.06161, 0.08277, 0.05334, 0.0741, 0.05315, 0.05346, 0.05722, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.27239, -11.19137, 2.10257, -6.99449, -0.00174, 0.00213, -18.45284, 12.81811, -44.40055, 21.04038, -79.11145, 30.1907, -123.50983, 41.26243, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 37500
Discounting theory with old adv calculation: [0.0, 1.30898, 0.00264, 0.36409, 0.0279, 0.14109, 0.05814, 0.08186, 0.07643, 0.06161, 0.08278, 0.05333, 0.07411, 0.05314, 0.05348, 0.05721, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.27166, -11.18935, 2.10143, -6.9932, -0.00174, 0.00213, -18.46027, 12.82109, -44.41196, 21.04184, -79.12534, 30.19017, -123.52422, 41.25915, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 38000
Discounting theory with old adv calculation: [0.0, 1.30898, 0.00264, 0.36433, 0.02789, 0.14112, 0.05814, 0.08186, 0.07643, 0.0616, 0.08278, 0.05333, 0.07411, 0.05313, 0.05349, 0.0572, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.27093, -11.18734, 2.1003, -6.99191, -0.00175, 0.00213, -18.46768, 12.82406, -44.42334, 21.04328, -79.13918, 30.18963, -123.53855, 41.25588, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 38500
Discounting theory with old adv calculation: [0.0, 1.30898, 0.00263, 0.36457, 0.02788, 0.14115, 0.05813, 0.08186, 0.07644, 0.0616, 0.08279, 0.05332, 0.07412, 0.05313, 0.0535, 0.05719, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.27021, -11.18533, 2.09917, -6.99062, -0.00175, 0.00213, -18.47507, 12.82702, -44.43468, 21.04472, -79.15298, 30.1891, -123.55282, 41.25261, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 39000
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00263, 0.36481, 0.02787, 0.14118, 0.05813, 0.08186, 0.07644, 0.06159, 0.0828, 0.05331, 0.07413, 0.05312, 0.05351, 0.05718, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.2695, -11.18333, 2.09804, -6.98934, -0.00175, 0.00213, -18.48244, 12.82996, -44.44599, 21.04615, -79.16673, 30.18855, -123.56704, 41.24934, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 39500
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00262, 0.36505, 0.02786, 0.14121, 0.05813, 0.08187, 0.07645, 0.06158, 0.08281, 0.05331, 0.07414, 0.05311, 0.05352, 0.05718, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.26879, -11.18132, 2.09691, -6.98805, -0.00175, 0.00214, -18.4898, 12.8329, -44.45726, 21.04758, -79.18043, 30.18801, -123.58121, 41.24607, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 40000
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00261, 0.3653, 0.02786, 0.14124, 0.05813, 0.08187, 0.07646, 0.06158, 0.08281, 0.0533, 0.07415, 0.0531, 0.05353, 0.05717, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.26808, -11.17931, 2.09579, -6.98677, -0.00175, 0.00214, -18.49713, 12.83583, -44.4685, 21.04899, -79.19409, 30.18746, -123.59532, 41.2428, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 40500
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00261, 0.36554, 0.02785, 0.14126, 0.05813, 0.08187, 0.07646, 0.06157, 0.08282, 0.0533, 0.07415, 0.0531, 0.05354, 0.05716, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.26737, -11.17731, 2.09467, -6.98549, -0.00176, 0.00214, -18.50444, 12.83875, -44.4797, 21.0504, -79.2077, 30.1869, -123.60937, 41.23953, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 41000
Discounting theory with old adv calculation: [0.0, 1.30899, 0.0026, 0.36578, 0.02784, 0.14129, 0.05813, 0.08187, 0.07647, 0.06157, 0.08283, 0.05329, 0.07416, 0.05309, 0.05355, 0.05715, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.26667, -11.17531, 2.09356, -6.9842, -0.00176, 0.00214, -18.51173, 12.84166, -44.49087, 21.0518, -79.22127, 30.18635, -123.62337, 41.23627, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 41500
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00259, 0.36601, 0.02783, 0.14132, 0.05812, 0.08188, 0.07647, 0.06156, 0.08283, 0.05328, 0.07417, 0.05308, 0.05356, 0.05714, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.26597, -11.17331, 2.09244, -6.98292, -0.00176, 0.00215, -18.51901, 12.84456, -44.50201, 21.0532, -79.23478, 30.18578, -123.63731, 41.233, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 42000
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00259, 0.36625, 0.02782, 0.14135, 0.05812, 0.08188, 0.07648, 0.06156, 0.08284, 0.05328, 0.07418, 0.05308, 0.05357, 0.05713, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.26528, -11.17131, 2.09133, -6.98164, -0.00176, 0.00215, -18.52626, 12.84744, -44.51312, 21.05458, -79.24826, 30.18522, -123.65121, 41.22974, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 42500
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00258, 0.36649, 0.02781, 0.14137, 0.05812, 0.08188, 0.07648, 0.06155, 0.08285, 0.05327, 0.07419, 0.05307, 0.05358, 0.05712, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.26458, -11.16931, 2.09022, -6.98036, -0.00176, 0.00215, -18.5335, 12.85032, -44.52419, 21.05596, -79.26169, 30.18465, -123.66504, 41.22648, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 43000
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00257, 0.36673, 0.0278, 0.1414, 0.05812, 0.08188, 0.07649, 0.06155, 0.08285, 0.05327, 0.07419, 0.05306, 0.05359, 0.05711, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.2639, -11.16731, 2.08912, -6.97908, -0.00177, 0.00215, -18.54071, 12.85319, -44.53523, 21.05733, -79.27507, 30.18407, -123.67883, 41.22322, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 43500
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00257, 0.36696, 0.02779, 0.14143, 0.05812, 0.08188, 0.0765, 0.06154, 0.08286, 0.05326, 0.0742, 0.05306, 0.0536, 0.0571, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.26321, -11.16532, 2.08802, -6.97781, -0.00177, 0.00216, -18.54791, 12.85605, -44.54624, 21.05869, -79.28841, 30.1835, -123.69256, 41.21997, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 44000
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00256, 0.3672, 0.02778, 0.14146, 0.05811, 0.08189, 0.0765, 0.06154, 0.08287, 0.05325, 0.07421, 0.05305, 0.05361, 0.0571, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.26253, -11.16332, 2.08692, -6.97653, -0.00177, 0.00216, -18.55509, 12.85891, -44.55721, 21.06005, -79.30171, 30.18291, -123.70624, 41.21671, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 44500
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00256, 0.36744, 0.02778, 0.14148, 0.05811, 0.08189, 0.07651, 0.06153, 0.08287, 0.05325, 0.07422, 0.05304, 0.05362, 0.05709, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.26185, -11.16133, 2.08582, -6.97525, -0.00177, 0.00216, -18.56225, 12.86175, -44.56815, 21.0614, -79.31496, 30.18233, -123.71987, 41.21346, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 45000
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00255, 0.36767, 0.02777, 0.14151, 0.05811, 0.08189, 0.07651, 0.06153, 0.08288, 0.05324, 0.07423, 0.05304, 0.05363, 0.05708, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.26118, -11.15934, 2.08473, -6.97398, -0.00177, 0.00216, -18.56939, 12.86458, -44.57906, 21.06274, -79.32817, 30.18174, -123.73344, 41.21021, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 45500
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00254, 0.3679, 0.02776, 0.14154, 0.05811, 0.08189, 0.07652, 0.06152, 0.08289, 0.05324, 0.07423, 0.05303, 0.05364, 0.05707, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.26051, -11.15735, 2.08363, -6.9727, -0.00177, 0.00216, -18.57652, 12.8674, -44.58994, 21.06408, -79.34134, 30.18115, -123.74697, 41.20696, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 46000
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00254, 0.36814, 0.02775, 0.14157, 0.05811, 0.0819, 0.07652, 0.06151, 0.08289, 0.05323, 0.07424, 0.05302, 0.05365, 0.05706, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.25984, -11.15536, 2.08255, -6.97143, -0.00178, 0.00217, -18.58362, 12.87022, -44.60078, 21.06541, -79.35446, 30.18055, -123.76044, 41.20371, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 46500
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00253, 0.36837, 0.02774, 0.14159, 0.0581, 0.0819, 0.07653, 0.06151, 0.0829, 0.05323, 0.07425, 0.05302, 0.05366, 0.05705, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.25918, -11.15338, 2.08146, -6.97016, -0.00178, 0.00217, -18.59071, 12.87302, -44.6116, 21.06673, -79.36754, 30.17995, -123.77387, 41.20046, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 47000
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00253, 0.3686, 0.02773, 0.14162, 0.0581, 0.0819, 0.07653, 0.0615, 0.08291, 0.05322, 0.07426, 0.05301, 0.05367, 0.05704, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.25851, -11.15139, 2.08038, -6.96888, -0.00178, 0.00217, -18.59777, 12.87582, -44.62238, 21.06804, -79.38058, 30.17935, -123.78724, 41.19722, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 47500
Discounting theory with old adv calculation: [0.0, 1.30899, 0.00252, 0.36884, 0.02772, 0.14165, 0.0581, 0.0819, 0.07654, 0.0615, 0.08291, 0.05321, 0.07426, 0.053, 0.05368, 0.05703, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.25786, -11.14941, 2.0793, -6.96761, -0.00178, 0.00217, -18.60482, 12.87861, -44.63313, 21.06935, -79.39357, 30.17875, -123.80056, 41.19397, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 48000
Discounting theory with old adv calculation: [0.0, 1.309, 0.00251, 0.36907, 0.02772, 0.14167, 0.0581, 0.0819, 0.07655, 0.06149, 0.08292, 0.05321, 0.07427, 0.05299, 0.05369, 0.05703, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.2572, -11.14743, 2.07822, -6.96634, -0.00178, 0.00218, -18.61186, 12.88139, -44.64385, 21.07065, -79.40652, 30.17814, -123.81383, 41.19073, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 48500
Discounting theory with old adv calculation: [0.0, 1.309, 0.00251, 0.3693, 0.02771, 0.1417, 0.0581, 0.08191, 0.07655, 0.06149, 0.08293, 0.0532, 0.07428, 0.05299, 0.0537, 0.05702, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.25655, -11.14545, 2.07714, -6.96507, -0.00179, 0.00218, -18.61887, 12.88415, -44.65454, 21.07195, -79.41943, 30.17752, -123.82706, 41.18749, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 49000
Discounting theory with old adv calculation: [0.0, 1.309, 0.0025, 0.36953, 0.0277, 0.14173, 0.0581, 0.08191, 0.07656, 0.06148, 0.08293, 0.0532, 0.07429, 0.05298, 0.05371, 0.05701, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.2559, -11.14347, 2.07607, -6.96381, -0.00179, 0.00218, -18.62586, 12.88692, -44.6652, 21.07323, -79.4323, 30.17691, -123.84023, 41.18426, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 49500
Discounting theory with old adv calculation: [0.0, 1.309, 0.0025, 0.36976, 0.02769, 0.14175, 0.05809, 0.08191, 0.07656, 0.06148, 0.08294, 0.05319, 0.0743, 0.05297, 0.05372, 0.057, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.25526, -11.14149, 2.075, -6.96254, -0.00179, 0.00218, -18.63284, 12.88967, -44.67583, 21.07451, -79.44513, 30.17629, -123.85336, 41.18102, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 50000
Discounting theory with old adv calculation: [0.0, 1.309, 0.00249, 0.36999, 0.02768, 0.14178, 0.05809, 0.08191, 0.07657, 0.06147, 0.08295, 0.05319, 0.0743, 0.05297, 0.05373, 0.05699, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.25461, -11.13952, 2.07393, -6.96127, -0.00179, 0.00218, -18.6398, 12.89241, -44.68643, 21.07579, -79.45792, 30.17566, -123.86643, 41.17779, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 50500
Discounting theory with old adv calculation: [0.0, 1.309, 0.00249, 0.37022, 0.02767, 0.14181, 0.05809, 0.08192, 0.07657, 0.06147, 0.08295, 0.05318, 0.07431, 0.05296, 0.05374, 0.05698, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.25398, -11.13754, 2.07287, -6.96001, -0.00179, 0.00219, -18.64675, 12.89514, -44.69699, 21.07705, -79.47066, 30.17504, -123.87946, 41.17456, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 51000
Discounting theory with old adv calculation: [0.0, 1.309, 0.00248, 0.37044, 0.02766, 0.14183, 0.05809, 0.08192, 0.07658, 0.06146, 0.08296, 0.05317, 0.07432, 0.05295, 0.05375, 0.05697, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.25334, -11.13557, 2.07181, -6.95874, -0.00179, 0.00219, -18.65367, 12.89787, -44.70753, 21.07831, -79.48337, 30.17441, -123.89245, 41.17133, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 51500
Discounting theory with old adv calculation: [0.0, 1.309, 0.00247, 0.37067, 0.02766, 0.14186, 0.05809, 0.08192, 0.07658, 0.06146, 0.08297, 0.05317, 0.07433, 0.05295, 0.05376, 0.05697, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.25271, -11.1336, 2.07075, -6.95748, -0.0018, 0.00219, -18.66058, 12.90059, -44.71804, 21.07957, -79.49603, 30.17378, -123.90538, 41.1681, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 52000
Discounting theory with old adv calculation: [0.0, 1.309, 0.00247, 0.3709, 0.02765, 0.14189, 0.05809, 0.08192, 0.07659, 0.06145, 0.08297, 0.05316, 0.07433, 0.05294, 0.05377, 0.05696, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.25208, -11.13163, 2.06969, -6.95622, -0.0018, 0.00219, -18.66747, 12.9033, -44.72852, 21.08082, -79.50866, 30.17314, -123.91826, 41.16488, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 52500
Discounting theory with old adv calculation: [0.0, 1.309, 0.00246, 0.37112, 0.02764, 0.14191, 0.05808, 0.08192, 0.07659, 0.06145, 0.08298, 0.05316, 0.07434, 0.05293, 0.05378, 0.05695, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.25145, -11.12967, 2.06864, -6.95495, -0.0018, 0.0022, -18.67434, 12.906, -44.73896, 21.08206, -79.52124, 30.1725, -123.9311, 41.16165, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 53000
Discounting theory with old adv calculation: [0.0, 1.309, 0.00246, 0.37135, 0.02763, 0.14194, 0.05808, 0.08193, 0.0766, 0.06144, 0.08298, 0.05315, 0.07435, 0.05293, 0.05379, 0.05694, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.25083, -11.1277, 2.06759, -6.95369, -0.0018, 0.0022, -18.6812, 12.90869, -44.74938, 21.08329, -79.53378, 30.17186, -123.9439, 41.15843, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 53500
Discounting theory with old adv calculation: [0.0, 1.309, 0.00245, 0.37158, 0.02762, 0.14196, 0.05808, 0.08193, 0.0766, 0.06144, 0.08299, 0.05315, 0.07436, 0.05292, 0.0538, 0.05693, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.25021, -11.12574, 2.06654, -6.95243, -0.0018, 0.0022, -18.68804, 12.91137, -44.75977, 21.08452, -79.54629, 30.17122, -123.95664, 41.15521, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 54000
Discounting theory with old adv calculation: [0.0, 1.309, 0.00245, 0.3718, 0.02762, 0.14199, 0.05808, 0.08193, 0.07661, 0.06143, 0.083, 0.05314, 0.07436, 0.05292, 0.05381, 0.05692, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.24959, -11.12377, 2.06549, -6.95118, -0.00181, 0.0022, -18.69486, 12.91404, -44.77013, 21.08574, -79.55876, 30.17057, -123.96934, 41.15199, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 54500
Discounting theory with old adv calculation: [0.0, 1.309, 0.00244, 0.37202, 0.02761, 0.14202, 0.05808, 0.08193, 0.07662, 0.06143, 0.083, 0.05314, 0.07437, 0.05291, 0.05382, 0.05692, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.24897, -11.12181, 2.06445, -6.94992, -0.00181, 0.0022, -18.70167, 12.91671, -44.78046, 21.08696, -79.57118, 30.16992, -123.98199, 41.14878, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 55000
Discounting theory with old adv calculation: [0.0, 1.309, 0.00243, 0.37225, 0.0276, 0.14204, 0.05807, 0.08194, 0.07662, 0.06142, 0.08301, 0.05313, 0.07438, 0.0529, 0.05383, 0.05691, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.24836, -11.11986, 2.06341, -6.94866, -0.00181, 0.00221, -18.70845, 12.91937, -44.79077, 21.08817, -79.58357, 30.16927, -123.9946, 41.14557, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 55500
Discounting theory with old adv calculation: [0.0, 1.309, 0.00243, 0.37247, 0.02759, 0.14207, 0.05807, 0.08194, 0.07663, 0.06142, 0.08302, 0.05312, 0.07439, 0.0529, 0.05384, 0.0569, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.24775, -11.1179, 2.06237, -6.94741, -0.00181, 0.00221, -18.71523, 12.92202, -44.80104, 21.08937, -79.59592, 30.16861, -124.00717, 41.14236, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 56000
Discounting theory with old adv calculation: [0.0, 1.309, 0.00242, 0.37269, 0.02758, 0.14209, 0.05807, 0.08194, 0.07663, 0.06141, 0.08302, 0.05312, 0.07439, 0.05289, 0.05385, 0.05689, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.24715, -11.11594, 2.06133, -6.94615, -0.00181, 0.00221, -18.72198, 12.92466, -44.81129, 21.09057, -79.60823, 30.16795, -124.01968, 41.13915, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 56500
Discounting theory with old adv calculation: [0.0, 1.309, 0.00242, 0.37292, 0.02757, 0.14212, 0.05807, 0.08194, 0.07664, 0.06141, 0.08303, 0.05311, 0.0744, 0.05288, 0.05386, 0.05688, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.24654, -11.11399, 2.0603, -6.9449, -0.00181, 0.00221, -18.72872, 12.92729, -44.8215, 21.09176, -79.6205, 30.16729, -124.03216, 41.13594, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 57000
Discounting theory with old adv calculation: [0.0, 1.309, 0.00241, 0.37314, 0.02757, 0.14215, 0.05807, 0.08194, 0.07664, 0.0614, 0.08304, 0.05311, 0.07441, 0.05288, 0.05387, 0.05688, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.24594, -11.11204, 2.05927, -6.94364, -0.00182, 0.00221, -18.73544, 12.92992, -44.83169, 21.09295, -79.63274, 30.16663, -124.04459, 41.13274, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 57500
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00241, 0.37336, 0.02756, 0.14217, 0.05807, 0.08195, 0.07665, 0.0614, 0.08304, 0.0531, 0.07441, 0.05287, 0.05388, 0.05687, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.24534, -11.11009, 2.05824, -6.94239, -0.00182, 0.00222, -18.74215, 12.93254, -44.84185, 21.09413, -79.64494, 30.16596, -124.05697, 41.12953, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 58000
Discounting theory with old adv calculation: [0.0, 1.30901, 0.0024, 0.37358, 0.02755, 0.1422, 0.05806, 0.08195, 0.07665, 0.06139, 0.08305, 0.0531, 0.07442, 0.05286, 0.05389, 0.05686, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.24475, -11.10814, 2.05721, -6.94114, -0.00182, 0.00222, -18.74884, 12.93515, -44.85199, 21.0953, -79.6571, 30.16529, -124.06931, 41.12633, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 58500
Discounting theory with old adv calculation: [0.0, 1.30901, 0.0024, 0.3738, 0.02754, 0.14222, 0.05806, 0.08195, 0.07666, 0.06139, 0.08305, 0.05309, 0.07443, 0.05286, 0.05389, 0.05685, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.24416, -11.10619, 2.05619, -6.93989, -0.00182, 0.00222, -18.75551, 12.93775, -44.86209, 21.09647, -79.66922, 30.16462, -124.08161, 41.12314, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 59000
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00239, 0.37402, 0.02753, 0.14225, 0.05806, 0.08195, 0.07666, 0.06138, 0.08306, 0.05309, 0.07444, 0.05285, 0.0539, 0.05684, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.24357, -11.10425, 2.05517, -6.93864, -0.00182, 0.00222, -18.76216, 12.94034, -44.87217, 21.09763, -79.68131, 30.16394, -124.09386, 41.11994, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 59500
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00239, 0.37424, 0.02753, 0.14227, 0.05806, 0.08195, 0.07667, 0.06138, 0.08307, 0.05308, 0.07444, 0.05284, 0.05391, 0.05683, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.24298, -11.1023, 2.05415, -6.93739, -0.00182, 0.00223, -18.76881, 12.94293, -44.88222, 21.09879, -79.69335, 30.16326, -124.10607, 41.11675, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 60000
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00238, 0.37446, 0.02752, 0.1423, 0.05806, 0.08196, 0.07667, 0.06137, 0.08307, 0.05308, 0.07445, 0.05284, 0.05392, 0.05683, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.2424, -11.10036, 2.05313, -6.93615, -0.00183, 0.00223, -18.77543, 12.94551, -44.89224, 21.09994, -79.70537, 30.16258, -124.11824, 41.11356, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 60500
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00238, 0.37467, 0.02751, 0.14232, 0.05806, 0.08196, 0.07668, 0.06137, 0.08308, 0.05307, 0.07446, 0.05283, 0.05393, 0.05682, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.24182, -11.09842, 2.05212, -6.9349, -0.00183, 0.00223, -18.78204, 12.94808, -44.90224, 21.10108, -79.71734, 30.1619, -124.13036, 41.11037, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 61000
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00237, 0.37489, 0.0275, 0.14235, 0.05805, 0.08196, 0.07668, 0.06136, 0.08309, 0.05306, 0.07447, 0.05283, 0.05394, 0.05681, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.24124, -11.09648, 2.0511, -6.93365, -0.00183, 0.00223, -18.78863, 12.95064, -44.91221, 21.10222, -79.72928, 30.16121, -124.14245, 41.10718, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 61500
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00237, 0.37511, 0.02749, 0.14237, 0.05805, 0.08196, 0.07669, 0.06136, 0.08309, 0.05306, 0.07447, 0.05282, 0.05395, 0.0568, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.24066, -11.09455, 2.0501, -6.93241, -0.00183, 0.00223, -18.79521, 12.95319, -44.92215, 21.10336, -79.74118, 30.16053, -124.15449, 41.104, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 62000
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00236, 0.37533, 0.02749, 0.1424, 0.05805, 0.08196, 0.07669, 0.06135, 0.0831, 0.05305, 0.07448, 0.05281, 0.05396, 0.05679, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.24009, -11.09261, 2.04909, -6.93117, -0.00183, 0.00224, -18.80177, 12.95574, -44.93206, 21.10448, -79.75305, 30.15984, -124.16648, 41.10082, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 62500
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00236, 0.37554, 0.02748, 0.14242, 0.05805, 0.08197, 0.0767, 0.06135, 0.0831, 0.05305, 0.07449, 0.05281, 0.05397, 0.05679, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23952, -11.09068, 2.04808, -6.92992, -0.00183, 0.00224, -18.80831, 12.95828, -44.94195, 21.10561, -79.76488, 30.15914, -124.17844, 41.09764, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 63000
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00235, 0.37576, 0.02747, 0.14245, 0.05805, 0.08197, 0.0767, 0.06134, 0.08311, 0.05304, 0.07449, 0.0528, 0.05398, 0.05678, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23895, -11.08875, 2.04708, -6.92868, -0.00184, 0.00224, -18.81484, 12.96081, -44.95181, 21.10672, -79.77668, 30.15845, -124.19036, 41.09446, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 63500
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00234, 0.37597, 0.02746, 0.14247, 0.05804, 0.08197, 0.07671, 0.06134, 0.08312, 0.05304, 0.0745, 0.05279, 0.05399, 0.05677, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.23839, -11.08682, 2.04608, -6.92744, -0.00184, 0.00224, -18.82136, 12.96333, -44.96165, 21.10783, -79.78844, 30.15775, -124.20223, 41.09129, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 64000
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00234, 0.37619, 0.02745, 0.1425, 0.05804, 0.08197, 0.07671, 0.06133, 0.08312, 0.05303, 0.07451, 0.05279, 0.054, 0.05676, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23782, -11.08489, 2.04508, -6.9262, -0.00184, 0.00224, -18.82786, 12.96585, -44.97146, 21.10894, -79.80017, 30.15705, -124.21407, 41.08811, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 64500
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00233, 0.3764, 0.02745, 0.14252, 0.05804, 0.08198, 0.07672, 0.06133, 0.08313, 0.05303, 0.07452, 0.05278, 0.05401, 0.05675, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.23726, -11.08297, 2.04409, -6.92497, -0.00184, 0.00225, -18.83434, 12.96836, -44.98124, 21.11004, -79.81186, 30.15635, -124.22586, 41.08494, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 65000
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00233, 0.37662, 0.02744, 0.14255, 0.05804, 0.08198, 0.07673, 0.06132, 0.08314, 0.05302, 0.07452, 0.05278, 0.05402, 0.05675, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23671, -11.08104, 2.0431, -6.92373, -0.00184, 0.00225, -18.84081, 12.97086, -44.991, 21.11113, -79.82351, 30.15564, -124.23761, 41.08178, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 65500
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00232, 0.37683, 0.02743, 0.14257, 0.05804, 0.08198, 0.07673, 0.06132, 0.08314, 0.05302, 0.07453, 0.05277, 0.05403, 0.05674, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23615, -11.07912, 2.0421, -6.92249, -0.00185, 0.00225, -18.84726, 12.97336, -45.00073, 21.11222, -79.83513, 30.15494, -124.24932, 41.07861, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 66000
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00232, 0.37704, 0.02742, 0.1426, 0.05804, 0.08198, 0.07674, 0.06131, 0.08315, 0.05301, 0.07454, 0.05276, 0.05404, 0.05673, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.2356, -11.0772, 2.04112, -6.92126, -0.00185, 0.00225, -18.8537, 12.97584, -45.01044, 21.11331, -79.84672, 30.15423, -124.26099, 41.07545, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 66500
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00231, 0.37725, 0.02742, 0.14262, 0.05803, 0.08198, 0.07674, 0.06131, 0.08315, 0.05301, 0.07454, 0.05276, 0.05404, 0.05672, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23505, -11.07528, 2.04013, -6.92002, -0.00185, 0.00225, -18.86013, 12.97832, -45.02012, 21.11438, -79.85827, 30.15351, -124.27263, 41.07229, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 67000
Discounting theory with old adv calculation: [0.0, 1.30901, 0.00231, 0.37747, 0.02741, 0.14265, 0.05803, 0.08199, 0.07675, 0.0613, 0.08316, 0.053, 0.07455, 0.05275, 0.05405, 0.05671, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.2345, -11.07337, 2.03915, -6.91879, -0.00185, 0.00226, -18.86653, 12.9808, -45.02977, 21.11546, -79.86979, 30.1528, -124.28422, 41.06913, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 67500
Discounting theory with old adv calculation: [0.0, 1.30902, 0.0023, 0.37768, 0.0274, 0.14267, 0.05803, 0.08199, 0.07675, 0.0613, 0.08317, 0.053, 0.07456, 0.05274, 0.05406, 0.05671, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.23395, -11.07145, 2.03817, -6.91756, -0.00185, 0.00226, -18.87293, 12.98326, -45.0394, 21.11653, -79.88128, 30.15208, -124.29577, 41.06597, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 68000
Discounting theory with old adv calculation: [0.0, 1.30902, 0.0023, 0.37789, 0.02739, 0.14269, 0.05803, 0.08199, 0.07676, 0.06129, 0.08317, 0.05299, 0.07457, 0.05274, 0.05407, 0.0567, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23341, -11.06954, 2.03719, -6.91633, -0.00185, 0.00226, -18.87931, 12.98572, -45.049, 21.11759, -79.89273, 30.15137, -124.30729, 41.06282, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 68500
Discounting theory with old adv calculation: [0.0, 1.30902, 0.0023, 0.3781, 0.02739, 0.14272, 0.05803, 0.08199, 0.07676, 0.06129, 0.08318, 0.05299, 0.07457, 0.05273, 0.05408, 0.05669, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23287, -11.06763, 2.03621, -6.9151, -0.00186, 0.00226, -18.88567, 12.98817, -45.05858, 21.11865, -79.90415, 30.15065, -124.31876, 41.05967, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 69000
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00229, 0.37831, 0.02738, 0.14274, 0.05803, 0.08199, 0.07677, 0.06128, 0.08318, 0.05298, 0.07458, 0.05273, 0.05409, 0.05668, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23233, -11.06572, 2.03523, -6.91387, -0.00186, 0.00226, -18.89202, 12.99061, -45.06814, 21.1197, -79.91553, 30.14992, -124.3302, 41.05652, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 69500
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00229, 0.37852, 0.02737, 0.14277, 0.05802, 0.082, 0.07677, 0.06128, 0.08319, 0.05298, 0.07459, 0.05272, 0.0541, 0.05668, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.2318, -11.06381, 2.03426, -6.91264, -0.00186, 0.00227, -18.89835, 12.99305, -45.07767, 21.12075, -79.92689, 30.1492, -124.3416, 41.05338, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 70000
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00228, 0.37873, 0.02736, 0.14279, 0.05802, 0.082, 0.07678, 0.06127, 0.0832, 0.05297, 0.07459, 0.05271, 0.05411, 0.05667, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23127, -11.0619, 2.03329, -6.91141, -0.00186, 0.00227, -18.90467, 12.99548, -45.08717, 21.12179, -79.93821, 30.14847, -124.35296, 41.05023, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 70500
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00228, 0.37893, 0.02736, 0.14281, 0.05802, 0.082, 0.07678, 0.06127, 0.0832, 0.05297, 0.0746, 0.05271, 0.05412, 0.05666, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.23073, -11.06, 2.03232, -6.91019, -0.00186, 0.00227, -18.91098, 12.9979, -45.09665, 21.12283, -79.94949, 30.14775, -124.36429, 41.04709, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 71000
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00227, 0.37914, 0.02735, 0.14284, 0.05802, 0.082, 0.07679, 0.06126, 0.08321, 0.05296, 0.07461, 0.0527, 0.05413, 0.05665, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.23021, -11.0581, 2.03136, -6.90896, -0.00186, 0.00227, -18.91727, 13.00032, -45.10611, 21.12386, -79.96075, 30.14701, -124.37557, 41.04396, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 71500
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00227, 0.37935, 0.02734, 0.14286, 0.05802, 0.082, 0.07679, 0.06126, 0.08321, 0.05295, 0.07462, 0.0527, 0.05414, 0.05665, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.22968, -11.0562, 2.03039, -6.90774, -0.00187, 0.00227, -18.92354, 13.00273, -45.11554, 21.12488, -79.97197, 30.14628, -124.38682, 41.04082, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 72000
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00226, 0.37956, 0.02733, 0.14289, 0.05802, 0.08201, 0.0768, 0.06125, 0.08322, 0.05295, 0.07462, 0.05269, 0.05414, 0.05664, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22916, -11.0543, 2.02943, -6.90651, -0.00187, 0.00228, -18.92981, 13.00513, -45.12495, 21.12591, -79.98316, 30.14555, -124.39803, 41.03769, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 72500
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00226, 0.37976, 0.02733, 0.14291, 0.05801, 0.08201, 0.0768, 0.06125, 0.08323, 0.05294, 0.07463, 0.05268, 0.05415, 0.05663, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.22864, -11.0524, 2.02847, -6.90529, -0.00187, 0.00228, -18.93605, 13.00752, -45.13433, 21.12692, -79.99431, 30.14481, -124.4092, 41.03456, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 73000
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00225, 0.37997, 0.02732, 0.14293, 0.05801, 0.08201, 0.07681, 0.06125, 0.08323, 0.05294, 0.07464, 0.05268, 0.05416, 0.05662, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22812, -11.05051, 2.02752, -6.90407, -0.00187, 0.00228, -18.94229, 13.00991, -45.14369, 21.12794, -80.00544, 30.14407, -124.42034, 41.03143, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 73500
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00225, 0.38018, 0.02731, 0.14296, 0.05801, 0.08201, 0.07681, 0.06124, 0.08324, 0.05293, 0.07464, 0.05267, 0.05417, 0.05661, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.2276, -11.04862, 2.02656, -6.90285, -0.00187, 0.00228, -18.94851, 13.01229, -45.15303, 21.12894, -80.01653, 30.14333, -124.43144, 41.0283, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 74000
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00224, 0.38038, 0.0273, 0.14298, 0.05801, 0.08201, 0.07682, 0.06124, 0.08324, 0.05293, 0.07465, 0.05267, 0.05418, 0.05661, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22708, -11.04672, 2.02561, -6.90163, -0.00187, 0.00228, -18.95471, 13.01467, -45.16234, 21.12995, -80.02759, 30.14259, -124.4425, 41.02518, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 74500
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00224, 0.38059, 0.0273, 0.143, 0.05801, 0.08202, 0.07682, 0.06123, 0.08325, 0.05292, 0.07466, 0.05266, 0.05419, 0.0566, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22657, -11.04484, 2.02466, -6.90042, -0.00187, 0.00229, -18.9609, 13.01704, -45.17163, 21.13095, -80.03863, 30.14185, -124.45353, 41.02206, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 75000
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00223, 0.38079, 0.02729, 0.14303, 0.05801, 0.08202, 0.07683, 0.06123, 0.08325, 0.05292, 0.07466, 0.05265, 0.0542, 0.05659, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22606, -11.04295, 2.02371, -6.8992, -0.00188, 0.00229, -18.96708, 13.0194, -45.18089, 21.13194, -80.04963, 30.1411, -124.46452, 41.01894, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 75500
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00223, 0.38099, 0.02728, 0.14305, 0.05801, 0.08202, 0.07683, 0.06122, 0.08326, 0.05291, 0.07467, 0.05265, 0.05421, 0.05658, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.22555, -11.04106, 2.02276, -6.89798, -0.00188, 0.00229, -18.97325, 13.02175, -45.19013, 21.13293, -80.06059, 30.14035, -124.47548, 41.01583, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 76000
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00222, 0.3812, 0.02727, 0.14307, 0.058, 0.08202, 0.07684, 0.06122, 0.08327, 0.05291, 0.07468, 0.05264, 0.05422, 0.05658, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22505, -11.03918, 2.02182, -6.89677, -0.00188, 0.00229, -18.9794, 13.0241, -45.19935, 21.13391, -80.07153, 30.1396, -124.4864, 41.01271, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 76500
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00222, 0.3814, 0.02727, 0.1431, 0.058, 0.08202, 0.07684, 0.06121, 0.08327, 0.0529, 0.07468, 0.05264, 0.05422, 0.05657, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22454, -11.0373, 2.02088, -6.89555, -0.00188, 0.00229, -18.98553, 13.02644, -45.20854, 21.13489, -80.08244, 30.13885, -124.49728, 41.0096, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 77000
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00222, 0.3816, 0.02726, 0.14312, 0.058, 0.08203, 0.07684, 0.06121, 0.08328, 0.0529, 0.07469, 0.05263, 0.05423, 0.05656, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.22404, -11.03542, 2.01994, -6.89434, -0.00188, 0.0023, -18.99165, 13.02878, -45.21772, 21.13586, -80.09332, 30.1381, -124.50813, 41.00649, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 77500
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00221, 0.38181, 0.02725, 0.14314, 0.058, 0.08203, 0.07685, 0.0612, 0.08328, 0.05289, 0.0747, 0.05262, 0.05424, 0.05655, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.22354, -11.03354, 2.019, -6.89313, -0.00188, 0.0023, -18.99776, 13.0311, -45.22687, 21.13683, -80.10416, 30.13735, -124.51895, 41.00339, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 78000
Discounting theory with old adv calculation: [0.0, 1.30902, 0.00221, 0.38201, 0.02724, 0.14317, 0.058, 0.08203, 0.07685, 0.0612, 0.08329, 0.05289, 0.0747, 0.05262, 0.05425, 0.05655, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.22305, -11.03166, 2.01806, -6.89192, -0.00189, 0.0023, -19.00386, 13.03343, -45.23599, 21.1378, -80.11498, 30.13659, -124.52973, 41.00029, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 78500
Discounting theory with old adv calculation: [0.0, 1.30902, 0.0022, 0.38221, 0.02724, 0.14319, 0.058, 0.08203, 0.07686, 0.06119, 0.0833, 0.05288, 0.07471, 0.05261, 0.05426, 0.05654, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.22255, -11.02979, 2.01713, -6.89071, -0.00189, 0.0023, -19.00994, 13.03574, -45.2451, 21.13876, -80.12576, 30.13583, -124.54047, 40.99719, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 79000
Discounting theory with old adv calculation: [0.0, 1.30903, 0.0022, 0.38241, 0.02723, 0.14321, 0.05799, 0.08203, 0.07686, 0.06119, 0.0833, 0.05288, 0.07472, 0.05261, 0.05427, 0.05653, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.22206, -11.02792, 2.0162, -6.8895, -0.00189, 0.0023, -19.01601, 13.03805, -45.25418, 21.13972, -80.13652, 30.13507, -124.55118, 40.99409, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 79500
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00219, 0.38261, 0.02722, 0.14324, 0.05799, 0.08204, 0.07687, 0.06119, 0.08331, 0.05287, 0.07472, 0.0526, 0.05428, 0.05652, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22157, -11.02605, 2.01527, -6.88829, -0.00189, 0.00231, -19.02206, 13.04035, -45.26323, 21.14067, -80.14724, 30.13431, -124.56186, 40.991, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 80000
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00219, 0.38281, 0.02722, 0.14326, 0.05799, 0.08204, 0.07687, 0.06118, 0.08331, 0.05287, 0.07473, 0.0526, 0.05429, 0.05652, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22108, -11.02418, 2.01434, -6.88709, -0.00189, 0.00231, -19.02811, 13.04265, -45.27227, 21.14162, -80.15794, 30.13355, -124.5725, 40.9879, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 80500
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00218, 0.38301, 0.02721, 0.14328, 0.05799, 0.08204, 0.07688, 0.06118, 0.08332, 0.05286, 0.07474, 0.05259, 0.05429, 0.05651, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22059, -11.02231, 2.01341, -6.88588, -0.00189, 0.00231, -19.03413, 13.04494, -45.28128, 21.14256, -80.16861, 30.13278, -124.58311, 40.98481, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 81000
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00218, 0.38321, 0.0272, 0.14331, 0.05799, 0.08204, 0.07688, 0.06117, 0.08332, 0.05286, 0.07474, 0.05258, 0.0543, 0.0565, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.22011, -11.02045, 2.01249, -6.88468, -0.0019, 0.00231, -19.04015, 13.04722, -45.29028, 21.1435, -80.17925, 30.13202, -124.59368, 40.98173, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 81500
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00218, 0.38341, 0.02719, 0.14333, 0.05799, 0.08204, 0.07689, 0.06117, 0.08333, 0.05285, 0.07475, 0.05258, 0.05431, 0.0565, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21963, -11.01858, 2.01157, -6.88348, -0.0019, 0.00231, -19.04615, 13.0495, -45.29925, 21.14443, -80.18985, 30.13125, -124.60422, 40.97864, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 82000
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00217, 0.38361, 0.02719, 0.14335, 0.05798, 0.08204, 0.07689, 0.06116, 0.08334, 0.05285, 0.07476, 0.05257, 0.05432, 0.05649, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21915, -11.01672, 2.01065, -6.88227, -0.0019, 0.00232, -19.05214, 13.05177, -45.30819, 21.14536, -80.20043, 30.13048, -124.61473, 40.97556, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 82500
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00217, 0.3838, 0.02718, 0.14337, 0.05798, 0.08205, 0.0769, 0.06116, 0.08334, 0.05284, 0.07476, 0.05257, 0.05433, 0.05648, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21867, -11.01486, 2.00973, -6.88107, -0.0019, 0.00232, -19.05812, 13.05403, -45.31712, 21.14629, -80.21098, 30.12971, -124.6252, 40.97248, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 83000
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00216, 0.384, 0.02717, 0.1434, 0.05798, 0.08205, 0.0769, 0.06115, 0.08335, 0.05284, 0.07477, 0.05256, 0.05434, 0.05647, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21819, -11.01301, 2.00882, -6.87987, -0.0019, 0.00232, -19.06408, 13.05629, -45.32602, 21.14721, -80.22151, 30.12894, -124.63565, 40.96941, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 83500
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00216, 0.3842, 0.02717, 0.14342, 0.05798, 0.08205, 0.07691, 0.06115, 0.08335, 0.05284, 0.07478, 0.05256, 0.05435, 0.05647, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21772, -11.01115, 2.0079, -6.87867, -0.0019, 0.00232, -19.07003, 13.05854, -45.33491, 21.14812, -80.232, 30.12817, -124.64605, 40.96633, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 84000
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00215, 0.3844, 0.02716, 0.14344, 0.05798, 0.08205, 0.07691, 0.06114, 0.08336, 0.05283, 0.07478, 0.05255, 0.05436, 0.05646, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21725, -11.0093, 2.00699, -6.87747, -0.00191, 0.00232, -19.07597, 13.06079, -45.34377, 21.14904, -80.24246, 30.12739, -124.65643, 40.96326, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 84500
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00215, 0.38459, 0.02715, 0.14346, 0.05798, 0.08205, 0.07692, 0.06114, 0.08336, 0.05283, 0.07479, 0.05254, 0.05436, 0.05645, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21678, -11.00745, 2.00608, -6.87628, -0.00191, 0.00233, -19.08189, 13.06303, -45.35261, 21.14994, -80.2529, 30.12661, -124.66677, 40.9602, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 85000
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00214, 0.38479, 0.02715, 0.14349, 0.05798, 0.08206, 0.07692, 0.06114, 0.08337, 0.05282, 0.0748, 0.05254, 0.05437, 0.05644, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21631, -11.0056, 2.00517, -6.87508, -0.00191, 0.00233, -19.0878, 13.06527, -45.36142, 21.15085, -80.26331, 30.12584, -124.67709, 40.95713, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 85500
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00214, 0.38498, 0.02714, 0.14351, 0.05797, 0.08206, 0.07693, 0.06113, 0.08338, 0.05282, 0.0748, 0.05253, 0.05438, 0.05644, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21584, -11.00375, 2.00427, -6.87389, -0.00191, 0.00233, -19.0937, 13.06749, -45.37022, 21.15175, -80.27369, 30.12506, -124.68737, 40.95407, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 86000
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00214, 0.38518, 0.02713, 0.14353, 0.05797, 0.08206, 0.07693, 0.06113, 0.08338, 0.05281, 0.07481, 0.05253, 0.05439, 0.05643, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21538, -11.0019, 2.00336, -6.87269, -0.00191, 0.00233, -19.09959, 13.06972, -45.379, 21.15264, -80.28404, 30.12428, -124.69761, 40.95101, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 86500
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00213, 0.38537, 0.02712, 0.14355, 0.05797, 0.08206, 0.07694, 0.06112, 0.08339, 0.05281, 0.07482, 0.05252, 0.0544, 0.05642, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21491, -11.00006, 2.00246, -6.8715, -0.00191, 0.00233, -19.10546, 13.07193, -45.38775, 21.15353, -80.29436, 30.12349, -124.70783, 40.94795, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 87000
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00213, 0.38557, 0.02712, 0.14358, 0.05797, 0.08206, 0.07694, 0.06112, 0.08339, 0.0528, 0.07482, 0.05252, 0.05441, 0.05642, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21445, -10.99822, 2.00156, -6.87031, -0.00191, 0.00233, -19.11133, 13.07414, -45.39648, 21.15442, -80.30466, 30.12271, -124.71801, 40.9449, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 87500
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00212, 0.38576, 0.02711, 0.1436, 0.05797, 0.08207, 0.07695, 0.06111, 0.0834, 0.0528, 0.07483, 0.05251, 0.05441, 0.05641, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21399, -10.99638, 2.00066, -6.86912, -0.00192, 0.00234, -19.11717, 13.07635, -45.4052, 21.15531, -80.31493, 30.12192, -124.72817, 40.94184, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 88000
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00212, 0.38595, 0.0271, 0.14362, 0.05797, 0.08207, 0.07695, 0.06111, 0.0834, 0.05279, 0.07484, 0.0525, 0.05442, 0.0564, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21354, -10.99454, 1.99977, -6.86793, -0.00192, 0.00234, -19.12301, 13.07854, -45.41389, 21.15618, -80.32517, 30.12114, -124.73829, 40.93879, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 88500
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00212, 0.38615, 0.0271, 0.14364, 0.05796, 0.08207, 0.07695, 0.0611, 0.08341, 0.05279, 0.07484, 0.0525, 0.05443, 0.05639, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21308, -10.9927, 1.99887, -6.86674, -0.00192, 0.00234, -19.12884, 13.08074, -45.42256, 21.15706, -80.33539, 30.12035, -124.74838, 40.93575, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 89000
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00211, 0.38634, 0.02709, 0.14366, 0.05796, 0.08207, 0.07696, 0.0611, 0.08341, 0.05278, 0.07485, 0.05249, 0.05444, 0.05639, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21263, -10.99087, 1.99798, -6.86555, -0.00192, 0.00234, -19.13465, 13.08292, -45.43121, 21.15793, -80.34558, 30.11956, -124.75844, 40.9327, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 89500
Discounting theory with old adv calculation: [0.0, 1.30903, 0.00211, 0.38653, 0.02708, 0.14369, 0.05796, 0.08207, 0.07696, 0.0611, 0.08342, 0.05278, 0.07486, 0.05249, 0.05445, 0.05638, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21218, -10.98903, 1.99709, -6.86437, -0.00192, 0.00234, -19.14045, 13.0851, -45.43984, 21.1588, -80.35574, 30.11877, -124.76847, 40.92966, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 90000
Discounting theory with old adv calculation: [0.0, 1.30903, 0.0021, 0.38672, 0.02708, 0.14371, 0.05796, 0.08208, 0.07697, 0.06109, 0.08343, 0.05277, 0.07486, 0.05248, 0.05446, 0.05637, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21173, -10.9872, 1.9962, -6.86318, -0.00192, 0.00235, -19.14623, 13.08728, -45.44845, 21.15966, -80.36587, 30.11798, -124.77846, 40.92662, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 90500
Discounting theory with old adv calculation: [0.0, 1.30903, 0.0021, 0.38692, 0.02707, 0.14373, 0.05796, 0.08208, 0.07697, 0.06109, 0.08343, 0.05277, 0.07487, 0.05248, 0.05446, 0.05637, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.21128, -10.98537, 1.99532, -6.862, -0.00193, 0.00235, -19.15201, 13.08945, -45.45704, 21.16052, -80.37598, 30.11719, -124.78843, 40.92359, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 91000
Discounting theory with old adv calculation: [0.0, 1.30903, 0.0021, 0.38711, 0.02706, 0.14375, 0.05796, 0.08208, 0.07698, 0.06108, 0.08344, 0.05276, 0.07488, 0.05247, 0.05447, 0.05636, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21084, -10.98355, 1.99443, -6.86081, -0.00193, 0.00235, -19.15777, 13.09161, -45.46561, 21.16138, -80.38606, 30.11639, -124.79837, 40.92056, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 91500
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00209, 0.3873, 0.02706, 0.14377, 0.05796, 0.08208, 0.07698, 0.06108, 0.08344, 0.05276, 0.07488, 0.05247, 0.05448, 0.05635, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.21039, -10.98172, 1.99355, -6.85963, -0.00193, 0.00235, -19.16352, 13.09377, -45.47416, 21.16223, -80.39611, 30.1156, -124.80828, 40.91753, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 92000
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00209, 0.38749, 0.02705, 0.14379, 0.05795, 0.08208, 0.07699, 0.06107, 0.08345, 0.05275, 0.07489, 0.05246, 0.05449, 0.05635, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.20995, -10.9799, 1.99267, -6.85845, -0.00193, 0.00235, -19.16926, 13.09592, -45.48269, 21.16307, -80.40614, 30.1148, -124.81815, 40.9145, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 92500
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00208, 0.38768, 0.02704, 0.14382, 0.05795, 0.08208, 0.07699, 0.06107, 0.08345, 0.05275, 0.07489, 0.05245, 0.0545, 0.05634, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.20951, -10.97808, 1.99179, -6.85727, -0.00193, 0.00236, -19.17499, 13.09807, -45.4912, 21.16392, -80.41614, 30.114, -124.828, 40.91147, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 93000
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00208, 0.38787, 0.02704, 0.14384, 0.05795, 0.08209, 0.077, 0.06106, 0.08346, 0.05274, 0.0749, 0.05245, 0.05451, 0.05633, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.20907, -10.97626, 1.99091, -6.85609, -0.00193, 0.00236, -19.18071, 13.10021, -45.49969, 21.16476, -80.42612, 30.11321, -124.83782, 40.90845, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 93500
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00208, 0.38806, 0.02703, 0.14386, 0.05795, 0.08209, 0.077, 0.06106, 0.08346, 0.05274, 0.07491, 0.05244, 0.05451, 0.05632, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.20863, -10.97444, 1.99004, -6.85491, -0.00193, 0.00236, -19.18641, 13.10234, -45.50816, 21.16559, -80.43607, 30.11241, -124.84761, 40.90543, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 94000
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00207, 0.38825, 0.02702, 0.14388, 0.05795, 0.08209, 0.07701, 0.06106, 0.08347, 0.05274, 0.07491, 0.05244, 0.05452, 0.05632, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.2082, -10.97262, 1.98916, -6.85373, -0.00194, 0.00236, -19.1921, 13.10447, -45.51661, 21.16643, -80.44599, 30.11161, -124.85737, 40.90241, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 94500
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00207, 0.38843, 0.02702, 0.1439, 0.05795, 0.08209, 0.07701, 0.06105, 0.08348, 0.05273, 0.07492, 0.05243, 0.05453, 0.05631, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.20777, -10.97081, 1.98829, -6.85256, -0.00194, 0.00236, -19.19778, 13.1066, -45.52504, 21.16726, -80.45589, 30.1108, -124.8671, 40.8994, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 95000
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00206, 0.38862, 0.02701, 0.14392, 0.05794, 0.08209, 0.07701, 0.06105, 0.08348, 0.05273, 0.07493, 0.05243, 0.05454, 0.0563, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.20734, -10.969, 1.98742, -6.85138, -0.00194, 0.00236, -19.20345, 13.10872, -45.53345, 21.16808, -80.46576, 30.11, -124.8768, 40.89639, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 95500
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00206, 0.38881, 0.027, 0.14395, 0.05794, 0.0821, 0.07702, 0.06104, 0.08349, 0.05272, 0.07493, 0.05242, 0.05455, 0.0563, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.20691, -10.96719, 1.98656, -6.85021, -0.00194, 0.00237, -19.20911, 13.11083, -45.54184, 21.1689, -80.47561, 30.1092, -124.88647, 40.89338, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 96000
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00206, 0.389, 0.027, 0.14397, 0.05794, 0.0821, 0.07702, 0.06104, 0.08349, 0.05272, 0.07494, 0.05242, 0.05456, 0.05629, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.20648, -10.96538, 1.98569, -6.84904, -0.00194, 0.00237, -19.21475, 13.11294, -45.55021, 21.16972, -80.48543, 30.10839, -124.89611, 40.89038, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 96500
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00205, 0.38918, 0.02699, 0.14399, 0.05794, 0.0821, 0.07703, 0.06103, 0.0835, 0.05271, 0.07495, 0.05241, 0.05456, 0.05628, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [-0.0, -0.0, 0.20605, -10.96358, 1.98482, -6.84786, -0.00194, 0.00237, -19.22038, 13.11504, -45.55857, 21.17053, -80.49523, 30.10758, -124.90573, 40.88737, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 97000
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00205, 0.38937, 0.02698, 0.14401, 0.05794, 0.0821, 0.07703, 0.06103, 0.0835, 0.05271, 0.07495, 0.05241, 0.05457, 0.05628, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.20563, -10.96177, 1.98396, -6.84669, -0.00195, 0.00237, -19.22601, 13.11714, -45.5669, 21.17134, -80.505, 30.10678, -124.91531, 40.88437, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 97500
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00204, 0.38956, 0.02698, 0.14403, 0.05794, 0.0821, 0.07704, 0.06103, 0.08351, 0.0527, 0.07496, 0.0524, 0.05458, 0.05627, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.2052, -10.95997, 1.9831, -6.84552, -0.00195, 0.00237, -19.23162, 13.11923, -45.57522, 21.17215, -80.51475, 30.10597, -124.92487, 40.88138, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 98000
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00204, 0.38974, 0.02697, 0.14405, 0.05794, 0.0821, 0.07704, 0.06102, 0.08351, 0.0527, 0.07496, 0.05239, 0.05459, 0.05626, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.20478, -10.95817, 1.98224, -6.84435, -0.00195, 0.00238, -19.23722, 13.12132, -45.58351, 21.17295, -80.52447, 30.10516, -124.9344, 40.87838, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 98500
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00204, 0.38993, 0.02696, 0.14407, 0.05793, 0.08211, 0.07705, 0.06102, 0.08352, 0.05269, 0.07497, 0.05239, 0.0546, 0.05626, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.20436, -10.95637, 1.98139, -6.84319, -0.00195, 0.00238, -19.2428, 13.1234, -45.59179, 21.17375, -80.53417, 30.10435, -124.9439, 40.87539, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 99000
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00203, 0.39011, 0.02696, 0.14409, 0.05793, 0.08211, 0.07705, 0.06101, 0.08352, 0.05269, 0.07498, 0.05238, 0.0546, 0.05625, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, 0.0, 0.20395, -10.95457, 1.98053, -6.84202, -0.00195, 0.00238, -19.24838, 13.12547, -45.60005, 21.17455, -80.54384, 30.10354, -124.95338, 40.8724, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 99500
Discounting theory with old adv calculation: [0.0, 1.30904, 0.00203, 0.3903, 0.02695, 0.14412, 0.05793, 0.08211, 0.07706, 0.06101, 0.08353, 0.05268, 0.07498, 0.05238, 0.05461, 0.05624, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 0 0 0 1 0 0]
Discounting theory with new adv calculation: [0.0, -0.0, 0.20353, -10.95278, 1.97968, -6.84085, -0.00195, 0.00238, -19.25395, 13.12754, -45.60829, 21.17534, -80.55349, 30.10272, -124.96282, 40.86941, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False  True False False False  True  True  True]
Iteration 0
Discounting practice with old adv calculation: [0.0082, -0.0082, 0.00772, -0.00772, 0.00674, -0.00674, 0.00475, -0.00475, 0.00075, -0.00075, -0.00734, 0.00734, -0.02368, 0.02368, -0.0567, 0.0567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 0 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0082, -0.0082, 0.00772, -0.00772, 0.00674, -0.00674, 0.00475, -0.00475, 0.00075, -0.00075, -0.00734, 0.00734, -0.02368, 0.02368, -0.0567, 0.0567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 0 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 500
Discounting practice with old adv calculation: [1e-05, -0.00899, 0.00024, -0.01056, 0.00205, -0.00883, 0.00214, -0.00339, -0.00326, 0.00327, -0.01545, 0.01188, -0.03646, 0.02432, -0.07025, 0.04314, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.09524, -4.19927, 1.26111, -5.42536, 1.42337, -2.25737, -0.00326, 0.00327, -11.41561, 8.77514, -27.04361, 18.03822, -52.27024, 32.09654, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 1000
Discounting practice with old adv calculation: [1e-05, -0.00891, 0.00016, -0.01046, 0.00186, -0.0089, 0.00202, -0.00325, -0.00353, 0.00353, -0.01599, 0.01207, -0.03719, 0.02425, -0.07092, 0.04247, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.05591, -3.75271, 1.13771, -5.42955, 1.34175, -2.1628, -0.00353, 0.00353, -11.91644, 8.992, -27.83286, 18.14734, -53.24367, 31.88627, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 1500
Discounting practice with old adv calculation: [1e-05, -0.00887, 0.00012, -0.01038, 0.00175, -0.00894, 0.00194, -0.00316, -0.00371, 0.00372, -0.01636, 0.01219, -0.03769, 0.0242, -0.07137, 0.04203, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [-0.0, -0.0, 0.03908, -3.44349, 1.05863, -5.41321, 1.28401, -2.09123, -0.00371, 0.00372, -12.23689, 9.11856, -28.31449, 18.18112, -53.79548, 31.68024, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 2000
Discounting practice with old adv calculation: [1e-05, -0.00885, 0.0001, -0.01032, 0.00166, -0.00896, 0.00188, -0.00308, -0.00385, 0.00385, -0.01664, 0.01228, -0.03806, 0.02416, -0.0717, 0.04169, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.02983, -3.20936, 1.00126, -5.39261, 1.23928, -2.03361, -0.00385, 0.00385, -12.47396, 9.20657, -28.66064, 18.19098, -54.17362, 31.49966, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 2500
Discounting practice with old adv calculation: [1e-05, -0.00884, 8e-05, -0.01027, 0.0016, -0.00898, 0.00183, -0.00302, -0.00396, 0.00397, -0.01686, 0.01235, -0.03836, 0.02412, -0.07197, 0.04142, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.02401, -3.02199, 0.95661, -5.37153, 1.20268, -1.98521, -0.00396, 0.00397, -12.66305, 9.27361, -28.93123, 18.19095, -54.45936, 31.3418, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 3000
Discounting practice with old adv calculation: [1e-05, -0.00883, 7e-05, -0.01023, 0.00155, -0.00899, 0.00179, -0.00296, -0.00406, 0.00406, -0.01706, 0.01241, -0.03862, 0.02409, -0.0722, 0.04119, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.02003, -2.86638, 0.92027, -5.35106, 1.17165, -1.94333, -0.00406, 0.00406, -12.82094, 9.32757, -29.1538, 18.18621, -54.68845, 31.20223, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 3500
Discounting practice with old adv calculation: [1e-05, -0.00882, 6e-05, -0.01019, 0.0015, -0.009, 0.00175, -0.00292, -0.00414, 0.00415, -0.01723, 0.01246, -0.03884, 0.02406, -0.0724, 0.041, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.01713, -2.73367, 0.88974, -5.33152, 1.14467, -1.90632, -0.00414, 0.00415, -12.95691, 9.37266, -29.34321, 18.17908, -54.87949, 31.0773, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 4000
Discounting practice with old adv calculation: [1e-05, -0.00881, 6e-05, -0.01016, 0.00146, -0.009, 0.00172, -0.00287, -0.00421, 0.00422, -0.01738, 0.01251, -0.03904, 0.02404, -0.07257, 0.04082, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.01493, -2.61823, 0.86349, -5.31297, 1.12076, -1.87308, -0.00421, 0.00422, -13.07661, 9.41134, -29.50836, 18.1707, -55.04335, 30.96422, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 4500
Discounting practice with old adv calculation: [1e-05, -0.00881, 5e-05, -0.01013, 0.00143, -0.00901, 0.00169, -0.00284, -0.00428, 0.00429, -0.01751, 0.01255, -0.03921, 0.02402, -0.07272, 0.04067, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.01321, -2.51626, 0.84053, -5.29536, 1.09926, -1.84286, -0.00428, 0.00429, -13.18375, 9.4452, -29.65499, 18.16166, -55.18684, 30.86089, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 5000
Discounting practice with old adv calculation: [1e-05, -0.0088, 5e-05, -0.0101, 0.0014, -0.00901, 0.00167, -0.0028, -0.00434, 0.00435, -0.01763, 0.01258, -0.03938, 0.024, -0.07287, 0.04053, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.01183, -2.42507, 0.82015, -5.27863, 1.07972, -1.8151, -0.00434, 0.00435, -13.2809, 9.47529, -29.78702, 18.15232, -55.31455, 30.76571, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 5500
Discounting practice with old adv calculation: [1e-05, -0.0088, 5e-05, -0.01008, 0.00137, -0.00902, 0.00164, -0.00277, -0.0044, 0.00441, -0.01775, 0.01261, -0.03952, 0.02398, -0.07299, 0.0404, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.0107, -2.34269, 0.80186, -5.2627, 1.06178, -1.78939, -0.0044, 0.00441, -13.36989, 9.50237, -29.90725, 18.14287, -55.42967, 30.67743, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 6000
Discounting practice with old adv calculation: [1e-05, -0.0088, 4e-05, -0.01006, 0.00135, -0.00902, 0.00162, -0.00274, -0.00445, 0.00446, -0.01785, 0.01264, -0.03966, 0.02396, -0.07311, 0.04028, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00975, -2.26766, 0.78528, -5.24751, 1.04519, -1.76542, -0.00445, 0.00446, -13.4521, 9.52698, -30.01773, 18.13344, -55.53453, 30.59505, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 6500
Discounting practice with old adv calculation: [1e-05, -0.0088, 4e-05, -0.01004, 0.00133, -0.00902, 0.0016, -0.00271, -0.0045, 0.00451, -0.01795, 0.01267, -0.03979, 0.02394, -0.07322, 0.04017, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00895, -2.19883, 0.77015, -5.23298, 1.02975, -1.74295, -0.0045, 0.00451, -13.52859, 9.54955, -30.12003, 18.12409, -55.63087, 30.51781, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 7000
Discounting practice with old adv calculation: [1e-05, -0.00879, 4e-05, -0.01002, 0.00131, -0.00903, 0.00158, -0.00268, -0.00454, 0.00455, -0.01804, 0.0127, -0.0399, 0.02392, -0.07333, 0.04007, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00826, -2.13531, 0.75623, -5.21906, 1.01529, -1.72178, -0.00454, 0.00455, -13.60017, 9.57037, -30.21536, 18.11486, -55.72001, 30.44504, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 7500
Discounting practice with old adv calculation: [1e-05, -0.00879, 4e-05, -0.01001, 0.00129, -0.00903, 0.00157, -0.00266, -0.00459, 0.00459, -0.01813, 0.01272, -0.04002, 0.02391, -0.07342, 0.03997, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00766, -2.07638, 0.74336, -5.20569, 1.00169, -1.70174, -0.00459, 0.00459, -13.66749, 9.58971, -30.30467, 18.10579, -55.80301, 30.37623, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 8000
Discounting practice with old adv calculation: [1e-05, -0.00879, 4e-05, -0.00999, 0.00127, -0.00903, 0.00155, -0.00264, -0.00463, 0.00464, -0.01821, 0.01274, -0.04012, 0.02389, -0.07352, 0.03988, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00714, -2.02146, 0.7314, -5.19282, 0.98885, -1.6827, -0.00463, 0.00464, -13.73109, 9.60777, -30.38875, 18.09689, -55.88069, 30.31093, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 8500
Discounting practice with old adv calculation: [1e-05, -0.00879, 3e-05, -0.00997, 0.00126, -0.00903, 0.00153, -0.00261, -0.00467, 0.00467, -0.01829, 0.01276, -0.04022, 0.02388, -0.0736, 0.03979, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00668, -1.97007, 0.72022, -5.18042, 0.97667, -1.66457, -0.00467, 0.00467, -13.79141, 9.6247, -30.46822, 18.08817, -55.95374, 30.24877, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 9000
Discounting practice with old adv calculation: [1e-05, -0.00879, 3e-05, -0.00996, 0.00124, -0.00903, 0.00152, -0.00259, -0.0047, 0.00471, -0.01836, 0.01278, -0.04032, 0.02386, -0.07369, 0.03971, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00627, -1.92181, 0.70975, -5.16844, 0.96509, -1.64724, -0.0047, 0.00471, -13.84879, 9.64064, -30.54361, 18.07962, -56.02271, 30.18944, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 9500
Discounting practice with old adv calculation: [1e-05, -0.00879, 3e-05, -0.00995, 0.00123, -0.00903, 0.0015, -0.00257, -0.00474, 0.00475, -0.01844, 0.0128, -0.04041, 0.02385, -0.07376, 0.03963, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00591, -1.87635, 0.6999, -5.15686, 0.95405, -1.63063, -0.00474, 0.00475, -13.90356, 9.6557, -30.61535, 18.07124, -56.08805, 30.13267, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 10000
Discounting practice with old adv calculation: [1e-05, -0.00879, 3e-05, -0.00994, 0.00121, -0.00903, 0.00149, -0.00255, -0.00477, 0.00478, -0.0185, 0.01282, -0.0405, 0.02384, -0.07384, 0.03955, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00558, -1.83339, 0.69061, -5.14564, 0.94349, -1.61469, -0.00477, 0.00478, -13.95597, 9.66997, -30.68382, 18.06304, -56.15015, 30.07822, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 10500
Discounting practice with old adv calculation: [1e-05, -0.00878, 3e-05, -0.00992, 0.0012, -0.00903, 0.00148, -0.00253, -0.0048, 0.00481, -0.01857, 0.01284, -0.04058, 0.02383, -0.07391, 0.03948, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00529, -1.7927, 0.68181, -5.13477, 0.93337, -1.59934, -0.0048, 0.00481, -14.00624, 9.68354, -30.74933, 18.055, -56.20934, 30.0259, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 11000
Discounting practice with old adv calculation: [1e-05, -0.00878, 3e-05, -0.00991, 0.00119, -0.00903, 0.00147, -0.00251, -0.00484, 0.00484, -0.01863, 0.01285, -0.04066, 0.02381, -0.07398, 0.03941, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00502, -1.75407, 0.67346, -5.1242, 0.92366, -1.58454, -0.00484, 0.00484, -14.05456, 9.69647, -30.81215, 18.04713, -56.26591, 29.97553, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 11500
Discounting practice with old adv calculation: [1e-05, -0.00878, 3e-05, -0.0099, 0.00118, -0.00903, 0.00145, -0.0025, -0.00487, 0.00487, -0.01869, 0.01287, -0.04074, 0.0238, -0.07405, 0.03935, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00477, -1.7173, 0.66553, -5.11393, 0.9143, -1.57025, -0.00487, 0.00487, -14.1011, 9.70882, -30.87252, 18.03941, -56.32009, 29.92695, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 12000
Discounting practice with old adv calculation: [1e-05, -0.00878, 3e-05, -0.00989, 0.00116, -0.00903, 0.00144, -0.00248, -0.0049, 0.0049, -0.01875, 0.01289, -0.04081, 0.02379, -0.07411, 0.03928, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00455, -1.68225, 0.65796, -5.10393, 0.90529, -1.55643, -0.0049, 0.0049, -14.146, 9.72064, -30.93065, 18.03184, -56.37209, 29.88003, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 12500
Discounting practice with old adv calculation: [1e-05, -0.00878, 3e-05, -0.00988, 0.00115, -0.00903, 0.00143, -0.00246, -0.00492, 0.00493, -0.01881, 0.0129, -0.04088, 0.02378, -0.07417, 0.03922, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00434, -1.64877, 0.65073, -5.0942, 0.89658, -1.54304, -0.00492, 0.00493, -14.1894, 9.73198, -30.98672, 18.02442, -56.4221, 29.83464, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 13000
Discounting practice with old adv calculation: [1e-05, -0.00878, 3e-05, -0.00987, 0.00114, -0.00903, 0.00142, -0.00245, -0.00495, 0.00496, -0.01886, 0.01291, -0.04095, 0.02377, -0.07423, 0.03916, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00415, -1.61674, 0.64382, -5.0847, 0.88817, -1.53005, -0.00495, 0.00496, -14.2314, 9.74287, -31.04088, 18.01714, -56.47028, 29.79068, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 13500
Discounting practice with old adv calculation: [1e-05, -0.00878, 2e-05, -0.00986, 0.00113, -0.00903, 0.00141, -0.00243, -0.00498, 0.00498, -0.01892, 0.01293, -0.04102, 0.02376, -0.07429, 0.0391, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00398, -1.58604, 0.63719, -5.07543, 0.88002, -1.51743, -0.00498, 0.00498, -14.27211, 9.75335, -31.09328, 18.00999, -56.51676, 29.74805, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 14000
Discounting practice with old adv calculation: [1e-05, -0.00878, 2e-05, -0.00985, 0.00112, -0.00903, 0.0014, -0.00241, -0.005, 0.00501, -0.01897, 0.01294, -0.04109, 0.02375, -0.07435, 0.03905, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00381, -1.55658, 0.63083, -5.06638, 0.87212, -1.50516, -0.005, 0.00501, -14.31161, 9.76345, -31.14404, 18.00297, -56.56168, 29.70665, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 14500
Discounting practice with old adv calculation: [1e-05, -0.00878, 2e-05, -0.00985, 0.00111, -0.00903, 0.00139, -0.0024, -0.00503, 0.00504, -0.01902, 0.01295, -0.04115, 0.02374, -0.0744, 0.03899, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00366, -1.52827, 0.62472, -5.05753, 0.86445, -1.49322, -0.00503, 0.00504, -14.35, 9.7732, -31.19328, 17.99607, -56.60515, 29.66642, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 15000
Discounting practice with old adv calculation: [1e-05, -0.00878, 2e-05, -0.00984, 0.00111, -0.00903, 0.00138, -0.00239, -0.00505, 0.00506, -0.01907, 0.01297, -0.04121, 0.02373, -0.07445, 0.03894, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00352, -1.50104, 0.61883, -5.04887, 0.85699, -1.48159, -0.00505, 0.00506, -14.38733, 9.78262, -31.24109, 17.98929, -56.64726, 29.62728, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 15500
Discounting practice with old adv calculation: [1e-05, -0.00878, 2e-05, -0.00983, 0.0011, -0.00902, 0.00137, -0.00237, -0.00508, 0.00508, -0.01912, 0.01298, -0.04127, 0.02372, -0.0745, 0.03889, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00339, -1.4748, 0.61316, -5.0404, 0.84974, -1.47024, -0.00508, 0.00508, -14.42368, 9.79173, -31.28758, 17.98262, -56.68811, 29.58916, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 16000
Discounting practice with old adv calculation: [1e-05, -0.00878, 2e-05, -0.00982, 0.00109, -0.00902, 0.00136, -0.00236, -0.0051, 0.00511, -0.01916, 0.01299, -0.04133, 0.02371, -0.07455, 0.03884, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00327, -1.4495, 0.60768, -5.03209, 0.84268, -1.45916, -0.0051, 0.00511, -14.4591, 9.80056, -31.33281, 17.97607, -56.72779, 29.55201, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 16500
Discounting practice with old adv calculation: [1e-05, -0.00878, 2e-05, -0.00982, 0.00108, -0.00902, 0.00135, -0.00234, -0.00512, 0.00513, -0.01921, 0.013, -0.04139, 0.0237, -0.0746, 0.03879, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00315, -1.42508, 0.60239, -5.02395, 0.8358, -1.44834, -0.00512, 0.00513, -14.49366, 9.80912, -31.37687, 17.96961, -56.76635, 29.51576, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 17000
Discounting practice with old adv calculation: [1e-05, -0.00878, 2e-05, -0.00981, 0.00107, -0.00902, 0.00134, -0.00233, -0.00514, 0.00515, -0.01925, 0.01301, -0.04144, 0.02369, -0.07465, 0.03874, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00304, -1.40148, 0.59728, -5.01597, 0.82909, -1.43776, -0.00514, 0.00515, -14.5274, 9.81742, -31.41983, 17.96326, -56.80388, 29.48038, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 17500
Discounting practice with old adv calculation: [1e-05, -0.00878, 2e-05, -0.0098, 0.00107, -0.00902, 0.00134, -0.00232, -0.00516, 0.00517, -0.0193, 0.01302, -0.0415, 0.02369, -0.0747, 0.0387, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00294, -1.37865, 0.59233, -5.00814, 0.82254, -1.42741, -0.00516, 0.00517, -14.56036, 9.82549, -31.46174, 17.95701, -56.84043, 29.44582, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 18000
Discounting practice with old adv calculation: [1e-05, -0.00878, 2e-05, -0.00979, 0.00106, -0.00902, 0.00133, -0.00231, -0.00519, 0.00519, -0.01934, 0.01303, -0.04155, 0.02368, -0.07474, 0.03865, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00285, -1.35656, 0.58753, -5.00044, 0.81615, -1.41728, -0.00519, 0.00519, -14.59259, 9.83334, -31.50267, 17.95084, -56.87606, 29.41202, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 18500
Discounting practice with old adv calculation: [1e-05, -0.00878, 2e-05, -0.00979, 0.00105, -0.00902, 0.00132, -0.00229, -0.00521, 0.00521, -0.01938, 0.01304, -0.0416, 0.02367, -0.07479, 0.03861, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00276, -1.33516, 0.58288, -4.99289, 0.80989, -1.40735, -0.00521, 0.00521, -14.62412, 9.84097, -31.54267, 17.94477, -56.91082, 29.37896, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 19000
Discounting practice with old adv calculation: [1e-05, -0.00878, 2e-05, -0.00978, 0.00105, -0.00902, 0.00131, -0.00228, -0.00523, 0.00523, -0.01942, 0.01305, -0.04165, 0.02366, -0.07483, 0.03856, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00267, -1.31441, 0.57836, -4.98546, 0.80377, -1.39762, -0.00523, 0.00523, -14.655, 9.84841, -31.58179, 17.93879, -56.94475, 29.3466, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 19500
Discounting practice with old adv calculation: [1e-05, -0.00878, 2e-05, -0.00978, 0.00104, -0.00902, 0.0013, -0.00227, -0.00525, 0.00525, -0.01946, 0.01306, -0.0417, 0.02365, -0.07487, 0.03852, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00259, -1.29428, 0.57398, -4.97816, 0.79778, -1.38808, -0.00525, 0.00525, -14.68525, 9.85566, -31.62006, 17.93288, -56.9779, 29.3149, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 20000
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00977, 0.00103, -0.00902, 0.0013, -0.00226, -0.00527, 0.00527, -0.0195, 0.01307, -0.04175, 0.02364, -0.07491, 0.03848, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00251, -1.27473, 0.56971, -4.97098, 0.79192, -1.37872, -0.00527, 0.00527, -14.7149, 9.86273, -31.65754, 17.92706, -57.01032, 29.28384, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 20500
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00976, 0.00103, -0.00902, 0.00129, -0.00225, -0.00528, 0.00529, -0.01954, 0.01308, -0.0418, 0.02364, -0.07495, 0.03844, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00244, -1.25574, 0.56557, -4.96391, 0.78617, -1.36952, -0.00528, 0.00529, -14.74398, 9.86963, -31.69426, 17.92132, -57.04203, 29.25338, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 21000
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00976, 0.00102, -0.00901, 0.00128, -0.00223, -0.0053, 0.00531, -0.01958, 0.01309, -0.04185, 0.02363, -0.07499, 0.0384, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00237, -1.23727, 0.56153, -4.95695, 0.78053, -1.36049, -0.0053, 0.00531, -14.77253, 9.87636, -31.73025, 17.91565, -57.07306, 29.2235, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 21500
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00975, 0.00102, -0.00901, 0.00127, -0.00222, -0.00532, 0.00533, -0.01961, 0.0131, -0.04189, 0.02362, -0.07503, 0.03836, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [-0.0, -0.0, 0.0023, -1.21932, 0.5576, -4.9501, 0.775, -1.35161, -0.00532, 0.00533, -14.80055, 9.88295, -31.76555, 17.91005, -57.10347, 29.19417, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 22000
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00975, 0.00101, -0.00901, 0.00127, -0.00221, -0.00534, 0.00535, -0.01965, 0.01311, -0.04194, 0.02361, -0.07507, 0.03832, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00224, -1.20184, 0.55376, -4.94335, 0.76958, -1.34289, -0.00534, 0.00535, -14.82808, 9.88938, -31.8002, 17.90452, -57.13326, 29.16537, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 22500
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00974, 0.001, -0.00901, 0.00126, -0.0022, -0.00536, 0.00537, -0.01969, 0.01311, -0.04198, 0.02361, -0.07511, 0.03828, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00218, -1.18481, 0.55002, -4.93669, 0.76425, -1.3343, -0.00536, 0.00537, -14.85514, 9.89568, -31.83421, 17.89906, -57.16247, 29.13708, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 23000
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00974, 0.001, -0.00901, 0.00125, -0.00219, -0.00537, 0.00538, -0.01972, 0.01312, -0.04203, 0.0236, -0.07515, 0.03825, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00212, -1.16823, 0.54638, -4.93013, 0.75901, -1.32585, -0.00537, 0.00538, -14.88174, 9.90184, -31.86762, 17.89366, -57.19113, 29.10927, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 23500
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00973, 0.00099, -0.00901, 0.00125, -0.00218, -0.00539, 0.0054, -0.01976, 0.01313, -0.04207, 0.02359, -0.07518, 0.03821, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00207, -1.15206, 0.54282, -4.92367, 0.75387, -1.31754, -0.00539, 0.0054, -14.9079, 9.90787, -31.90045, 17.88833, -57.21925, 29.08193, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 24000
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00973, 0.00099, -0.00901, 0.00124, -0.00217, -0.00541, 0.00542, -0.01979, 0.01314, -0.04211, 0.02358, -0.07522, 0.03818, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00201, -1.13629, 0.53934, -4.91728, 0.74881, -1.30935, -0.00541, 0.00542, -14.93365, 9.91379, -31.93273, 17.88306, -57.24687, 29.05504, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 24500
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00972, 0.00098, -0.00901, 0.00123, -0.00216, -0.00543, 0.00543, -0.01982, 0.01314, -0.04216, 0.02358, -0.07525, 0.03814, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00196, -1.1209, 0.53594, -4.91099, 0.74383, -1.30128, -0.00543, 0.00543, -14.959, 9.91958, -31.96447, 17.87785, -57.274, 29.02859, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 25000
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00972, 0.00098, -0.009, 0.00123, -0.00215, -0.00544, 0.00545, -0.01986, 0.01315, -0.0422, 0.02357, -0.07529, 0.03811, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00191, -1.10588, 0.53261, -4.90477, 0.73894, -1.29333, -0.00544, 0.00545, -14.98396, 9.92526, -31.9957, 17.87269, -57.30066, 29.00254, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 25500
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00971, 0.00097, -0.009, 0.00122, -0.00214, -0.00546, 0.00547, -0.01989, 0.01316, -0.04224, 0.02356, -0.07532, 0.03807, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00187, -1.09121, 0.52936, -4.89864, 0.73412, -1.28549, -0.00546, 0.00547, -15.00855, 9.93083, -32.02644, 17.86759, -57.32687, 28.9769, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 26000
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00971, 0.00097, -0.009, 0.00122, -0.00213, -0.00547, 0.00548, -0.01992, 0.01317, -0.04228, 0.02356, -0.07535, 0.03804, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00182, -1.07688, 0.52618, -4.89258, 0.72937, -1.27776, -0.00547, 0.00548, -15.03278, 9.9363, -32.0567, 17.86255, -57.35265, 28.95164, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 26500
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.0097, 0.00096, -0.009, 0.00121, -0.00212, -0.00549, 0.0055, -0.01995, 0.01317, -0.04232, 0.02355, -0.07539, 0.03801, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00178, -1.06287, 0.52307, -4.88659, 0.72469, -1.27014, -0.00549, 0.0055, -15.05666, 9.94167, -32.08651, 17.85756, -57.37801, 28.92676, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 27000
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.0097, 0.00096, -0.009, 0.0012, -0.00211, -0.0055, 0.00551, -0.01998, 0.01318, -0.04235, 0.02354, -0.07542, 0.03797, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00174, -1.04918, 0.52001, -4.88068, 0.72009, -1.26262, -0.0055, 0.00551, -15.08022, 9.94695, -32.11588, 17.85262, -57.40297, 28.90223, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 27500
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00969, 0.00095, -0.009, 0.0012, -0.0021, -0.00552, 0.00553, -0.02001, 0.01319, -0.04239, 0.02354, -0.07545, 0.03794, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0017, -1.03578, 0.51702, -4.87483, 0.71555, -1.25519, -0.00552, 0.00553, -15.10345, 9.95213, -32.14482, 17.84772, -57.42755, 28.87805, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 28000
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00969, 0.00095, -0.009, 0.00119, -0.00209, -0.00553, 0.00554, -0.02005, 0.0132, -0.04243, 0.02353, -0.07548, 0.03791, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00166, -1.02267, 0.51409, -4.86905, 0.71107, -1.24787, -0.00553, 0.00554, -15.12637, 9.95722, -32.17336, 17.84288, -57.45175, 28.85421, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 28500
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00969, 0.00095, -0.009, 0.00119, -0.00208, -0.00555, 0.00556, -0.02008, 0.0132, -0.04247, 0.02352, -0.07551, 0.03788, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00162, -1.00984, 0.51122, -4.86334, 0.70666, -1.24063, -0.00555, 0.00556, -15.14899, 9.96222, -32.2015, 17.83808, -57.4756, 28.83069, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 29000
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00968, 0.00094, -0.00899, 0.00118, -0.00208, -0.00556, 0.00557, -0.0201, 0.01321, -0.0425, 0.02352, -0.07554, 0.03785, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00159, -0.99728, 0.5084, -4.85769, 0.7023, -1.23348, -0.00556, 0.00557, -15.17132, 9.96714, -32.22925, 17.83333, -57.4991, 28.80748, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 29500
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00968, 0.00094, -0.00899, 0.00118, -0.00207, -0.00558, 0.00559, -0.02013, 0.01321, -0.04254, 0.02351, -0.07557, 0.03782, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00156, -0.98498, 0.50564, -4.8521, 0.698, -1.22642, -0.00558, 0.00559, -15.19337, 9.97199, -32.25664, 17.82863, -57.52226, 28.78458, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 30000
Discounting practice with old adv calculation: [1e-05, -0.00877, 2e-05, -0.00967, 0.00093, -0.00899, 0.00117, -0.00206, -0.00559, 0.0056, -0.02016, 0.01322, -0.04258, 0.02351, -0.0756, 0.03779, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00152, -0.97293, 0.50292, -4.84657, 0.69376, -1.21944, -0.00559, 0.0056, -15.21515, 9.97675, -32.28367, 17.82396, -57.54511, 28.76197, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 30500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00967, 0.00093, -0.00899, 0.00117, -0.00205, -0.00561, 0.00562, -0.02019, 0.01323, -0.04261, 0.0235, -0.07563, 0.03776, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00149, -0.96111, 0.50026, -4.8411, 0.68958, -1.21254, -0.00561, 0.00562, -15.23667, 9.98144, -32.31035, 17.81934, -57.56764, 28.73965, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 31000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00967, 0.00093, -0.00899, 0.00116, -0.00204, -0.00562, 0.00563, -0.02022, 0.01323, -0.04265, 0.02349, -0.07566, 0.03773, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00146, -0.94953, 0.49764, -4.83569, 0.68544, -1.20572, -0.00562, 0.00563, -15.25793, 9.98606, -32.3367, 17.81476, -57.58986, 28.7176, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 31500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00966, 0.00092, -0.00899, 0.00115, -0.00203, -0.00564, 0.00564, -0.02025, 0.01324, -0.04268, 0.02349, -0.07569, 0.0377, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00143, -0.93818, 0.49507, -4.83033, 0.68136, -1.19897, -0.00564, 0.00564, -15.27895, 9.9906, -32.36273, 17.81022, -57.6118, 28.69582, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 32000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00966, 0.00092, -0.00899, 0.00115, -0.00202, -0.00565, 0.00566, -0.02028, 0.01325, -0.04271, 0.02348, -0.07572, 0.03767, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0014, -0.92704, 0.49254, -4.82502, 0.67732, -1.1923, -0.00565, 0.00566, -15.29972, 9.99508, -32.38843, 17.80571, -57.63345, 28.67431, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 32500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00966, 0.00091, -0.00899, 0.00114, -0.00202, -0.00566, 0.00567, -0.0203, 0.01325, -0.04275, 0.02348, -0.07575, 0.03764, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00137, -0.91612, 0.49006, -4.81977, 0.67333, -1.18571, -0.00566, 0.00567, -15.32027, 9.9995, -32.41384, 17.80125, -57.65482, 28.65304, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 33000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00965, 0.00091, -0.00898, 0.00114, -0.00201, -0.00568, 0.00568, -0.02033, 0.01326, -0.04278, 0.02347, -0.07578, 0.03762, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00135, -0.9054, 0.48761, -4.81456, 0.66939, -1.17918, -0.00568, 0.00568, -15.34058, 10.00385, -32.43894, 17.79682, -57.67593, 28.63202, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 33500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00965, 0.00091, -0.00898, 0.00113, -0.002, -0.00569, 0.0057, -0.02036, 0.01326, -0.04281, 0.02346, -0.0758, 0.03759, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00132, -0.89487, 0.48521, -4.80941, 0.6655, -1.17272, -0.00569, 0.0057, -15.36068, 10.00813, -32.46376, 17.79243, -57.69678, 28.61124, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 34000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00965, 0.0009, -0.00898, 0.00113, -0.00199, -0.0057, 0.00571, -0.02038, 0.01327, -0.04285, 0.02346, -0.07583, 0.03756, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0013, -0.88454, 0.48285, -4.8043, 0.66165, -1.16632, -0.0057, 0.00571, -15.38056, 10.01236, -32.4883, 17.78807, -57.71738, 28.59069, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 34500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00964, 0.0009, -0.00898, 0.00112, -0.00198, -0.00572, 0.00572, -0.02041, 0.01327, -0.04288, 0.02345, -0.07586, 0.03754, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00127, -0.8744, 0.48052, -4.79924, 0.65784, -1.15999, -0.00572, 0.00572, -15.40024, 10.01653, -32.51257, 17.78375, -57.73774, 28.57037, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 35000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00964, 0.0009, -0.00898, 0.00112, -0.00198, -0.00573, 0.00574, -0.02043, 0.01328, -0.04291, 0.02345, -0.07588, 0.03751, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00125, -0.86443, 0.47824, -4.79423, 0.65407, -1.15372, -0.00573, 0.00574, -15.41972, 10.02064, -32.53657, 17.77946, -57.75785, 28.55026, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 35500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00964, 0.00089, -0.00898, 0.00112, -0.00197, -0.00574, 0.00575, -0.02046, 0.01329, -0.04294, 0.02344, -0.07591, 0.03748, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00122, -0.85464, 0.47598, -4.78926, 0.65034, -1.14751, -0.00574, 0.00575, -15.43899, 10.0247, -32.56032, 17.77521, -57.77774, 28.53037, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 36000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00963, 0.00089, -0.00898, 0.00111, -0.00196, -0.00575, 0.00576, -0.02049, 0.01329, -0.04297, 0.02344, -0.07593, 0.03746, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0012, -0.84502, 0.47376, -4.78434, 0.64665, -1.14136, -0.00575, 0.00576, -15.45808, 10.0287, -32.58381, 17.77099, -57.7974, 28.51069, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 36500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00963, 0.00089, -0.00898, 0.00111, -0.00195, -0.00577, 0.00577, -0.02051, 0.0133, -0.043, 0.02343, -0.07596, 0.03743, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00118, -0.83556, 0.47158, -4.77945, 0.643, -1.13527, -0.00577, 0.00577, -15.47698, 10.03266, -32.60706, 17.76679, -57.81685, 28.49121, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 37000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00963, 0.00088, -0.00897, 0.0011, -0.00194, -0.00578, 0.00579, -0.02054, 0.0133, -0.04303, 0.02343, -0.07599, 0.03741, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00116, -0.82627, 0.46943, -4.77461, 0.63939, -1.12924, -0.00578, 0.00579, -15.4957, 10.03656, -32.63008, 17.76263, -57.83608, 28.47193, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 37500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00962, 0.00088, -0.00897, 0.0011, -0.00194, -0.00579, 0.0058, -0.02056, 0.01331, -0.04306, 0.02342, -0.07601, 0.03738, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00114, -0.81713, 0.4673, -4.76981, 0.63581, -1.12326, -0.00579, 0.0058, -15.51425, 10.04041, -32.65286, 17.7585, -57.85511, 28.45284, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 38000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00962, 0.00088, -0.00897, 0.00109, -0.00193, -0.0058, 0.00581, -0.02059, 0.01331, -0.04309, 0.02342, -0.07604, 0.03736, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00112, -0.80814, 0.46521, -4.76505, 0.63227, -1.11733, -0.0058, 0.00581, -15.53262, 10.04421, -32.67542, 17.7544, -57.87393, 28.43394, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 38500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00962, 0.00087, -0.00897, 0.00109, -0.00192, -0.00581, 0.00582, -0.02061, 0.01332, -0.04312, 0.02341, -0.07606, 0.03733, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.0011, -0.7993, 0.46315, -4.76032, 0.62876, -1.11146, -0.00581, 0.00582, -15.55082, 10.04797, -32.69776, 17.75032, -57.89256, 28.41522, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 39000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00961, 0.00087, -0.00897, 0.00108, -0.00192, -0.00583, 0.00584, -0.02063, 0.01332, -0.04315, 0.0234, -0.07608, 0.03731, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00108, -0.7906, 0.46112, -4.75564, 0.62529, -1.10563, -0.00583, 0.00584, -15.56886, 10.05168, -32.71989, 17.74628, -57.911, 28.39669, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 39500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00961, 0.00087, -0.00897, 0.00108, -0.00191, -0.00584, 0.00585, -0.02066, 0.01333, -0.04318, 0.0234, -0.07611, 0.03728, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00106, -0.78205, 0.45912, -4.75099, 0.62185, -1.09986, -0.00584, 0.00585, -15.58674, 10.05535, -32.7418, 17.74226, -57.92926, 28.37832, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 40000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00961, 0.00086, -0.00897, 0.00107, -0.0019, -0.00585, 0.00586, -0.02068, 0.01333, -0.04321, 0.02339, -0.07613, 0.03726, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00104, -0.77362, 0.45715, -4.74638, 0.61844, -1.09414, -0.00585, 0.00586, -15.60446, 10.05898, -32.76351, 17.73827, -57.94733, 28.36013, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 40500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00961, 0.00086, -0.00896, 0.00107, -0.00189, -0.00586, 0.00587, -0.0207, 0.01334, -0.04324, 0.02339, -0.07616, 0.03724, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00103, -0.76534, 0.4552, -4.7418, 0.61506, -1.08846, -0.00586, 0.00587, -15.62204, 10.06256, -32.78503, 17.7343, -57.96522, 28.3421, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 41000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.0096, 0.00086, -0.00896, 0.00107, -0.00189, -0.00587, 0.00588, -0.02073, 0.01334, -0.04327, 0.02338, -0.07618, 0.03721, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00101, -0.75718, 0.45327, -4.73726, 0.61171, -1.08283, -0.00587, 0.00588, -15.63946, 10.0661, -32.80635, 17.73036, -57.98295, 28.32424, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 41500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.0096, 0.00085, -0.00896, 0.00106, -0.00188, -0.00588, 0.00589, -0.02075, 0.01335, -0.0433, 0.02338, -0.0762, 0.03719, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00099, -0.74914, 0.45138, -4.73275, 0.6084, -1.07724, -0.00588, 0.00589, -15.65674, 10.0696, -32.82748, 17.72645, -58.0005, 28.30653, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 42000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.0096, 0.00085, -0.00896, 0.00106, -0.00187, -0.0059, 0.0059, -0.02077, 0.01335, -0.04332, 0.02337, -0.07623, 0.03717, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00098, -0.74123, 0.44951, -4.72827, 0.60511, -1.0717, -0.0059, 0.0059, -15.67387, 10.07306, -32.84842, 17.72256, -58.01789, 28.28898, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 42500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00959, 0.00085, -0.00896, 0.00105, -0.00187, -0.00591, 0.00592, -0.0208, 0.01336, -0.04335, 0.02337, -0.07625, 0.03714, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00096, -0.73344, 0.44766, -4.72383, 0.60185, -1.0662, -0.00591, 0.00592, -15.69087, 10.07649, -32.86919, 17.71869, -58.03512, 28.27159, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 43000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00959, 0.00085, -0.00896, 0.00105, -0.00186, -0.00592, 0.00593, -0.02082, 0.01336, -0.04338, 0.02336, -0.07627, 0.03712, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00094, -0.72576, 0.44583, -4.71942, 0.59861, -1.06075, -0.00592, 0.00593, -15.70773, 10.07987, -32.88977, 17.71485, -58.05219, 28.25434, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 43500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00959, 0.00084, -0.00896, 0.00104, -0.00185, -0.00593, 0.00594, -0.02084, 0.01336, -0.04341, 0.02336, -0.07629, 0.0371, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00093, -0.7182, 0.44403, -4.71504, 0.59541, -1.05534, -0.00593, 0.00594, -15.72445, 10.08322, -32.91019, 17.71103, -58.0691, 28.23723, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 44000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00959, 0.00084, -0.00896, 0.00104, -0.00185, -0.00594, 0.00595, -0.02086, 0.01337, -0.04343, 0.02335, -0.07632, 0.03708, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00092, -0.71075, 0.44225, -4.71069, 0.59223, -1.04996, -0.00594, 0.00595, -15.74105, 10.08654, -32.93043, 17.70723, -58.08587, 28.22027, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 44500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00958, 0.00084, -0.00895, 0.00104, -0.00184, -0.00595, 0.00596, -0.02089, 0.01337, -0.04346, 0.02335, -0.07634, 0.03705, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0009, -0.70341, 0.4405, -4.70637, 0.58908, -1.04463, -0.00595, 0.00596, -15.75752, 10.08982, -32.95051, 17.70346, -58.10249, 28.20345, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 45000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00958, 0.00084, -0.00895, 0.00103, -0.00183, -0.00596, 0.00597, -0.02091, 0.01338, -0.04349, 0.02334, -0.07636, 0.03703, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00089, -0.69617, 0.43876, -4.70208, 0.58595, -1.03933, -0.00596, 0.00597, -15.77387, 10.09306, -32.97043, 17.69971, -58.11897, 28.18676, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 45500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00958, 0.00083, -0.00895, 0.00103, -0.00183, -0.00597, 0.00598, -0.02093, 0.01338, -0.04351, 0.02334, -0.07638, 0.03701, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00087, -0.68904, 0.43705, -4.69782, 0.58285, -1.03408, -0.00597, 0.00598, -15.79009, 10.09628, -32.99019, 17.69598, -58.13531, 28.17021, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 46000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00958, 0.00083, -0.00895, 0.00102, -0.00182, -0.00598, 0.00599, -0.02095, 0.01339, -0.04354, 0.02333, -0.0764, 0.03699, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00086, -0.68201, 0.43535, -4.69359, 0.57977, -1.02886, -0.00598, 0.00599, -15.8062, 10.09946, -33.00979, 17.69227, -58.1515, 28.15379, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 46500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00957, 0.00083, -0.00895, 0.00102, -0.00181, -0.00599, 0.006, -0.02097, 0.01339, -0.04356, 0.02333, -0.07642, 0.03697, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00085, -0.67508, 0.43368, -4.68938, 0.57671, -1.02368, -0.00599, 0.006, -15.82218, 10.1026, -33.02924, 17.68858, -58.16757, 28.1375, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 47000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00957, 0.00083, -0.00895, 0.00102, -0.00181, -0.006, 0.00601, -0.02099, 0.01339, -0.04359, 0.02333, -0.07645, 0.03695, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00083, -0.66824, 0.43202, -4.6852, 0.57368, -1.01853, -0.006, 0.00601, -15.83806, 10.10572, -33.04855, 17.68492, -58.1835, 28.12133, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 47500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00957, 0.00082, -0.00895, 0.00101, -0.0018, -0.00601, 0.00602, -0.02101, 0.0134, -0.04361, 0.02332, -0.07647, 0.03693, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00082, -0.6615, 0.43039, -4.68105, 0.57067, -1.01342, -0.00601, 0.00602, -15.85382, 10.1088, -33.0677, 17.68127, -58.19931, 28.10529, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 48000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00957, 0.00082, -0.00895, 0.00101, -0.00179, -0.00602, 0.00603, -0.02103, 0.0134, -0.04364, 0.02332, -0.07649, 0.03691, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00081, -0.65485, 0.42877, -4.67693, 0.56769, -1.00834, -0.00602, 0.00603, -15.86947, 10.11186, -33.08672, 17.67764, -58.21498, 28.08936, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 48500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00956, 0.00082, -0.00894, 0.00101, -0.00179, -0.00603, 0.00604, -0.02106, 0.01341, -0.04366, 0.02331, -0.07651, 0.03689, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.0008, -0.64829, 0.42717, -4.67283, 0.56472, -1.0033, -0.00603, 0.00604, -15.88501, 10.11489, -33.10559, 17.67403, -58.23054, 28.07356, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 49000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00956, 0.00082, -0.00894, 0.001, -0.00178, -0.00604, 0.00605, -0.02108, 0.01341, -0.04369, 0.02331, -0.07653, 0.03686, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00079, -0.64182, 0.42559, -4.66875, 0.56178, -0.99829, -0.00604, 0.00605, -15.90045, 10.11788, -33.12433, 17.67044, -58.24597, 28.05787, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 49500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00956, 0.00081, -0.00894, 0.001, -0.00177, -0.00606, 0.00606, -0.0211, 0.01342, -0.04371, 0.0233, -0.07655, 0.03684, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00077, -0.63544, 0.42403, -4.66471, 0.55886, -0.99332, -0.00606, 0.00606, -15.91579, 10.12085, -33.14293, 17.66687, -58.26129, 28.04229, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 50000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00956, 0.00081, -0.00894, 0.00099, -0.00177, -0.00607, 0.00607, -0.02112, 0.01342, -0.04374, 0.0233, -0.07657, 0.03682, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00076, -0.62914, 0.42248, -4.66068, 0.55596, -0.98837, -0.00607, 0.00607, -15.93102, 10.12379, -33.1614, 17.66332, -58.27648, 28.02683, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 50500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00955, 0.00081, -0.00894, 0.00099, -0.00176, -0.00608, 0.00608, -0.02114, 0.01342, -0.04376, 0.02329, -0.07659, 0.0368, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00075, -0.62293, 0.42095, -4.65668, 0.55308, -0.98346, -0.00608, 0.00608, -15.94615, 10.12671, -33.17974, 17.65978, -58.29157, 28.01147, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 51000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00955, 0.00081, -0.00894, 0.00099, -0.00176, -0.00609, 0.00609, -0.02116, 0.01343, -0.04379, 0.02329, -0.07661, 0.03678, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00074, -0.61679, 0.41944, -4.6527, 0.55022, -0.97858, -0.00609, 0.00609, -15.96118, 10.1296, -33.19795, 17.65627, -58.30654, 27.99623, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 51500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00955, 0.0008, -0.00894, 0.00098, -0.00175, -0.00609, 0.0061, -0.02118, 0.01343, -0.04381, 0.02328, -0.07663, 0.03676, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00073, -0.61074, 0.41794, -4.64875, 0.54738, -0.97372, -0.00609, 0.0061, -15.97612, 10.13246, -33.21604, 17.65277, -58.3214, 27.98108, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 52000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00955, 0.0008, -0.00893, 0.00098, -0.00174, -0.0061, 0.00611, -0.0212, 0.01343, -0.04384, 0.02328, -0.07665, 0.03674, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00072, -0.60476, 0.41646, -4.64482, 0.54456, -0.9689, -0.0061, 0.00611, -15.99097, 10.1353, -33.23401, 17.64929, -58.33616, 27.96605, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 52500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00955, 0.0008, -0.00893, 0.00098, -0.00174, -0.00611, 0.00612, -0.02122, 0.01344, -0.04386, 0.02328, -0.07667, 0.03673, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00071, -0.59886, 0.415, -4.64091, 0.54175, -0.96411, -0.00611, 0.00612, -16.00572, 10.13811, -33.25186, 17.64582, -58.35081, 27.95111, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 53000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00954, 0.0008, -0.00893, 0.00097, -0.00173, -0.00612, 0.00613, -0.02124, 0.01344, -0.04388, 0.02327, -0.07669, 0.03671, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0007, -0.59303, 0.41355, -4.63702, 0.53897, -0.95934, -0.00612, 0.00613, -16.02038, 10.14089, -33.26959, 17.64237, -58.36535, 27.93627, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 53500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00954, 0.00079, -0.00893, 0.00097, -0.00172, -0.00613, 0.00614, -0.02126, 0.01345, -0.04391, 0.02327, -0.07671, 0.03669, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00069, -0.58728, 0.41211, -4.63315, 0.5362, -0.95461, -0.00613, 0.00614, -16.03495, 10.14366, -33.2872, 17.63894, -58.37979, 27.92153, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 54000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00954, 0.00079, -0.00893, 0.00097, -0.00172, -0.00614, 0.00615, -0.02127, 0.01345, -0.04393, 0.02326, -0.07673, 0.03667, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00068, -0.5816, 0.41069, -4.62931, 0.53345, -0.9499, -0.00614, 0.00615, -16.04944, 10.1464, -33.3047, 17.63552, -58.39414, 27.90689, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 54500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00954, 0.00079, -0.00893, 0.00096, -0.00171, -0.00615, 0.00616, -0.02129, 0.01345, -0.04395, 0.02326, -0.07674, 0.03665, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00067, -0.57599, 0.40928, -4.62549, 0.53072, -0.94522, -0.00615, 0.00616, -16.06383, 10.14911, -33.32209, 17.63212, -58.40838, 27.89235, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 55000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00953, 0.00079, -0.00893, 0.00096, -0.00171, -0.00616, 0.00617, -0.02131, 0.01346, -0.04398, 0.02325, -0.07676, 0.03663, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00066, -0.57045, 0.40789, -4.62168, 0.52801, -0.94056, -0.00616, 0.00617, -16.07815, 10.1518, -33.33936, 17.62874, -58.42253, 27.87789, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 55500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00953, 0.00079, -0.00893, 0.00095, -0.0017, -0.00617, 0.00618, -0.02133, 0.01346, -0.044, 0.02325, -0.07678, 0.03661, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00065, -0.56498, 0.40651, -4.6179, 0.52531, -0.93593, -0.00617, 0.00618, -16.09238, 10.15447, -33.35653, 17.62537, -58.43658, 27.86353, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 56000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00953, 0.00078, -0.00892, 0.00095, -0.0017, -0.00618, 0.00619, -0.02135, 0.01346, -0.04402, 0.02324, -0.0768, 0.03659, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00065, -0.55957, 0.40514, -4.61414, 0.52263, -0.93133, -0.00618, 0.00619, -16.10653, 10.15712, -33.3736, 17.62201, -58.45054, 27.84926, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 56500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00953, 0.00078, -0.00892, 0.00095, -0.00169, -0.00619, 0.0062, -0.02137, 0.01347, -0.04404, 0.02324, -0.07682, 0.03657, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00064, -0.55423, 0.40379, -4.61039, 0.51997, -0.92675, -0.00619, 0.0062, -16.12059, 10.15975, -33.39056, 17.61867, -58.46441, 27.83507, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 57000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00953, 0.00078, -0.00892, 0.00094, -0.00168, -0.0062, 0.00621, -0.02139, 0.01347, -0.04407, 0.02324, -0.07684, 0.03656, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00063, -0.54895, 0.40245, -4.60667, 0.51732, -0.9222, -0.0062, 0.00621, -16.13458, 10.16235, -33.40742, 17.61535, -58.47819, 27.82098, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 57500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00952, 0.00078, -0.00892, 0.00094, -0.00168, -0.00621, 0.00622, -0.02141, 0.01348, -0.04409, 0.02323, -0.07686, 0.03654, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00062, -0.54374, 0.40112, -4.60297, 0.51469, -0.91767, -0.00621, 0.00622, -16.14849, 10.16494, -33.42417, 17.61203, -58.49188, 27.80697, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 58000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00952, 0.00078, -0.00892, 0.00094, -0.00167, -0.00622, 0.00623, -0.02143, 0.01348, -0.04411, 0.02323, -0.07687, 0.03652, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00061, -0.53859, 0.39981, -4.59928, 0.51207, -0.91317, -0.00622, 0.00623, -16.16233, 10.1675, -33.44083, 17.60874, -58.50549, 27.79304, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 58500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00952, 0.00077, -0.00892, 0.00093, -0.00167, -0.00623, 0.00624, -0.02144, 0.01348, -0.04413, 0.02322, -0.07689, 0.0365, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0006, -0.5335, 0.39851, -4.59561, 0.50947, -0.90869, -0.00623, 0.00624, -16.17609, 10.17005, -33.45739, 17.60545, -58.519, 27.77919, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 59000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00952, 0.00077, -0.00892, 0.00093, -0.00166, -0.00624, 0.00625, -0.02146, 0.01349, -0.04416, 0.02322, -0.07691, 0.03648, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.0006, -0.52846, 0.39722, -4.59196, 0.50688, -0.90423, -0.00624, 0.00625, -16.18977, 10.17257, -33.47386, 17.60218, -58.53244, 27.76543, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 59500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00952, 0.00077, -0.00892, 0.00093, -0.00166, -0.00625, 0.00625, -0.02148, 0.01349, -0.04418, 0.02321, -0.07693, 0.03647, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00059, -0.52349, 0.39594, -4.58833, 0.50431, -0.8998, -0.00625, 0.00625, -16.20339, 10.17507, -33.49023, 17.59893, -58.54579, 27.75175, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 60000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00952, 0.00077, -0.00891, 0.00092, -0.00165, -0.00625, 0.00626, -0.0215, 0.01349, -0.0442, 0.02321, -0.07695, 0.03645, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00058, -0.51857, 0.39467, -4.58472, 0.50176, -0.89539, -0.00625, 0.00626, -16.21693, 10.17756, -33.50651, 17.59569, -58.55906, 27.73814, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 60500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00951, 0.00077, -0.00891, 0.00092, -0.00164, -0.00626, 0.00627, -0.02152, 0.0135, -0.04422, 0.02321, -0.07696, 0.03643, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00057, -0.51371, 0.39342, -4.58112, 0.49921, -0.891, -0.00626, 0.00627, -16.2304, 10.18002, -33.52269, 17.59246, -58.57225, 27.72462, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 61000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00951, 0.00076, -0.00891, 0.00092, -0.00164, -0.00627, 0.00628, -0.02153, 0.0135, -0.04424, 0.0232, -0.07698, 0.03641, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00057, -0.50891, 0.39217, -4.57754, 0.49668, -0.88664, -0.00627, 0.00628, -16.2438, 10.18247, -33.53879, 17.58924, -58.58537, 27.71117, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 61500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00951, 0.00076, -0.00891, 0.00091, -0.00163, -0.00628, 0.00629, -0.02155, 0.0135, -0.04426, 0.0232, -0.077, 0.03639, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00056, -0.50416, 0.39094, -4.57398, 0.49417, -0.88229, -0.00628, 0.00629, -16.25713, 10.1849, -33.5548, 17.58604, -58.5984, 27.6978, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 62000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00951, 0.00076, -0.00891, 0.00091, -0.00163, -0.00629, 0.0063, -0.02157, 0.01351, -0.04428, 0.02319, -0.07702, 0.03638, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00055, -0.49946, 0.38972, -4.57043, 0.49167, -0.87797, -0.00629, 0.0063, -16.2704, 10.18731, -33.57072, 17.58284, -58.61136, 27.68449, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 62500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00951, 0.00076, -0.00891, 0.00091, -0.00162, -0.0063, 0.00631, -0.02159, 0.01351, -0.04431, 0.02319, -0.07703, 0.03636, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00055, -0.49482, 0.3885, -4.5669, 0.48918, -0.87367, -0.0063, 0.00631, -16.2836, 10.1897, -33.58656, 17.57967, -58.62424, 27.67127, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 63000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.0095, 0.00076, -0.00891, 0.00091, -0.00162, -0.00631, 0.00632, -0.02161, 0.01351, -0.04433, 0.02319, -0.07705, 0.03634, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00054, -0.49023, 0.3873, -4.56339, 0.4867, -0.86939, -0.00631, 0.00632, -16.29673, 10.19208, -33.60231, 17.5765, -58.63705, 27.65811, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 63500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.0095, 0.00075, -0.00891, 0.0009, -0.00161, -0.00632, 0.00632, -0.02162, 0.01352, -0.04435, 0.02318, -0.07707, 0.03633, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00053, -0.48569, 0.38611, -4.55989, 0.48424, -0.86512, -0.00632, 0.00632, -16.3098, 10.19444, -33.61798, 17.57334, -58.64979, 27.64503, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 64000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.0095, 0.00075, -0.0089, 0.0009, -0.00161, -0.00632, 0.00633, -0.02164, 0.01352, -0.04437, 0.02318, -0.07708, 0.03631, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00053, -0.4812, 0.38493, -4.55641, 0.48179, -0.86088, -0.00632, 0.00633, -16.32281, 10.19678, -33.63357, 17.5702, -58.66246, 27.63201, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 64500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.0095, 0.00075, -0.0089, 0.0009, -0.0016, -0.00633, 0.00634, -0.02166, 0.01352, -0.04439, 0.02317, -0.0771, 0.03629, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00052, -0.47676, 0.38376, -4.55294, 0.47936, -0.85666, -0.00633, 0.00634, -16.33575, 10.1991, -33.64907, 17.56707, -58.67505, 27.61907, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 65000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.0095, 0.00075, -0.0089, 0.00089, -0.0016, -0.00634, 0.00635, -0.02167, 0.01352, -0.04441, 0.02317, -0.07712, 0.03628, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00051, -0.47236, 0.3826, -4.54949, 0.47693, -0.85246, -0.00634, 0.00635, -16.34864, 10.20141, -33.6645, 17.56395, -58.68758, 27.60619, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 65500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.0095, 0.00075, -0.0089, 0.00089, -0.00159, -0.00635, 0.00636, -0.02169, 0.01353, -0.04443, 0.02317, -0.07713, 0.03626, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00051, -0.46802, 0.38145, -4.54606, 0.47452, -0.84828, -0.00635, 0.00636, -16.36146, 10.20371, -33.67985, 17.56084, -58.70004, 27.59338, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 66000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00949, 0.00074, -0.0089, 0.00089, -0.00158, -0.00636, 0.00637, -0.02171, 0.01353, -0.04445, 0.02316, -0.07715, 0.03624, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.0005, -0.46372, 0.3803, -4.54263, 0.47212, -0.84412, -0.00636, 0.00637, -16.37422, 10.20598, -33.69512, 17.55775, -58.71243, 27.58063, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 66500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00949, 0.00074, -0.0089, 0.00088, -0.00158, -0.00637, 0.00638, -0.02173, 0.01353, -0.04447, 0.02316, -0.07717, 0.03623, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00049, -0.45947, 0.37917, -4.53923, 0.46973, -0.83997, -0.00637, 0.00638, -16.38693, 10.20824, -33.71032, 17.55466, -58.72475, 27.56795, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 67000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00949, 0.00074, -0.0089, 0.00088, -0.00157, -0.00637, 0.00638, -0.02174, 0.01354, -0.04449, 0.02315, -0.07718, 0.03621, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00049, -0.45526, 0.37805, -4.53584, 0.46736, -0.83585, -0.00637, 0.00638, -16.39958, 10.21049, -33.72544, 17.55158, -58.73701, 27.55534, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 67500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00949, 0.00074, -0.00889, 0.00088, -0.00157, -0.00638, 0.00639, -0.02176, 0.01354, -0.04451, 0.02315, -0.0772, 0.03619, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00048, -0.4511, 0.37693, -4.53246, 0.46499, -0.83174, -0.00638, 0.00639, -16.41217, 10.21272, -33.74049, 17.54852, -58.74921, 27.54278, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 68000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00949, 0.00074, -0.00889, 0.00087, -0.00156, -0.00639, 0.0064, -0.02178, 0.01354, -0.04453, 0.02315, -0.07722, 0.03618, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00048, -0.44698, 0.37583, -4.52909, 0.46264, -0.82765, -0.00639, 0.0064, -16.4247, 10.21493, -33.75546, 17.54547, -58.76134, 27.53029, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 68500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00948, 0.00074, -0.00889, 0.00087, -0.00156, -0.0064, 0.00641, -0.02179, 0.01355, -0.04455, 0.02314, -0.07723, 0.03616, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00047, -0.4429, 0.37473, -4.52574, 0.4603, -0.82357, -0.0064, 0.00641, -16.43718, 10.21713, -33.77037, 17.54242, -58.77341, 27.51786, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 69000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00948, 0.00073, -0.00889, 0.00087, -0.00155, -0.00641, 0.00642, -0.02181, 0.01355, -0.04457, 0.02314, -0.07725, 0.03614, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00047, -0.43887, 0.37364, -4.52241, 0.45797, -0.81952, -0.00641, 0.00642, -16.4496, 10.21932, -33.7852, 17.53939, -58.78541, 27.50549, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 69500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00948, 0.00073, -0.00889, 0.00087, -0.00155, -0.00642, 0.00643, -0.02183, 0.01355, -0.04459, 0.02313, -0.07726, 0.03613, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00046, -0.43488, 0.37256, -4.51908, 0.45565, -0.81548, -0.00642, 0.00643, -16.46197, 10.22149, -33.79996, 17.53637, -58.79736, 27.49318, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 70000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00948, 0.00073, -0.00889, 0.00086, -0.00154, -0.00642, 0.00643, -0.02184, 0.01356, -0.04461, 0.02313, -0.07728, 0.03611, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00045, -0.43093, 0.37149, -4.51577, 0.45334, -0.81146, -0.00642, 0.00643, -16.47429, 10.22364, -33.81466, 17.53336, -58.80925, 27.48093, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 70500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00948, 0.00073, -0.00889, 0.00086, -0.00154, -0.00643, 0.00644, -0.02186, 0.01356, -0.04463, 0.02313, -0.0773, 0.0361, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00045, -0.42702, 0.37043, -4.51248, 0.45104, -0.80746, -0.00643, 0.00644, -16.48655, 10.22579, -33.82928, 17.53035, -58.82107, 27.46874, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 71000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00948, 0.00073, -0.00889, 0.00086, -0.00153, -0.00644, 0.00645, -0.02188, 0.01356, -0.04465, 0.02312, -0.07731, 0.03608, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00044, -0.42316, 0.36937, -4.50919, 0.44875, -0.80347, -0.00644, 0.00645, -16.49876, 10.22791, -33.84384, 17.52736, -58.83284, 27.4566, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 71500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00948, 0.00073, -0.00888, 0.00085, -0.00153, -0.00645, 0.00646, -0.02189, 0.01356, -0.04467, 0.02312, -0.07733, 0.03606, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00044, -0.41933, 0.36833, -4.50592, 0.44647, -0.7995, -0.00645, 0.00646, -16.51092, 10.23003, -33.85834, 17.52438, -58.84455, 27.44452, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 72000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00947, 0.00072, -0.00888, 0.00085, -0.00152, -0.00646, 0.00647, -0.02191, 0.01357, -0.04469, 0.02312, -0.07734, 0.03605, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00043, -0.41554, 0.36729, -4.50267, 0.4442, -0.79554, -0.00646, 0.00647, -16.52303, 10.23213, -33.87276, 17.5214, -58.85621, 27.4325, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 72500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00947, 0.00072, -0.00888, 0.00085, -0.00152, -0.00646, 0.00647, -0.02192, 0.01357, -0.04471, 0.02311, -0.07736, 0.03603, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00043, -0.41178, 0.36626, -4.49942, 0.44195, -0.7916, -0.00646, 0.00647, -16.53509, 10.23422, -33.88713, 17.51844, -58.86781, 27.42053, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 73000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00947, 0.00072, -0.00888, 0.00084, -0.00151, -0.00647, 0.00648, -0.02194, 0.01357, -0.04473, 0.02311, -0.07737, 0.03602, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00042, -0.40807, 0.36523, -4.49619, 0.4397, -0.78768, -0.00647, 0.00648, -16.54711, 10.23629, -33.90143, 17.51548, -58.87935, 27.40862, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 73500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00947, 0.00072, -0.00888, 0.00084, -0.00151, -0.00648, 0.00649, -0.02196, 0.01358, -0.04475, 0.0231, -0.07739, 0.036, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00042, -0.40439, 0.36422, -4.49297, 0.43746, -0.78377, -0.00648, 0.00649, -16.55907, 10.23836, -33.91567, 17.51254, -58.89084, 27.39676, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 74000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00947, 0.00072, -0.00888, 0.00084, -0.0015, -0.00649, 0.0065, -0.02197, 0.01358, -0.04476, 0.0231, -0.07741, 0.03599, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00041, -0.40075, 0.36321, -4.48976, 0.43523, -0.77988, -0.00649, 0.0065, -16.57098, 10.2404, -33.92985, 17.5096, -58.90227, 27.38495, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 74500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00947, 0.00072, -0.00888, 0.00084, -0.0015, -0.0065, 0.00651, -0.02199, 0.01358, -0.04478, 0.0231, -0.07742, 0.03597, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00041, -0.39715, 0.36221, -4.48656, 0.43301, -0.776, -0.0065, 0.00651, -16.58285, 10.24244, -33.94396, 17.50668, -58.91365, 27.3732, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 75000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00946, 0.00072, -0.00888, 0.00083, -0.00149, -0.0065, 0.00651, -0.022, 0.01358, -0.0448, 0.02309, -0.07744, 0.03596, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0004, -0.39358, 0.36121, -4.48338, 0.4308, -0.77214, -0.0065, 0.00651, -16.59468, 10.24447, -33.95802, 17.50376, -58.92498, 27.36149, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 75500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00946, 0.00071, -0.00887, 0.00083, -0.00149, -0.00651, 0.00652, -0.02202, 0.01359, -0.04482, 0.02309, -0.07745, 0.03594, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0004, -0.39005, 0.36022, -4.4802, 0.4286, -0.76829, -0.00651, 0.00652, -16.60645, 10.24648, -33.97202, 17.50085, -58.93626, 27.34984, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 76000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00946, 0.00071, -0.00887, 0.00083, -0.00148, -0.00652, 0.00653, -0.02204, 0.01359, -0.04484, 0.02309, -0.07747, 0.03593, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.0004, -0.38655, 0.35924, -4.47704, 0.42641, -0.76446, -0.00652, 0.00653, -16.61818, 10.24848, -33.98595, 17.49795, -58.94749, 27.33824, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 76500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00946, 0.00071, -0.00887, 0.00082, -0.00148, -0.00653, 0.00654, -0.02205, 0.01359, -0.04486, 0.02308, -0.07748, 0.03591, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00039, -0.38308, 0.35827, -4.47389, 0.42423, -0.76064, -0.00653, 0.00654, -16.62987, 10.25047, -33.99983, 17.49505, -58.95867, 27.32668, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 77000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00946, 0.00071, -0.00887, 0.00082, -0.00147, -0.00654, 0.00654, -0.02207, 0.01359, -0.04488, 0.02308, -0.0775, 0.0359, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00039, -0.37965, 0.3573, -4.47075, 0.42205, -0.75683, -0.00654, 0.00654, -16.64151, 10.25244, -34.01366, 17.49217, -58.96979, 27.31518, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 77500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00946, 0.00071, -0.00887, 0.00082, -0.00147, -0.00654, 0.00655, -0.02208, 0.0136, -0.04489, 0.02307, -0.07751, 0.03588, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00038, -0.37625, 0.35635, -4.46762, 0.41989, -0.75304, -0.00654, 0.00655, -16.65311, 10.25441, -34.02742, 17.48929, -58.98087, 27.30372, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False  True  True  True  True  True  True  True  True  True]
Iteration 78000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00946, 0.00071, -0.00887, 0.00082, -0.00146, -0.00655, 0.00656, -0.0221, 0.0136, -0.04491, 0.02307, -0.07753, 0.03587, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00038, -0.37289, 0.35539, -4.46451, 0.41773, -0.74926, -0.00655, 0.00656, -16.66466, 10.25636, -34.04114, 17.48643, -58.9919, 27.29232, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 78500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00945, 0.0007, -0.00887, 0.00081, -0.00146, -0.00656, 0.00657, -0.02211, 0.0136, -0.04493, 0.02307, -0.07754, 0.03585, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00037, -0.36955, 0.35445, -4.4614, 0.41559, -0.7455, -0.00656, 0.00657, -16.67618, 10.2583, -34.05479, 17.48357, -59.00288, 27.28096, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 79000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00945, 0.0007, -0.00887, 0.00081, -0.00145, -0.00657, 0.00658, -0.02213, 0.01361, -0.04495, 0.02306, -0.07756, 0.03584, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00037, -0.36625, 0.35351, -4.4583, 0.41345, -0.74175, -0.00657, 0.00658, -16.68765, 10.26023, -34.06839, 17.48072, -59.01382, 27.26964, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 79500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00945, 0.0007, -0.00886, 0.00081, -0.00145, -0.00657, 0.00658, -0.02214, 0.01361, -0.04497, 0.02306, -0.07757, 0.03582, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00037, -0.36298, 0.35257, -4.45522, 0.41132, -0.73801, -0.00657, 0.00658, -16.69907, 10.26215, -34.08194, 17.47787, -59.0247, 27.25838, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 80000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00945, 0.0007, -0.00886, 0.00081, -0.00145, -0.00658, 0.00659, -0.02216, 0.01361, -0.04499, 0.02306, -0.07758, 0.03581, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00036, -0.35974, 0.35165, -4.45215, 0.40919, -0.73429, -0.00658, 0.00659, -16.71046, 10.26406, -34.09544, 17.47504, -59.03554, 27.24715, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 80500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00945, 0.0007, -0.00886, 0.0008, -0.00144, -0.00659, 0.0066, -0.02217, 0.01361, -0.045, 0.02305, -0.0776, 0.03579, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00036, -0.35654, 0.35073, -4.44908, 0.40708, -0.73058, -0.00659, 0.0066, -16.72181, 10.26595, -34.10888, 17.47221, -59.04634, 27.23598, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 81000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00945, 0.0007, -0.00886, 0.0008, -0.00144, -0.0066, 0.00661, -0.02219, 0.01362, -0.04502, 0.02305, -0.07761, 0.03578, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00035, -0.35336, 0.34981, -4.44603, 0.40497, -0.72688, -0.0066, 0.00661, -16.73312, 10.26784, -34.12227, 17.46939, -59.05709, 27.22484, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 81500
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00945, 0.0007, -0.00886, 0.0008, -0.00143, -0.0066, 0.00661, -0.0222, 0.01362, -0.04504, 0.02305, -0.07763, 0.03576, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00035, -0.35021, 0.3489, -4.44298, 0.40288, -0.7232, -0.0066, 0.00661, -16.74438, 10.26972, -34.13561, 17.46658, -59.0678, 27.21376, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 82000
Discounting practice with old adv calculation: [1e-05, -0.00877, 1e-05, -0.00944, 0.00069, -0.00886, 0.00079, -0.00143, -0.00661, 0.00662, -0.02222, 0.01362, -0.04506, 0.02304, -0.07764, 0.03575, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00035, -0.34709, 0.348, -4.43995, 0.40079, -0.71952, -0.00661, 0.00662, -16.75561, 10.27158, -34.1489, 17.46378, -59.07846, 27.20271, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 82500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00944, 0.00069, -0.00886, 0.00079, -0.00142, -0.00662, 0.00663, -0.02223, 0.01362, -0.04508, 0.02304, -0.07766, 0.03574, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00034, -0.344, 0.3471, -4.43693, 0.3987, -0.71586, -0.00662, 0.00663, -16.7668, 10.27344, -34.16214, 17.46098, -59.08908, 27.19171, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 83000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00944, 0.00069, -0.00886, 0.00079, -0.00142, -0.00663, 0.00664, -0.02225, 0.01363, -0.04509, 0.02304, -0.07767, 0.03572, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00034, -0.34094, 0.34621, -4.43391, 0.39663, -0.71222, -0.00663, 0.00664, -16.77795, 10.27528, -34.17533, 17.45819, -59.09966, 27.18075, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 83500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00944, 0.00069, -0.00885, 0.00079, -0.00141, -0.00663, 0.00664, -0.02226, 0.01363, -0.04511, 0.02303, -0.07768, 0.03571, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00033, -0.33791, 0.34533, -4.43091, 0.39456, -0.70858, -0.00663, 0.00664, -16.78907, 10.27712, -34.18847, 17.45541, -59.11019, 27.16983, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 84000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00944, 0.00069, -0.00885, 0.00078, -0.00141, -0.00664, 0.00665, -0.02228, 0.01363, -0.04513, 0.02303, -0.0777, 0.03569, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00033, -0.3349, 0.34445, -4.42791, 0.3925, -0.70496, -0.00664, 0.00665, -16.80014, 10.27894, -34.20157, 17.45263, -59.12068, 27.15896, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 84500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00944, 0.00069, -0.00885, 0.00078, -0.0014, -0.00665, 0.00666, -0.02229, 0.01363, -0.04515, 0.02302, -0.07771, 0.03568, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00033, -0.33192, 0.34357, -4.42493, 0.39045, -0.70134, -0.00665, 0.00666, -16.81118, 10.28076, -34.21461, 17.44986, -59.13113, 27.14812, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 85000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00944, 0.00069, -0.00885, 0.00078, -0.0014, -0.00665, 0.00666, -0.02231, 0.01364, -0.04516, 0.02302, -0.07773, 0.03567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00032, -0.32897, 0.3427, -4.42195, 0.3884, -0.69774, -0.00665, 0.00666, -16.82219, 10.28256, -34.22761, 17.4471, -59.14154, 27.13733, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 85500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00943, 0.00068, -0.00885, 0.00078, -0.00139, -0.00666, 0.00667, -0.02232, 0.01364, -0.04518, 0.02302, -0.07774, 0.03565, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00032, -0.32605, 0.34184, -4.41898, 0.38637, -0.69416, -0.00666, 0.00667, -16.83315, 10.28436, -34.24056, 17.44435, -59.15191, 27.12658, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 86000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00943, 0.00068, -0.00885, 0.00077, -0.00139, -0.00667, 0.00668, -0.02234, 0.01364, -0.0452, 0.02301, -0.07776, 0.03564, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00032, -0.32315, 0.34098, -4.41603, 0.38434, -0.69058, -0.00667, 0.00668, -16.84408, 10.28614, -34.25347, 17.4416, -59.16224, 27.11586, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 86500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00943, 0.00068, -0.00885, 0.00077, -0.00139, -0.00668, 0.00669, -0.02235, 0.01364, -0.04521, 0.02301, -0.07777, 0.03562, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00031, -0.32028, 0.34013, -4.41308, 0.38231, -0.68701, -0.00668, 0.00669, -16.85498, 10.28792, -34.26633, 17.43886, -59.17253, 27.10519, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 87000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00943, 0.00068, -0.00885, 0.00077, -0.00138, -0.00668, 0.00669, -0.02237, 0.01365, -0.04523, 0.02301, -0.07778, 0.03561, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00031, -0.31743, 0.33929, -4.41014, 0.3803, -0.68346, -0.00668, 0.00669, -16.86584, 10.28969, -34.27914, 17.43613, -59.18278, 27.09455, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 87500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00943, 0.00068, -0.00884, 0.00077, -0.00138, -0.00669, 0.0067, -0.02238, 0.01365, -0.04525, 0.023, -0.0778, 0.0356, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00031, -0.31462, 0.33844, -4.40721, 0.37829, -0.67992, -0.00669, 0.0067, -16.87667, 10.29144, -34.29191, 17.4334, -59.19299, 27.08395, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 88000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00943, 0.00068, -0.00884, 0.00076, -0.00137, -0.0067, 0.00671, -0.0224, 0.01365, -0.04527, 0.023, -0.07781, 0.03558, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.0003, -0.31182, 0.33761, -4.40429, 0.37628, -0.67638, -0.0067, 0.00671, -16.88746, 10.29319, -34.30464, 17.43068, -59.20317, 27.07339, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 88500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00943, 0.00068, -0.00884, 0.00076, -0.00137, -0.00671, 0.00672, -0.02241, 0.01365, -0.04528, 0.023, -0.07782, 0.03557, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.0003, -0.30905, 0.33678, -4.40137, 0.37429, -0.67286, -0.00671, 0.00672, -16.89822, 10.29493, -34.31732, 17.42797, -59.2133, 27.06287, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 89000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00943, 0.00068, -0.00884, 0.00076, -0.00136, -0.00671, 0.00672, -0.02243, 0.01366, -0.0453, 0.02299, -0.07784, 0.03556, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0003, -0.30631, 0.33595, -4.39847, 0.3723, -0.66935, -0.00671, 0.00672, -16.90895, 10.29666, -34.32996, 17.42526, -59.2234, 27.05239, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 89500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00942, 0.00067, -0.00884, 0.00076, -0.00136, -0.00672, 0.00673, -0.02244, 0.01366, -0.04532, 0.02299, -0.07785, 0.03554, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00029, -0.30358, 0.33513, -4.39557, 0.37031, -0.66585, -0.00672, 0.00673, -16.91964, 10.29839, -34.34256, 17.42256, -59.23346, 27.04194, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 90000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00942, 0.00067, -0.00884, 0.00075, -0.00135, -0.00673, 0.00674, -0.02245, 0.01366, -0.04533, 0.02299, -0.07786, 0.03553, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00029, -0.30089, 0.33431, -4.39268, 0.36834, -0.66236, -0.00673, 0.00674, -16.9303, 10.3001, -34.35512, 17.41986, -59.24349, 27.03153, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 90500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00942, 0.00067, -0.00884, 0.00075, -0.00135, -0.00673, 0.00674, -0.02247, 0.01366, -0.04535, 0.02298, -0.07788, 0.03551, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00029, -0.29821, 0.3335, -4.38981, 0.36637, -0.65888, -0.00673, 0.00674, -16.94093, 10.3018, -34.36763, 17.41717, -59.25347, 27.02115, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 91000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00942, 0.00067, -0.00884, 0.00075, -0.00135, -0.00674, 0.00675, -0.02248, 0.01367, -0.04537, 0.02298, -0.07789, 0.0355, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00029, -0.29556, 0.3327, -4.38693, 0.3644, -0.65541, -0.00674, 0.00675, -16.95152, 10.3035, -34.3801, 17.41449, -59.26343, 27.01081, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 91500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00942, 0.00067, -0.00884, 0.00075, -0.00134, -0.00675, 0.00676, -0.0225, 0.01367, -0.04538, 0.02298, -0.0779, 0.03549, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00028, -0.29294, 0.33189, -4.38407, 0.36245, -0.65195, -0.00675, 0.00676, -16.96209, 10.30519, -34.39253, 17.41182, -59.27334, 27.0005, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 92000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00942, 0.00067, -0.00883, 0.00074, -0.00134, -0.00675, 0.00676, -0.02251, 0.01367, -0.0454, 0.02297, -0.07792, 0.03547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00028, -0.29033, 0.3311, -4.38122, 0.3605, -0.6485, -0.00675, 0.00676, -16.97262, 10.30687, -34.40493, 17.40915, -59.28323, 26.99023, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 92500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00942, 0.00067, -0.00883, 0.00074, -0.00133, -0.00676, 0.00677, -0.02252, 0.01367, -0.04542, 0.02297, -0.07793, 0.03546, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00028, -0.28775, 0.3303, -4.37837, 0.35855, -0.64507, -0.00676, 0.00677, -16.98312, 10.30854, -34.41728, 17.40648, -59.29307, 26.98, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 93000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00942, 0.00067, -0.00883, 0.00074, -0.00133, -0.00677, 0.00678, -0.02254, 0.01367, -0.04543, 0.02297, -0.07794, 0.03545, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [-0.0, -0.0, 0.00027, -0.28519, 0.32952, -4.37553, 0.35661, -0.64164, -0.00677, 0.00678, -16.99359, 10.3102, -34.42959, 17.40382, -59.30289, 26.9698, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 93500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00941, 0.00066, -0.00883, 0.00074, -0.00132, -0.00678, 0.00679, -0.02255, 0.01368, -0.04545, 0.02296, -0.07796, 0.03543, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00027, -0.28266, 0.32873, -4.3727, 0.35468, -0.63822, -0.00678, 0.00679, -17.00404, 10.31185, -34.44186, 17.40117, -59.31267, 26.95963, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 94000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00941, 0.00066, -0.00883, 0.00073, -0.00132, -0.00678, 0.00679, -0.02257, 0.01368, -0.04547, 0.02296, -0.07797, 0.03542, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00027, -0.28014, 0.32795, -4.36988, 0.35276, -0.63481, -0.00678, 0.00679, -17.01445, 10.3135, -34.4541, 17.39853, -59.32241, 26.9495, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 94500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00941, 0.00066, -0.00883, 0.00073, -0.00131, -0.00679, 0.0068, -0.02258, 0.01368, -0.04548, 0.02296, -0.07798, 0.03541, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00026, -0.27765, 0.32718, -4.36706, 0.35083, -0.63141, -0.00679, 0.0068, -17.02483, 10.31514, -34.46629, 17.39589, -59.33212, 26.93939, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 95000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00941, 0.00066, -0.00883, 0.00073, -0.00131, -0.0068, 0.00681, -0.02259, 0.01368, -0.0455, 0.02295, -0.078, 0.0354, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00026, -0.27518, 0.32641, -4.36425, 0.34892, -0.62802, -0.0068, 0.00681, -17.03518, 10.31677, -34.47845, 17.39325, -59.3418, 26.92933, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 95500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00941, 0.00066, -0.00883, 0.00073, -0.00131, -0.0068, 0.00681, -0.02261, 0.01369, -0.04551, 0.02295, -0.07801, 0.03538, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00026, -0.27272, 0.32564, -4.36145, 0.34701, -0.62464, -0.0068, 0.00681, -17.0455, 10.31839, -34.49057, 17.39062, -59.35145, 26.91929, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 96000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00941, 0.00066, -0.00882, 0.00072, -0.0013, -0.00681, 0.00682, -0.02262, 0.01369, -0.04553, 0.02295, -0.07802, 0.03537, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00026, -0.27029, 0.32488, -4.35866, 0.34511, -0.62127, -0.00681, 0.00682, -17.0558, 10.32, -34.50265, 17.388, -59.36107, 26.90929, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 96500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00941, 0.00066, -0.00882, 0.00072, -0.0013, -0.00682, 0.00683, -0.02264, 0.01369, -0.04555, 0.02294, -0.07804, 0.03536, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00025, -0.26788, 0.32413, -4.35588, 0.34321, -0.6179, -0.00682, 0.00683, -17.06607, 10.32161, -34.5147, 17.38538, -59.37065, 26.89932, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 97000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.00941, 0.00066, -0.00882, 0.00072, -0.00129, -0.00682, 0.00683, -0.02265, 0.01369, -0.04556, 0.02294, -0.07805, 0.03534, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00025, -0.26549, 0.32337, -4.3531, 0.34132, -0.61455, -0.00682, 0.00683, -17.0763, 10.32321, -34.52671, 17.38277, -59.3802, 26.88938, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 97500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.0094, 0.00065, -0.00882, 0.00072, -0.00129, -0.00683, 0.00684, -0.02266, 0.01369, -0.04558, 0.02294, -0.07806, 0.03533, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00025, -0.26313, 0.32262, -4.35033, 0.33943, -0.61121, -0.00683, 0.00684, -17.08651, 10.3248, -34.53868, 17.38016, -59.38972, 26.87947, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 98000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.0094, 0.00065, -0.00882, 0.00071, -0.00128, -0.00684, 0.00685, -0.02268, 0.0137, -0.0456, 0.02293, -0.07807, 0.03532, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00025, -0.26078, 0.32188, -4.34757, 0.33755, -0.60787, -0.00684, 0.00685, -17.0967, 10.32639, -34.55062, 17.37756, -59.39921, 26.86959, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 98500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.0094, 0.00065, -0.00882, 0.00071, -0.00128, -0.00684, 0.00685, -0.02269, 0.0137, -0.04561, 0.02293, -0.07809, 0.0353, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00024, -0.25845, 0.32114, -4.34481, 0.33568, -0.60454, -0.00684, 0.00685, -17.10685, 10.32796, -34.56252, 17.37496, -59.40867, 26.85974, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 99000
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.0094, 0.00065, -0.00882, 0.00071, -0.00128, -0.00685, 0.00686, -0.0227, 0.0137, -0.04563, 0.02293, -0.0781, 0.03529, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00024, -0.25614, 0.3204, -4.34206, 0.33381, -0.60123, -0.00685, 0.00686, -17.11698, 10.32953, -34.57439, 17.37237, -59.4181, 26.84992, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 99500
Discounting practice with old adv calculation: [1e-05, -0.00876, 1e-05, -0.0094, 0.00065, -0.00882, 0.00071, -0.00127, -0.00686, 0.00687, -0.02272, 0.0137, -0.04564, 0.02292, -0.07811, 0.03528, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00024, -0.25385, 0.31967, -4.33932, 0.33194, -0.59792, -0.00686, 0.00687, -17.12708, 10.3311, -34.58622, 17.36978, -59.42749, 26.84014, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 0
Discounting practice with old adv calculation: [0.07752, 0.06111, 0.07703, 0.0616, 0.07605, 0.06258, 0.07407, 0.06456, 0.07006, 0.06856, 0.06198, 0.07665, 0.04563, 0.093, 0.01262, 0.12601, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [0 0 0 0 0 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0082, -0.0082, 0.00772, -0.00772, 0.00674, -0.00674, 0.00475, -0.00475, 0.00075, -0.00075, -0.00734, 0.00734, -0.02368, 0.02368, -0.0567, 0.0567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 0 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 500
Discounting practice with old adv calculation: [0.00013, 0.66485, 0.00248, 0.37031, 0.02295, 0.15798, 0.05103, 0.09161, 0.06597, 0.07267, 0.06788, 0.0689, 0.05514, 0.07542, 0.02639, 0.09101, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.09524, -4.19927, 1.26111, -5.42536, 1.42337, -2.25737, -0.00326, 0.00327, -11.41561, 8.77514, -27.04361, 18.03822, -52.27024, 32.09654, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True  True  True  True  True  True]
Iteration 1000
Discounting practice with old adv calculation: [0.00013, 0.66494, 0.00163, 0.41168, 0.02089, 0.16641, 0.05028, 0.09275, 0.0657, 0.07293, 0.06839, 0.06829, 0.05577, 0.07445, 0.02728, 0.0894, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.05591, -3.75271, 1.13771, -5.42955, 1.34175, -2.1628, -0.00353, 0.00353, -11.91644, 8.992, -27.83286, 18.14734, -53.24367, 31.88627, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 1500
Discounting practice with old adv calculation: [0.00013, 0.66497, 0.00125, 0.43861, 0.01961, 0.17211, 0.04981, 0.09349, 0.06552, 0.07311, 0.06874, 0.06787, 0.05621, 0.0738, 0.02789, 0.08833, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [-0.0, -0.0, 0.03908, -3.44349, 1.05863, -5.41321, 1.28401, -2.09123, -0.00371, 0.00372, -12.23689, 9.11856, -28.31449, 18.18112, -53.79548, 31.68024, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 2000
Discounting practice with old adv calculation: [0.00013, 0.665, 0.00102, 0.45843, 0.01869, 0.17645, 0.04946, 0.09403, 0.06538, 0.07325, 0.06901, 0.06756, 0.05655, 0.0733, 0.02836, 0.08753, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.02983, -3.20936, 1.00126, -5.39261, 1.23928, -2.03361, -0.00385, 0.00385, -12.47396, 9.20657, -28.66064, 18.19098, -54.17362, 31.49966, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 2500
Discounting practice with old adv calculation: [0.00013, 0.66501, 0.00087, 0.47403, 0.01799, 0.17996, 0.04919, 0.09446, 0.06527, 0.07336, 0.06923, 0.0673, 0.05682, 0.0729, 0.02874, 0.08688, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.02401, -3.02199, 0.95661, -5.37153, 1.20268, -1.98521, -0.00396, 0.00397, -12.66305, 9.27361, -28.93123, 18.19095, -54.45936, 31.3418, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 3000
Discounting practice with old adv calculation: [0.00013, 0.66502, 0.00077, 0.48685, 0.01742, 0.18292, 0.04897, 0.09482, 0.06518, 0.07346, 0.06942, 0.06708, 0.05705, 0.07257, 0.02906, 0.08634, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.02003, -2.86638, 0.92027, -5.35106, 1.17165, -1.94333, -0.00406, 0.00406, -12.82094, 9.32757, -29.1538, 18.18621, -54.68845, 31.20223, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 3500
Discounting practice with old adv calculation: [0.00013, 0.66503, 0.00069, 0.4977, 0.01694, 0.18548, 0.04878, 0.09512, 0.06509, 0.07354, 0.06959, 0.06689, 0.05725, 0.07228, 0.02934, 0.08587, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.01713, -2.73367, 0.88974, -5.33152, 1.14467, -1.90632, -0.00414, 0.00415, -12.95691, 9.37266, -29.34321, 18.17908, -54.87949, 31.0773, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 4000
Discounting practice with old adv calculation: [0.00013, 0.66503, 0.00063, 0.50708, 0.01652, 0.18775, 0.04862, 0.09538, 0.06502, 0.07362, 0.06973, 0.06672, 0.05743, 0.07202, 0.02959, 0.08545, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.01493, -2.61823, 0.86349, -5.31297, 1.12076, -1.87308, -0.00421, 0.00422, -13.07661, 9.41134, -29.50836, 18.1707, -55.04335, 30.96422, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 4500
Discounting practice with old adv calculation: [0.00013, 0.66504, 0.00058, 0.51533, 0.01616, 0.18978, 0.04847, 0.09561, 0.06495, 0.07368, 0.06986, 0.06657, 0.05759, 0.07179, 0.02982, 0.08509, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.01321, -2.51626, 0.84053, -5.29536, 1.09926, -1.84286, -0.00428, 0.00429, -13.18375, 9.4452, -29.65499, 18.16166, -55.18684, 30.86089, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 5000
Discounting practice with old adv calculation: [0.00013, 0.66504, 0.00054, 0.52268, 0.01584, 0.19162, 0.04834, 0.09582, 0.06489, 0.07374, 0.06998, 0.06643, 0.05774, 0.07158, 0.03002, 0.08475, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.01183, -2.42507, 0.82015, -5.27863, 1.07972, -1.8151, -0.00434, 0.00435, -13.2809, 9.47529, -29.78702, 18.15232, -55.31455, 30.76571, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 5500
Discounting practice with old adv calculation: [0.00013, 0.66505, 0.0005, 0.52929, 0.01556, 0.19331, 0.04823, 0.09601, 0.06484, 0.0738, 0.07009, 0.06631, 0.05787, 0.07139, 0.03021, 0.08445, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.0107, -2.34269, 0.80186, -5.2627, 1.06178, -1.78939, -0.0044, 0.00441, -13.36989, 9.50237, -29.90725, 18.14287, -55.42967, 30.67743, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 6000
Discounting practice with old adv calculation: [0.00013, 0.66505, 0.00047, 0.5353, 0.0153, 0.19487, 0.04812, 0.09618, 0.06478, 0.07385, 0.07019, 0.06619, 0.058, 0.07121, 0.03039, 0.08416, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00975, -2.26766, 0.78528, -5.24751, 1.04519, -1.76542, -0.00445, 0.00446, -13.4521, 9.52698, -30.01773, 18.13344, -55.53453, 30.59505, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 6500
Discounting practice with old adv calculation: [0.00013, 0.66505, 0.00045, 0.5408, 0.01506, 0.19632, 0.04803, 0.09634, 0.06474, 0.0739, 0.07029, 0.06608, 0.05812, 0.07105, 0.03055, 0.0839, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00895, -2.19883, 0.77015, -5.23298, 1.02975, -1.74295, -0.0045, 0.00451, -13.52859, 9.54955, -30.12003, 18.12409, -55.63087, 30.51781, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 7000
Discounting practice with old adv calculation: [0.00013, 0.66505, 0.00042, 0.54587, 0.01484, 0.19768, 0.04794, 0.09649, 0.06469, 0.07395, 0.07038, 0.06598, 0.05823, 0.07089, 0.03071, 0.08366, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00826, -2.13531, 0.75623, -5.21906, 1.01529, -1.72178, -0.00454, 0.00455, -13.60017, 9.57037, -30.21536, 18.11486, -55.72001, 30.44504, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 7500
Discounting practice with old adv calculation: [0.00013, 0.66505, 0.00041, 0.55056, 0.01464, 0.19895, 0.04785, 0.09662, 0.06465, 0.07399, 0.07046, 0.06588, 0.05833, 0.07075, 0.03085, 0.08343, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00766, -2.07638, 0.74336, -5.20569, 1.00169, -1.70174, -0.00459, 0.00459, -13.66749, 9.58971, -30.30467, 18.10579, -55.80301, 30.37623, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 8000
Discounting practice with old adv calculation: [0.00013, 0.66506, 0.00039, 0.55492, 0.01445, 0.20016, 0.04777, 0.09675, 0.06461, 0.07403, 0.07054, 0.06579, 0.05843, 0.07061, 0.03099, 0.08321, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00714, -2.02146, 0.7314, -5.19282, 0.98885, -1.6827, -0.00463, 0.00464, -13.73109, 9.60777, -30.38875, 18.09689, -55.88069, 30.31093, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 8500
Discounting practice with old adv calculation: [0.00013, 0.66506, 0.00037, 0.559, 0.01427, 0.2013, 0.0477, 0.09687, 0.06457, 0.07407, 0.07062, 0.0657, 0.05852, 0.07048, 0.03112, 0.08301, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00668, -1.97007, 0.72022, -5.18042, 0.97667, -1.66457, -0.00467, 0.00467, -13.79141, 9.6247, -30.46822, 18.08817, -55.95374, 30.24877, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 9000
Discounting practice with old adv calculation: [0.00013, 0.66506, 0.00036, 0.56282, 0.01411, 0.20238, 0.04763, 0.09699, 0.06453, 0.07411, 0.07069, 0.06562, 0.05861, 0.07036, 0.03125, 0.08281, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00627, -1.92181, 0.70975, -5.16844, 0.96509, -1.64724, -0.0047, 0.00471, -13.84879, 9.64064, -30.54361, 18.07962, -56.02271, 30.18944, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 9500
Discounting practice with old adv calculation: [0.00013, 0.66506, 0.00035, 0.56641, 0.01395, 0.20341, 0.04757, 0.09709, 0.0645, 0.07414, 0.07076, 0.06554, 0.0587, 0.07024, 0.03137, 0.08263, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00591, -1.87635, 0.6999, -5.15686, 0.95405, -1.63063, -0.00474, 0.00475, -13.90356, 9.6557, -30.61535, 18.07124, -56.08805, 30.13267, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 10000
Discounting practice with old adv calculation: [0.00013, 0.66506, 0.00033, 0.5698, 0.01381, 0.2044, 0.04751, 0.0972, 0.06446, 0.07417, 0.07083, 0.06546, 0.05878, 0.07013, 0.03148, 0.08245, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00558, -1.83339, 0.69061, -5.14564, 0.94349, -1.61469, -0.00477, 0.00478, -13.95597, 9.66997, -30.68382, 18.06304, -56.15015, 30.07822, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 10500
Discounting practice with old adv calculation: [0.00013, 0.66506, 0.00032, 0.57301, 0.01367, 0.20534, 0.04745, 0.09729, 0.06443, 0.07421, 0.07089, 0.06539, 0.05886, 0.07002, 0.03159, 0.08228, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00529, -1.7927, 0.68181, -5.13477, 0.93337, -1.59934, -0.0048, 0.00481, -14.00624, 9.68354, -30.74933, 18.055, -56.20934, 30.0259, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 11000
Discounting practice with old adv calculation: [0.00013, 0.66506, 0.00031, 0.57605, 0.01353, 0.20625, 0.04739, 0.09738, 0.0644, 0.07424, 0.07095, 0.06532, 0.05894, 0.06992, 0.0317, 0.08212, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00502, -1.75407, 0.67346, -5.1242, 0.92366, -1.58454, -0.00484, 0.00484, -14.05456, 9.69647, -30.81215, 18.04713, -56.26591, 29.97553, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 11500
Discounting practice with old adv calculation: [0.00013, 0.66506, 0.00031, 0.57894, 0.01341, 0.20712, 0.04734, 0.09747, 0.06437, 0.07427, 0.07101, 0.06526, 0.05901, 0.06982, 0.0318, 0.08196, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00477, -1.7173, 0.66553, -5.11393, 0.9143, -1.57025, -0.00487, 0.00487, -14.1011, 9.70882, -30.87252, 18.03941, -56.32009, 29.92695, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 12000
Discounting practice with old adv calculation: [0.00013, 0.66506, 0.0003, 0.5817, 0.01329, 0.20796, 0.04729, 0.09756, 0.06434, 0.0743, 0.07107, 0.06519, 0.05908, 0.06972, 0.0319, 0.08181, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00455, -1.68225, 0.65796, -5.10393, 0.90529, -1.55643, -0.0049, 0.0049, -14.146, 9.72064, -30.93065, 18.03184, -56.37209, 29.88003, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 12500
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00029, 0.58432, 0.01317, 0.20877, 0.04724, 0.09764, 0.06431, 0.07432, 0.07113, 0.06513, 0.05915, 0.06963, 0.03199, 0.08167, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00434, -1.64877, 0.65073, -5.0942, 0.89658, -1.54304, -0.00492, 0.00493, -14.1894, 9.73198, -30.98672, 18.02442, -56.4221, 29.83464, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 13000
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00028, 0.58683, 0.01307, 0.20955, 0.04719, 0.09772, 0.06429, 0.07435, 0.07118, 0.06507, 0.05922, 0.06954, 0.03209, 0.08153, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00415, -1.61674, 0.64382, -5.0847, 0.88817, -1.53005, -0.00495, 0.00496, -14.2314, 9.74287, -31.04088, 18.01714, -56.47028, 29.79068, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 13500
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00028, 0.58923, 0.01296, 0.21031, 0.04715, 0.09779, 0.06426, 0.07438, 0.07123, 0.06501, 0.05928, 0.06945, 0.03218, 0.08139, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00398, -1.58604, 0.63719, -5.07543, 0.88002, -1.51743, -0.00498, 0.00498, -14.27211, 9.75335, -31.09328, 18.00999, -56.51676, 29.74805, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 14000
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00027, 0.59154, 0.01286, 0.21104, 0.0471, 0.09786, 0.06423, 0.0744, 0.07128, 0.06495, 0.05934, 0.06937, 0.03226, 0.08126, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00381, -1.55658, 0.63083, -5.06638, 0.87212, -1.50516, -0.005, 0.00501, -14.31161, 9.76345, -31.14404, 18.00297, -56.56168, 29.70665, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 14500
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00026, 0.59375, 0.01276, 0.21175, 0.04706, 0.09793, 0.06421, 0.07443, 0.07133, 0.0649, 0.0594, 0.06929, 0.03235, 0.08113, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00366, -1.52827, 0.62472, -5.05753, 0.86445, -1.49322, -0.00503, 0.00504, -14.35, 9.7732, -31.19328, 17.99607, -56.60515, 29.66642, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 15000
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00026, 0.59587, 0.01267, 0.21245, 0.04702, 0.098, 0.06418, 0.07445, 0.07138, 0.06484, 0.05946, 0.06921, 0.03243, 0.08101, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00352, -1.50104, 0.61883, -5.04887, 0.85699, -1.48159, -0.00505, 0.00506, -14.38733, 9.78262, -31.24109, 17.98929, -56.64726, 29.62728, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 15500
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00025, 0.59791, 0.01258, 0.21312, 0.04698, 0.09807, 0.06416, 0.07448, 0.07143, 0.06479, 0.05952, 0.06913, 0.03251, 0.08089, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00339, -1.4748, 0.61316, -5.0404, 0.84974, -1.47024, -0.00508, 0.00508, -14.42368, 9.79173, -31.28758, 17.98262, -56.68811, 29.58916, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 16000
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00025, 0.59988, 0.01249, 0.21377, 0.04695, 0.09813, 0.06414, 0.0745, 0.07148, 0.06474, 0.05958, 0.06906, 0.03259, 0.08077, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00327, -1.4495, 0.60768, -5.03209, 0.84268, -1.45916, -0.0051, 0.00511, -14.4591, 9.80056, -31.33281, 17.97607, -56.72779, 29.55201, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 16500
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00024, 0.60178, 0.01241, 0.21441, 0.04691, 0.09819, 0.06412, 0.07452, 0.07152, 0.06469, 0.05963, 0.06898, 0.03267, 0.08066, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00315, -1.42508, 0.60239, -5.02395, 0.8358, -1.44834, -0.00512, 0.00513, -14.49366, 9.80912, -31.37687, 17.96961, -56.76635, 29.51576, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 17000
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00024, 0.60361, 0.01232, 0.21503, 0.04687, 0.09825, 0.06409, 0.07454, 0.07156, 0.06464, 0.05968, 0.06891, 0.03274, 0.08055, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00304, -1.40148, 0.59728, -5.01597, 0.82909, -1.43776, -0.00514, 0.00515, -14.5274, 9.81742, -31.41983, 17.96326, -56.80388, 29.48038, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 17500
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00023, 0.60539, 0.01225, 0.21563, 0.04684, 0.09831, 0.06407, 0.07457, 0.07161, 0.06459, 0.05974, 0.06884, 0.03282, 0.08044, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00294, -1.37865, 0.59233, -5.00814, 0.82254, -1.42741, -0.00516, 0.00517, -14.56036, 9.82549, -31.46174, 17.95701, -56.84043, 29.44582, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 18000
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00023, 0.6071, 0.01217, 0.21622, 0.04681, 0.09836, 0.06405, 0.07459, 0.07165, 0.06454, 0.05979, 0.06877, 0.03289, 0.08033, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00285, -1.35656, 0.58753, -5.00044, 0.81615, -1.41728, -0.00519, 0.00519, -14.59259, 9.83334, -31.50267, 17.95084, -56.87606, 29.41202, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 18500
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00023, 0.60875, 0.01209, 0.2168, 0.04677, 0.09842, 0.06403, 0.07461, 0.07169, 0.0645, 0.05984, 0.06871, 0.03296, 0.08023, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00276, -1.33516, 0.58288, -4.99289, 0.80989, -1.40735, -0.00521, 0.00521, -14.62412, 9.84097, -31.54267, 17.94477, -56.91082, 29.37896, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 19000
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00022, 0.61036, 0.01202, 0.21737, 0.04674, 0.09847, 0.06401, 0.07463, 0.07173, 0.06445, 0.05989, 0.06864, 0.03303, 0.08013, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00267, -1.31441, 0.57836, -4.98546, 0.80377, -1.39762, -0.00523, 0.00523, -14.655, 9.84841, -31.58179, 17.93879, -56.94475, 29.3466, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 19500
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00022, 0.61191, 0.01195, 0.21792, 0.04671, 0.09852, 0.06399, 0.07465, 0.07177, 0.06441, 0.05994, 0.06858, 0.03309, 0.08003, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00259, -1.29428, 0.57398, -4.97816, 0.79778, -1.38808, -0.00525, 0.00525, -14.68525, 9.85566, -31.62006, 17.93288, -56.9779, 29.3149, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 20000
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00022, 0.61342, 0.01188, 0.21846, 0.04668, 0.09857, 0.06397, 0.07467, 0.07181, 0.06437, 0.05998, 0.06851, 0.03316, 0.07993, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00251, -1.27473, 0.56971, -4.97098, 0.79192, -1.37872, -0.00527, 0.00527, -14.7149, 9.86273, -31.65754, 17.92706, -57.01032, 29.28384, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 20500
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00021, 0.61489, 0.01182, 0.21899, 0.04665, 0.09862, 0.06395, 0.07469, 0.07185, 0.06433, 0.06003, 0.06845, 0.03322, 0.07984, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00244, -1.25574, 0.56557, -4.96391, 0.78617, -1.36952, -0.00528, 0.00529, -14.74398, 9.86963, -31.69426, 17.92132, -57.04203, 29.25338, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 21000
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00021, 0.61631, 0.01175, 0.21951, 0.04663, 0.09867, 0.06393, 0.0747, 0.07188, 0.06428, 0.06008, 0.06839, 0.03329, 0.07975, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00237, -1.23727, 0.56153, -4.95695, 0.78053, -1.36049, -0.0053, 0.00531, -14.77253, 9.87636, -31.73025, 17.91565, -57.07306, 29.2235, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 21500
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00021, 0.61769, 0.01169, 0.22001, 0.0466, 0.09872, 0.06392, 0.07472, 0.07192, 0.06424, 0.06012, 0.06833, 0.03335, 0.07965, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [-0.0, -0.0, 0.0023, -1.21932, 0.5576, -4.9501, 0.775, -1.35161, -0.00532, 0.00533, -14.80055, 9.88295, -31.76555, 17.91005, -57.10347, 29.19417, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 22000
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.0002, 0.61904, 0.01163, 0.22051, 0.04657, 0.09876, 0.0639, 0.07474, 0.07196, 0.0642, 0.06016, 0.06828, 0.03341, 0.07956, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00224, -1.20184, 0.55376, -4.94335, 0.76958, -1.34289, -0.00534, 0.00535, -14.82808, 9.88938, -31.8002, 17.90452, -57.13326, 29.16537, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 22500
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.0002, 0.62034, 0.01157, 0.221, 0.04654, 0.09881, 0.06388, 0.07476, 0.07199, 0.06416, 0.06021, 0.06822, 0.03347, 0.07948, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00218, -1.18481, 0.55002, -4.93669, 0.76425, -1.3343, -0.00536, 0.00537, -14.85514, 9.89568, -31.83421, 17.89906, -57.16247, 29.13708, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 23000
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.0002, 0.62161, 0.01151, 0.22148, 0.04652, 0.09885, 0.06386, 0.07478, 0.07203, 0.06413, 0.06025, 0.06816, 0.03353, 0.07939, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00212, -1.16823, 0.54638, -4.93013, 0.75901, -1.32585, -0.00537, 0.00538, -14.88174, 9.90184, -31.86762, 17.89366, -57.19113, 29.10927, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 23500
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.0002, 0.62285, 0.01145, 0.22195, 0.04649, 0.0989, 0.06385, 0.07479, 0.07206, 0.06409, 0.06029, 0.06811, 0.03359, 0.07931, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00207, -1.15206, 0.54282, -4.92367, 0.75387, -1.31754, -0.00539, 0.0054, -14.9079, 9.90787, -31.90045, 17.88833, -57.21925, 29.08193, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 24000
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00019, 0.62406, 0.0114, 0.22242, 0.04647, 0.09894, 0.06383, 0.07481, 0.07209, 0.06405, 0.06033, 0.06805, 0.03365, 0.07922, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00201, -1.13629, 0.53934, -4.91728, 0.74881, -1.30935, -0.00541, 0.00542, -14.93365, 9.91379, -31.93273, 17.88306, -57.24687, 29.05504, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 24500
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00019, 0.62524, 0.01134, 0.22287, 0.04645, 0.09898, 0.06381, 0.07483, 0.07213, 0.06401, 0.06037, 0.068, 0.03371, 0.07914, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00196, -1.1209, 0.53594, -4.91099, 0.74383, -1.30128, -0.00543, 0.00543, -14.959, 9.91958, -31.96447, 17.87785, -57.274, 29.02859, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 25000
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00019, 0.62639, 0.01129, 0.22332, 0.04642, 0.09902, 0.0638, 0.07484, 0.07216, 0.06398, 0.06041, 0.06795, 0.03376, 0.07906, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00191, -1.10588, 0.53261, -4.90477, 0.73894, -1.29333, -0.00544, 0.00545, -14.98396, 9.92526, -31.9957, 17.87269, -57.30066, 29.00254, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 25500
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00019, 0.62751, 0.01123, 0.22376, 0.0464, 0.09906, 0.06378, 0.07486, 0.07219, 0.06394, 0.06045, 0.0679, 0.03382, 0.07898, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00187, -1.09121, 0.52936, -4.89864, 0.73412, -1.28549, -0.00546, 0.00547, -15.00855, 9.93083, -32.02644, 17.86759, -57.32687, 28.9769, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 26000
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00019, 0.6286, 0.01118, 0.2242, 0.04638, 0.0991, 0.06376, 0.07487, 0.07222, 0.06391, 0.06049, 0.06785, 0.03387, 0.0789, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00182, -1.07688, 0.52618, -4.89258, 0.72937, -1.27776, -0.00547, 0.00548, -15.03278, 9.9363, -32.0567, 17.86255, -57.35265, 28.95164, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 26500
Discounting practice with old adv calculation: [0.00013, 0.66507, 0.00018, 0.62967, 0.01113, 0.22462, 0.04635, 0.09914, 0.06375, 0.07489, 0.07225, 0.06387, 0.06053, 0.0678, 0.03393, 0.07883, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00178, -1.06287, 0.52307, -4.88659, 0.72469, -1.27014, -0.00549, 0.0055, -15.05666, 9.94167, -32.08651, 17.85756, -57.37801, 28.92676, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 27000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00018, 0.63071, 0.01108, 0.22504, 0.04633, 0.09917, 0.06373, 0.07491, 0.07229, 0.06384, 0.06057, 0.06775, 0.03398, 0.07875, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00174, -1.04918, 0.52001, -4.88068, 0.72009, -1.26262, -0.0055, 0.00551, -15.08022, 9.94695, -32.11588, 17.85262, -57.40297, 28.90223, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 27500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00018, 0.63173, 0.01103, 0.22546, 0.04631, 0.09921, 0.06372, 0.07492, 0.07232, 0.06381, 0.06061, 0.0677, 0.03403, 0.07868, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0017, -1.03578, 0.51702, -4.87483, 0.71555, -1.25519, -0.00552, 0.00553, -15.10345, 9.95213, -32.14482, 17.84772, -57.42755, 28.87805, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 28000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00018, 0.63272, 0.01099, 0.22586, 0.04629, 0.09924, 0.0637, 0.07494, 0.07235, 0.06377, 0.06064, 0.06765, 0.03408, 0.07861, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00166, -1.02267, 0.51409, -4.86905, 0.71107, -1.24787, -0.00553, 0.00554, -15.12637, 9.95722, -32.17336, 17.84288, -57.45175, 28.85421, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 28500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00018, 0.6337, 0.01094, 0.22627, 0.04627, 0.09928, 0.06369, 0.07495, 0.07238, 0.06374, 0.06068, 0.06761, 0.03413, 0.07853, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00162, -1.00984, 0.51122, -4.86334, 0.70666, -1.24063, -0.00555, 0.00556, -15.14899, 9.96222, -32.2015, 17.83808, -57.4756, 28.83069, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 29000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00017, 0.63465, 0.0109, 0.22666, 0.04625, 0.09931, 0.06367, 0.07496, 0.07241, 0.06371, 0.06072, 0.06756, 0.03418, 0.07846, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00159, -0.99728, 0.5084, -4.85769, 0.7023, -1.23348, -0.00556, 0.00557, -15.17132, 9.96714, -32.22925, 17.83333, -57.4991, 28.80748, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 29500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00017, 0.63558, 0.01085, 0.22705, 0.04623, 0.09935, 0.06366, 0.07498, 0.07243, 0.06368, 0.06075, 0.06751, 0.03423, 0.07839, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00156, -0.98498, 0.50564, -4.8521, 0.698, -1.22642, -0.00558, 0.00559, -15.19337, 9.97199, -32.25664, 17.82863, -57.52226, 28.78458, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 30000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00017, 0.63649, 0.01081, 0.22744, 0.04621, 0.09938, 0.06364, 0.07499, 0.07246, 0.06364, 0.06079, 0.06747, 0.03428, 0.07832, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00152, -0.97293, 0.50292, -4.84657, 0.69376, -1.21944, -0.00559, 0.0056, -15.21515, 9.97675, -32.28367, 17.82396, -57.54511, 28.76197, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 30500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00017, 0.63738, 0.01076, 0.22782, 0.04619, 0.09942, 0.06363, 0.07501, 0.07249, 0.06361, 0.06082, 0.06742, 0.03433, 0.07825, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00149, -0.96111, 0.50026, -4.8411, 0.68958, -1.21254, -0.00561, 0.00562, -15.23667, 9.98144, -32.31035, 17.81934, -57.56764, 28.73965, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 31000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00017, 0.63826, 0.01072, 0.2282, 0.04617, 0.09945, 0.06362, 0.07502, 0.07252, 0.06358, 0.06086, 0.06738, 0.03438, 0.07819, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00146, -0.94953, 0.49764, -4.83569, 0.68544, -1.20572, -0.00562, 0.00563, -15.25793, 9.98606, -32.3367, 17.81476, -57.58986, 28.7176, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 31500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00017, 0.63911, 0.01068, 0.22857, 0.04615, 0.09948, 0.0636, 0.07504, 0.07255, 0.06355, 0.06089, 0.06734, 0.03443, 0.07812, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00143, -0.93818, 0.49507, -4.83033, 0.68136, -1.19897, -0.00564, 0.00564, -15.27895, 9.9906, -32.36273, 17.81022, -57.6118, 28.69582, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 32000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00017, 0.63995, 0.01064, 0.22893, 0.04613, 0.09951, 0.06359, 0.07505, 0.07258, 0.06352, 0.06092, 0.06729, 0.03447, 0.07805, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0014, -0.92704, 0.49254, -4.82502, 0.67732, -1.1923, -0.00565, 0.00566, -15.29972, 9.99508, -32.38843, 17.80571, -57.63345, 28.67431, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 32500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00016, 0.64078, 0.0106, 0.22929, 0.04612, 0.09954, 0.06358, 0.07506, 0.0726, 0.06349, 0.06096, 0.06725, 0.03452, 0.07799, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00137, -0.91612, 0.49006, -4.81977, 0.67333, -1.18571, -0.00566, 0.00567, -15.32027, 9.9995, -32.41384, 17.80125, -57.65482, 28.65304, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 33000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00016, 0.64158, 0.01056, 0.22965, 0.0461, 0.09957, 0.06356, 0.07508, 0.07263, 0.06346, 0.06099, 0.06721, 0.03456, 0.07793, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00135, -0.9054, 0.48761, -4.81456, 0.66939, -1.17918, -0.00568, 0.00568, -15.34058, 10.00385, -32.43894, 17.79682, -57.67593, 28.63202, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 33500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00016, 0.64237, 0.01052, 0.23, 0.04608, 0.0996, 0.06355, 0.07509, 0.07266, 0.06343, 0.06102, 0.06717, 0.03461, 0.07786, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00132, -0.89487, 0.48521, -4.80941, 0.6655, -1.17272, -0.00569, 0.0057, -15.36068, 10.00813, -32.46376, 17.79243, -57.69678, 28.61124, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 34000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00016, 0.64315, 0.01048, 0.23035, 0.04606, 0.09963, 0.06354, 0.0751, 0.07268, 0.06341, 0.06105, 0.06712, 0.03465, 0.0778, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0013, -0.88454, 0.48285, -4.8043, 0.66165, -1.16632, -0.0057, 0.00571, -15.38056, 10.01236, -32.4883, 17.78807, -57.71738, 28.59069, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 34500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00016, 0.64391, 0.01044, 0.2307, 0.04605, 0.09966, 0.06352, 0.07512, 0.07271, 0.06338, 0.06109, 0.06708, 0.0347, 0.07774, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00127, -0.8744, 0.48052, -4.79924, 0.65784, -1.15999, -0.00572, 0.00572, -15.40024, 10.01653, -32.51257, 17.78375, -57.73774, 28.57037, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 35000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00016, 0.64465, 0.0104, 0.23104, 0.04603, 0.09969, 0.06351, 0.07513, 0.07273, 0.06335, 0.06112, 0.06704, 0.03474, 0.07768, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00125, -0.86443, 0.47824, -4.79423, 0.65407, -1.15372, -0.00573, 0.00574, -15.41972, 10.02064, -32.53657, 17.77946, -57.75785, 28.55026, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 35500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00016, 0.64539, 0.01037, 0.23137, 0.04601, 0.09972, 0.0635, 0.07514, 0.07276, 0.06332, 0.06115, 0.067, 0.03479, 0.07762, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00122, -0.85464, 0.47598, -4.78926, 0.65034, -1.14751, -0.00574, 0.00575, -15.43899, 10.0247, -32.56032, 17.77521, -57.77774, 28.53037, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 36000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00016, 0.6461, 0.01033, 0.23171, 0.046, 0.09975, 0.06349, 0.07515, 0.07278, 0.06329, 0.06118, 0.06696, 0.03483, 0.07756, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0012, -0.84502, 0.47376, -4.78434, 0.64665, -1.14136, -0.00575, 0.00576, -15.45808, 10.0287, -32.58381, 17.77099, -57.7974, 28.51069, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 36500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00015, 0.64681, 0.0103, 0.23203, 0.04598, 0.09977, 0.06347, 0.07517, 0.07281, 0.06327, 0.06121, 0.06693, 0.03487, 0.0775, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00118, -0.83556, 0.47158, -4.77945, 0.643, -1.13527, -0.00577, 0.00577, -15.47698, 10.03266, -32.60706, 17.76679, -57.81685, 28.49121, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 37000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00015, 0.6475, 0.01026, 0.23236, 0.04597, 0.0998, 0.06346, 0.07518, 0.07283, 0.06324, 0.06124, 0.06689, 0.03491, 0.07744, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00116, -0.82627, 0.46943, -4.77461, 0.63939, -1.12924, -0.00578, 0.00579, -15.4957, 10.03656, -32.63008, 17.76263, -57.83608, 28.47193, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 37500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00015, 0.64818, 0.01023, 0.23268, 0.04595, 0.09983, 0.06345, 0.07519, 0.07286, 0.06321, 0.06127, 0.06685, 0.03496, 0.07738, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00114, -0.81713, 0.4673, -4.76981, 0.63581, -1.12326, -0.00579, 0.0058, -15.51425, 10.04041, -32.65286, 17.7585, -57.85511, 28.45284, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 38000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00015, 0.64885, 0.01019, 0.233, 0.04594, 0.09985, 0.06344, 0.0752, 0.07288, 0.06319, 0.0613, 0.06681, 0.035, 0.07732, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00112, -0.80814, 0.46521, -4.76505, 0.63227, -1.11733, -0.0058, 0.00581, -15.53262, 10.04421, -32.67542, 17.7544, -57.87393, 28.43394, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 38500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00015, 0.64951, 0.01016, 0.23332, 0.04592, 0.09988, 0.06342, 0.07521, 0.07291, 0.06316, 0.06133, 0.06677, 0.03504, 0.07727, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.0011, -0.7993, 0.46315, -4.76032, 0.62876, -1.11146, -0.00581, 0.00582, -15.55082, 10.04797, -32.69776, 17.75032, -57.89256, 28.41522, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 39000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00015, 0.65016, 0.01012, 0.23363, 0.04591, 0.0999, 0.06341, 0.07523, 0.07293, 0.06313, 0.06136, 0.06674, 0.03508, 0.07721, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00108, -0.7906, 0.46112, -4.75564, 0.62529, -1.10563, -0.00583, 0.00584, -15.56886, 10.05168, -32.71989, 17.74628, -57.911, 28.39669, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 39500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00015, 0.65079, 0.01009, 0.23394, 0.04589, 0.09993, 0.0634, 0.07524, 0.07296, 0.06311, 0.06139, 0.0667, 0.03512, 0.07715, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00106, -0.78205, 0.45912, -4.75099, 0.62185, -1.09986, -0.00584, 0.00585, -15.58674, 10.05535, -32.7418, 17.74226, -57.92926, 28.37832, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 40000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00015, 0.65141, 0.01006, 0.23424, 0.04588, 0.09995, 0.06339, 0.07525, 0.07298, 0.06308, 0.06142, 0.06666, 0.03516, 0.0771, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00104, -0.77362, 0.45715, -4.74638, 0.61844, -1.09414, -0.00585, 0.00586, -15.60446, 10.05898, -32.76351, 17.73827, -57.94733, 28.36013, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 40500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00015, 0.65203, 0.01003, 0.23455, 0.04586, 0.09998, 0.06338, 0.07526, 0.073, 0.06306, 0.06145, 0.06663, 0.0352, 0.07704, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00103, -0.76534, 0.4552, -4.7418, 0.61506, -1.08846, -0.00586, 0.00587, -15.62204, 10.06256, -32.78503, 17.7343, -57.96522, 28.3421, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 41000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00015, 0.65263, 0.01, 0.23485, 0.04585, 0.1, 0.06337, 0.07527, 0.07303, 0.06303, 0.06147, 0.06659, 0.03524, 0.07699, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00101, -0.75718, 0.45327, -4.73726, 0.61171, -1.08283, -0.00587, 0.00588, -15.63946, 10.0661, -32.80635, 17.73036, -57.98295, 28.32424, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 41500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00015, 0.65322, 0.00996, 0.23514, 0.04584, 0.10003, 0.06335, 0.07528, 0.07305, 0.06301, 0.0615, 0.06656, 0.03528, 0.07694, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00099, -0.74914, 0.45138, -4.73275, 0.6084, -1.07724, -0.00588, 0.00589, -15.65674, 10.0696, -32.82748, 17.72645, -58.0005, 28.30653, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 42000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00014, 0.65381, 0.00993, 0.23544, 0.04582, 0.10005, 0.06334, 0.07529, 0.07307, 0.06298, 0.06153, 0.06652, 0.03532, 0.07688, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00098, -0.74123, 0.44951, -4.72827, 0.60511, -1.0717, -0.0059, 0.0059, -15.67387, 10.07306, -32.84842, 17.72256, -58.01789, 28.28898, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 42500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00014, 0.65438, 0.0099, 0.23573, 0.04581, 0.10008, 0.06333, 0.07531, 0.07309, 0.06296, 0.06156, 0.06649, 0.03536, 0.07683, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00096, -0.73344, 0.44766, -4.72383, 0.60185, -1.0662, -0.00591, 0.00592, -15.69087, 10.07649, -32.86919, 17.71869, -58.03512, 28.27159, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 43000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00014, 0.65495, 0.00987, 0.23602, 0.04579, 0.1001, 0.06332, 0.07532, 0.07312, 0.06293, 0.06159, 0.06645, 0.03539, 0.07678, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00094, -0.72576, 0.44583, -4.71942, 0.59861, -1.06075, -0.00592, 0.00593, -15.70773, 10.07987, -32.88977, 17.71485, -58.05219, 28.25434, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 43500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00014, 0.6555, 0.00984, 0.2363, 0.04578, 0.10012, 0.06331, 0.07533, 0.07314, 0.06291, 0.06161, 0.06642, 0.03543, 0.07673, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00093, -0.7182, 0.44403, -4.71504, 0.59541, -1.05534, -0.00593, 0.00594, -15.72445, 10.08322, -32.91019, 17.71103, -58.0691, 28.23723, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 44000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00014, 0.65605, 0.00981, 0.23659, 0.04577, 0.10014, 0.0633, 0.07534, 0.07316, 0.06289, 0.06164, 0.06638, 0.03547, 0.07667, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00092, -0.71075, 0.44225, -4.71069, 0.59223, -1.04996, -0.00594, 0.00595, -15.74105, 10.08654, -32.93043, 17.70723, -58.08587, 28.22027, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 44500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00014, 0.65659, 0.00979, 0.23687, 0.04576, 0.10017, 0.06329, 0.07535, 0.07318, 0.06286, 0.06167, 0.06635, 0.03551, 0.07662, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0009, -0.70341, 0.4405, -4.70637, 0.58908, -1.04463, -0.00595, 0.00596, -15.75752, 10.08982, -32.95051, 17.70346, -58.10249, 28.20345, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 45000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00014, 0.65712, 0.00976, 0.23715, 0.04574, 0.10019, 0.06328, 0.07536, 0.0732, 0.06284, 0.06169, 0.06632, 0.03554, 0.07657, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00089, -0.69617, 0.43876, -4.70208, 0.58595, -1.03933, -0.00596, 0.00597, -15.77387, 10.09306, -32.97043, 17.69971, -58.11897, 28.18676, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 45500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00014, 0.65764, 0.00973, 0.23743, 0.04573, 0.10021, 0.06327, 0.07537, 0.07323, 0.06282, 0.06172, 0.06628, 0.03558, 0.07652, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00087, -0.68904, 0.43705, -4.69782, 0.58285, -1.03408, -0.00597, 0.00598, -15.79009, 10.09628, -32.99019, 17.69598, -58.13531, 28.17021, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 46000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00014, 0.65816, 0.0097, 0.2377, 0.04572, 0.10023, 0.06326, 0.07538, 0.07325, 0.06279, 0.06175, 0.06625, 0.03562, 0.07647, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00086, -0.68201, 0.43535, -4.69359, 0.57977, -1.02886, -0.00598, 0.00599, -15.8062, 10.09946, -33.00979, 17.69227, -58.1515, 28.15379, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 46500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00014, 0.65866, 0.00967, 0.23797, 0.04571, 0.10025, 0.06325, 0.07539, 0.07327, 0.06277, 0.06177, 0.06622, 0.03565, 0.07642, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00085, -0.67508, 0.43368, -4.68938, 0.57671, -1.02368, -0.00599, 0.006, -15.82218, 10.1026, -33.02924, 17.68858, -58.16757, 28.1375, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 47000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00014, 0.65916, 0.00965, 0.23824, 0.04569, 0.10027, 0.06324, 0.0754, 0.07329, 0.06275, 0.0618, 0.06619, 0.03569, 0.07638, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00083, -0.66824, 0.43202, -4.6852, 0.57368, -1.01853, -0.006, 0.00601, -15.83806, 10.10572, -33.04855, 17.68492, -58.1835, 28.12133, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 47500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00014, 0.65965, 0.00962, 0.23851, 0.04568, 0.1003, 0.06323, 0.07541, 0.07331, 0.06272, 0.06182, 0.06615, 0.03573, 0.07633, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00082, -0.6615, 0.43039, -4.68105, 0.57067, -1.01342, -0.00601, 0.00602, -15.85382, 10.1088, -33.0677, 17.68127, -58.19931, 28.10529, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 48000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00014, 0.66014, 0.00959, 0.23877, 0.04567, 0.10032, 0.06321, 0.07542, 0.07333, 0.0627, 0.06185, 0.06612, 0.03576, 0.07628, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00081, -0.65485, 0.42877, -4.67693, 0.56769, -1.00834, -0.00602, 0.00603, -15.86947, 10.11186, -33.08672, 17.67764, -58.21498, 28.08936, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 48500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.66061, 0.00957, 0.23904, 0.04566, 0.10034, 0.0632, 0.07543, 0.07335, 0.06268, 0.06187, 0.06609, 0.0358, 0.07623, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.0008, -0.64829, 0.42717, -4.67283, 0.56472, -1.0033, -0.00603, 0.00604, -15.88501, 10.11489, -33.10559, 17.67403, -58.23054, 28.07356, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 49000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.66108, 0.00954, 0.2393, 0.04565, 0.10036, 0.06319, 0.07544, 0.07337, 0.06266, 0.0619, 0.06606, 0.03583, 0.07618, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00079, -0.64182, 0.42559, -4.66875, 0.56178, -0.99829, -0.00604, 0.00605, -15.90045, 10.11788, -33.12433, 17.67044, -58.24597, 28.05787, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 49500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.66154, 0.00951, 0.23956, 0.04563, 0.10038, 0.06318, 0.07545, 0.07339, 0.06263, 0.06193, 0.06603, 0.03587, 0.07614, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00077, -0.63544, 0.42403, -4.66471, 0.55886, -0.99332, -0.00606, 0.00606, -15.91579, 10.12085, -33.14293, 17.66687, -58.26129, 28.04229, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 50000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.662, 0.00949, 0.23981, 0.04562, 0.1004, 0.06317, 0.07546, 0.07341, 0.06261, 0.06195, 0.066, 0.0359, 0.07609, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00076, -0.62914, 0.42248, -4.66068, 0.55596, -0.98837, -0.00607, 0.00607, -15.93102, 10.12379, -33.1614, 17.66332, -58.27648, 28.02683, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 50500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.66245, 0.00946, 0.24007, 0.04561, 0.10042, 0.06316, 0.07547, 0.07344, 0.06259, 0.06197, 0.06597, 0.03594, 0.07604, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00075, -0.62293, 0.42095, -4.65668, 0.55308, -0.98346, -0.00608, 0.00608, -15.94615, 10.12671, -33.17974, 17.65978, -58.29157, 28.01147, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 51000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.66289, 0.00944, 0.24032, 0.0456, 0.10044, 0.06315, 0.07548, 0.07346, 0.06257, 0.062, 0.06594, 0.03597, 0.076, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00074, -0.61679, 0.41944, -4.6527, 0.55022, -0.97858, -0.00609, 0.00609, -15.96118, 10.1296, -33.19795, 17.65627, -58.30654, 27.99623, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 51500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.66333, 0.00941, 0.24057, 0.04559, 0.10046, 0.06314, 0.07549, 0.07348, 0.06255, 0.06202, 0.06591, 0.036, 0.07595, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00073, -0.61074, 0.41794, -4.64875, 0.54738, -0.97372, -0.00609, 0.0061, -15.97612, 10.13246, -33.21604, 17.65277, -58.3214, 27.98108, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 52000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.66376, 0.00939, 0.24082, 0.04558, 0.10048, 0.06313, 0.0755, 0.0735, 0.06253, 0.06205, 0.06588, 0.03604, 0.07591, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00072, -0.60476, 0.41646, -4.64482, 0.54456, -0.9689, -0.0061, 0.00611, -15.99097, 10.1353, -33.23401, 17.64929, -58.33616, 27.96605, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 52500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.66419, 0.00936, 0.24107, 0.04557, 0.10049, 0.06313, 0.07551, 0.07351, 0.06251, 0.06207, 0.06585, 0.03607, 0.07586, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00071, -0.59886, 0.415, -4.64091, 0.54175, -0.96411, -0.00611, 0.00612, -16.00572, 10.13811, -33.25186, 17.64582, -58.35081, 27.95111, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 53000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.66461, 0.00934, 0.24132, 0.04556, 0.10051, 0.06312, 0.07552, 0.07353, 0.06248, 0.0621, 0.06582, 0.03611, 0.07582, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0007, -0.59303, 0.41355, -4.63702, 0.53897, -0.95934, -0.00612, 0.00613, -16.02038, 10.14089, -33.26959, 17.64237, -58.36535, 27.93627, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 53500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.66502, 0.00932, 0.24156, 0.04555, 0.10053, 0.06311, 0.07553, 0.07355, 0.06246, 0.06212, 0.06579, 0.03614, 0.07577, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00069, -0.58728, 0.41211, -4.63315, 0.5362, -0.95461, -0.00613, 0.00614, -16.03495, 10.14366, -33.2872, 17.63894, -58.37979, 27.92153, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 54000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.66543, 0.00929, 0.2418, 0.04554, 0.10055, 0.0631, 0.07554, 0.07357, 0.06244, 0.06214, 0.06576, 0.03617, 0.07573, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00068, -0.5816, 0.41069, -4.62931, 0.53345, -0.9499, -0.00614, 0.00615, -16.04944, 10.1464, -33.3047, 17.63552, -58.39414, 27.90689, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 54500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.66583, 0.00927, 0.24204, 0.04553, 0.10057, 0.06309, 0.07555, 0.07359, 0.06242, 0.06217, 0.06573, 0.0362, 0.07569, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00067, -0.57599, 0.40928, -4.62549, 0.53072, -0.94522, -0.00615, 0.00616, -16.06383, 10.14911, -33.32209, 17.63212, -58.40838, 27.89235, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 55000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.66623, 0.00925, 0.24228, 0.04552, 0.10059, 0.06308, 0.07556, 0.07361, 0.0624, 0.06219, 0.0657, 0.03624, 0.07564, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00066, -0.57045, 0.40789, -4.62168, 0.52801, -0.94056, -0.00616, 0.00617, -16.07815, 10.1518, -33.33936, 17.62874, -58.42253, 27.87789, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 55500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.66662, 0.00922, 0.24252, 0.0455, 0.1006, 0.06307, 0.07557, 0.07363, 0.06238, 0.06221, 0.06567, 0.03627, 0.0756, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00065, -0.56498, 0.40651, -4.6179, 0.52531, -0.93593, -0.00617, 0.00618, -16.09238, 10.15447, -33.35653, 17.62537, -58.43658, 27.86353, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 56000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.667, 0.0092, 0.24276, 0.04549, 0.10062, 0.06306, 0.07558, 0.07365, 0.06236, 0.06224, 0.06564, 0.0363, 0.07556, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00065, -0.55957, 0.40514, -4.61414, 0.52263, -0.93133, -0.00618, 0.00619, -16.10653, 10.15712, -33.3736, 17.62201, -58.45054, 27.84926, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 56500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.66738, 0.00918, 0.24299, 0.04548, 0.10064, 0.06305, 0.07559, 0.07367, 0.06234, 0.06226, 0.06561, 0.03633, 0.07551, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00064, -0.55423, 0.40379, -4.61039, 0.51997, -0.92675, -0.00619, 0.0062, -16.12059, 10.15975, -33.39056, 17.61867, -58.46441, 27.83507, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 57000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00013, 0.66776, 0.00915, 0.24322, 0.04547, 0.10066, 0.06304, 0.0756, 0.07369, 0.06232, 0.06228, 0.06559, 0.03637, 0.07547, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00063, -0.54895, 0.40245, -4.60667, 0.51732, -0.9222, -0.0062, 0.00621, -16.13458, 10.16235, -33.40742, 17.61535, -58.47819, 27.82098, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 57500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.66813, 0.00913, 0.24345, 0.04547, 0.10067, 0.06303, 0.07561, 0.07371, 0.0623, 0.06231, 0.06556, 0.0364, 0.07543, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00062, -0.54374, 0.40112, -4.60297, 0.51469, -0.91767, -0.00621, 0.00622, -16.14849, 10.16494, -33.42417, 17.61203, -58.49188, 27.80697, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 58000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.6685, 0.00911, 0.24368, 0.04546, 0.10069, 0.06302, 0.07562, 0.07372, 0.06228, 0.06233, 0.06553, 0.03643, 0.07539, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00061, -0.53859, 0.39981, -4.59928, 0.51207, -0.91317, -0.00622, 0.00623, -16.16233, 10.1675, -33.44083, 17.60874, -58.50549, 27.79304, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 58500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.66886, 0.00909, 0.24391, 0.04545, 0.10071, 0.06301, 0.07563, 0.07374, 0.06226, 0.06235, 0.0655, 0.03646, 0.07535, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0006, -0.5335, 0.39851, -4.59561, 0.50947, -0.90869, -0.00623, 0.00624, -16.17609, 10.17005, -33.45739, 17.60545, -58.519, 27.77919, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 59000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.66922, 0.00907, 0.24414, 0.04544, 0.10072, 0.063, 0.07563, 0.07376, 0.06224, 0.06237, 0.06547, 0.03649, 0.07531, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.0006, -0.52846, 0.39722, -4.59196, 0.50688, -0.90423, -0.00624, 0.00625, -16.18977, 10.17257, -33.47386, 17.60218, -58.53244, 27.76543, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 59500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.66957, 0.00905, 0.24436, 0.04543, 0.10074, 0.06299, 0.07564, 0.07378, 0.06222, 0.0624, 0.06545, 0.03652, 0.07526, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00059, -0.52349, 0.39594, -4.58833, 0.50431, -0.8998, -0.00625, 0.00625, -16.20339, 10.17507, -33.49023, 17.59893, -58.54579, 27.75175, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 60000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.66992, 0.00903, 0.24459, 0.04542, 0.10076, 0.06299, 0.07565, 0.0738, 0.0622, 0.06242, 0.06542, 0.03655, 0.07522, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00058, -0.51857, 0.39467, -4.58472, 0.50176, -0.89539, -0.00625, 0.00626, -16.21693, 10.17756, -33.50651, 17.59569, -58.55906, 27.73814, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 60500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67026, 0.009, 0.24481, 0.04541, 0.10077, 0.06298, 0.07566, 0.07382, 0.06218, 0.06244, 0.06539, 0.03658, 0.07518, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00057, -0.51371, 0.39342, -4.58112, 0.49921, -0.891, -0.00626, 0.00627, -16.2304, 10.18002, -33.52269, 17.59246, -58.57225, 27.72462, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 61000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.6706, 0.00898, 0.24503, 0.0454, 0.10079, 0.06297, 0.07567, 0.07383, 0.06216, 0.06246, 0.06537, 0.03661, 0.07514, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00057, -0.50891, 0.39217, -4.57754, 0.49668, -0.88664, -0.00627, 0.00628, -16.2438, 10.18247, -33.53879, 17.58924, -58.58537, 27.71117, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 61500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67093, 0.00896, 0.24525, 0.04539, 0.10081, 0.06296, 0.07568, 0.07385, 0.06214, 0.06248, 0.06534, 0.03665, 0.0751, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00056, -0.50416, 0.39094, -4.57398, 0.49417, -0.88229, -0.00628, 0.00629, -16.25713, 10.1849, -33.5548, 17.58604, -58.5984, 27.6978, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 62000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67126, 0.00894, 0.24547, 0.04538, 0.10082, 0.06295, 0.07569, 0.07387, 0.06213, 0.06251, 0.06531, 0.03668, 0.07506, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00055, -0.49946, 0.38972, -4.57043, 0.49167, -0.87797, -0.00629, 0.0063, -16.2704, 10.18731, -33.57072, 17.58284, -58.61136, 27.68449, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 62500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67159, 0.00892, 0.24569, 0.04537, 0.10084, 0.06294, 0.0757, 0.07389, 0.06211, 0.06253, 0.06529, 0.03671, 0.07502, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00055, -0.49482, 0.3885, -4.5669, 0.48918, -0.87367, -0.0063, 0.00631, -16.2836, 10.1897, -33.58656, 17.57967, -58.62424, 27.67127, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 63000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67191, 0.0089, 0.2459, 0.04536, 0.10085, 0.06293, 0.07571, 0.07391, 0.06209, 0.06255, 0.06526, 0.03674, 0.07498, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00054, -0.49023, 0.3873, -4.56339, 0.4867, -0.86939, -0.00631, 0.00632, -16.29673, 10.19208, -33.60231, 17.5765, -58.63705, 27.65811, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 63500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67223, 0.00888, 0.24612, 0.04535, 0.10087, 0.06292, 0.07571, 0.07392, 0.06207, 0.06257, 0.06523, 0.03677, 0.07494, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00053, -0.48569, 0.38611, -4.55989, 0.48424, -0.86512, -0.00632, 0.00632, -16.3098, 10.19444, -33.61798, 17.57334, -58.64979, 27.64503, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 64000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67254, 0.00886, 0.24633, 0.04535, 0.10088, 0.06292, 0.07572, 0.07394, 0.06205, 0.06259, 0.06521, 0.0368, 0.07491, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00053, -0.4812, 0.38493, -4.55641, 0.48179, -0.86088, -0.00632, 0.00633, -16.32281, 10.19678, -33.63357, 17.5702, -58.66246, 27.63201, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 64500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67285, 0.00884, 0.24654, 0.04534, 0.1009, 0.06291, 0.07573, 0.07396, 0.06203, 0.06261, 0.06518, 0.03683, 0.07487, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00052, -0.47676, 0.38376, -4.55294, 0.47936, -0.85666, -0.00633, 0.00634, -16.33575, 10.1991, -33.64907, 17.56707, -58.67505, 27.61907, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 65000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67316, 0.00882, 0.24675, 0.04533, 0.10091, 0.0629, 0.07574, 0.07398, 0.06201, 0.06263, 0.06516, 0.03685, 0.07483, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00051, -0.47236, 0.3826, -4.54949, 0.47693, -0.85246, -0.00634, 0.00635, -16.34864, 10.20141, -33.6645, 17.56395, -58.68758, 27.60619, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 65500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67346, 0.0088, 0.24696, 0.04532, 0.10093, 0.06289, 0.07575, 0.07399, 0.062, 0.06266, 0.06513, 0.03688, 0.07479, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00051, -0.46802, 0.38145, -4.54606, 0.47452, -0.84828, -0.00635, 0.00636, -16.36146, 10.20371, -33.67985, 17.56084, -58.70004, 27.59338, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 66000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67376, 0.00878, 0.24717, 0.04531, 0.10094, 0.06288, 0.07576, 0.07401, 0.06198, 0.06268, 0.0651, 0.03691, 0.07475, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.0005, -0.46372, 0.3803, -4.54263, 0.47212, -0.84412, -0.00636, 0.00637, -16.37422, 10.20598, -33.69512, 17.55775, -58.71243, 27.58063, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 66500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67406, 0.00877, 0.24738, 0.0453, 0.10096, 0.06287, 0.07577, 0.07403, 0.06196, 0.0627, 0.06508, 0.03694, 0.07471, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00049, -0.45947, 0.37917, -4.53923, 0.46973, -0.83997, -0.00637, 0.00638, -16.38693, 10.20824, -33.71032, 17.55466, -58.72475, 27.56795, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 67000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67435, 0.00875, 0.24758, 0.04529, 0.10097, 0.06287, 0.07577, 0.07404, 0.06194, 0.06272, 0.06505, 0.03697, 0.07468, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00049, -0.45526, 0.37805, -4.53584, 0.46736, -0.83585, -0.00637, 0.00638, -16.39958, 10.21049, -33.72544, 17.55158, -58.73701, 27.55534, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 67500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67464, 0.00873, 0.24779, 0.04529, 0.10099, 0.06286, 0.07578, 0.07406, 0.06192, 0.06274, 0.06503, 0.037, 0.07464, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00048, -0.4511, 0.37693, -4.53246, 0.46499, -0.83174, -0.00638, 0.00639, -16.41217, 10.21272, -33.74049, 17.54852, -58.74921, 27.54278, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 68000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67492, 0.00871, 0.24799, 0.04528, 0.101, 0.06285, 0.07579, 0.07408, 0.0619, 0.06276, 0.065, 0.03703, 0.0746, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00048, -0.44698, 0.37583, -4.52909, 0.46264, -0.82765, -0.00639, 0.0064, -16.4247, 10.21493, -33.75546, 17.54547, -58.76134, 27.53029, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 68500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.6752, 0.00869, 0.2482, 0.04527, 0.10102, 0.06284, 0.0758, 0.0741, 0.06189, 0.06278, 0.06498, 0.03706, 0.07456, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00047, -0.4429, 0.37473, -4.52574, 0.4603, -0.82357, -0.0064, 0.00641, -16.43718, 10.21713, -33.77037, 17.54242, -58.77341, 27.51786, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 69000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67548, 0.00867, 0.2484, 0.04526, 0.10103, 0.06283, 0.07581, 0.07411, 0.06187, 0.0628, 0.06495, 0.03709, 0.07453, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00047, -0.43887, 0.37364, -4.52241, 0.45797, -0.81952, -0.00641, 0.00642, -16.4496, 10.21932, -33.7852, 17.53939, -58.78541, 27.50549, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 69500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67576, 0.00865, 0.2486, 0.04525, 0.10105, 0.06282, 0.07581, 0.07413, 0.06185, 0.06282, 0.06493, 0.03711, 0.07449, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00046, -0.43488, 0.37256, -4.51908, 0.45565, -0.81548, -0.00642, 0.00643, -16.46197, 10.22149, -33.79996, 17.53637, -58.79736, 27.49318, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 70000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.67603, 0.00864, 0.2488, 0.04525, 0.10106, 0.06282, 0.07582, 0.07415, 0.06183, 0.06284, 0.0649, 0.03714, 0.07445, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00045, -0.43093, 0.37149, -4.51577, 0.45334, -0.81146, -0.00642, 0.00643, -16.47429, 10.22364, -33.81466, 17.53336, -58.80925, 27.48093, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 70500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00012, 0.6763, 0.00862, 0.249, 0.04524, 0.10107, 0.06281, 0.07583, 0.07416, 0.06182, 0.06286, 0.06488, 0.03717, 0.07442, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00045, -0.42702, 0.37043, -4.51248, 0.45104, -0.80746, -0.00643, 0.00644, -16.48655, 10.22579, -33.82928, 17.53035, -58.82107, 27.46874, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 71000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.67656, 0.0086, 0.24919, 0.04523, 0.10109, 0.0628, 0.07584, 0.07418, 0.0618, 0.06288, 0.06486, 0.0372, 0.07438, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00044, -0.42316, 0.36937, -4.50919, 0.44875, -0.80347, -0.00644, 0.00645, -16.49876, 10.22791, -33.84384, 17.52736, -58.83284, 27.4566, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 71500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.67683, 0.00858, 0.24939, 0.04522, 0.1011, 0.06279, 0.07585, 0.07419, 0.06178, 0.0629, 0.06483, 0.03723, 0.07435, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00044, -0.41933, 0.36833, -4.50592, 0.44647, -0.7995, -0.00645, 0.00646, -16.51092, 10.23003, -33.85834, 17.52438, -58.84455, 27.44452, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 72000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.67708, 0.00857, 0.24959, 0.04521, 0.10111, 0.06278, 0.07586, 0.07421, 0.06176, 0.06292, 0.06481, 0.03725, 0.07431, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00043, -0.41554, 0.36729, -4.50267, 0.4442, -0.79554, -0.00646, 0.00647, -16.52303, 10.23213, -33.87276, 17.5214, -58.85621, 27.4325, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 72500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.67734, 0.00855, 0.24978, 0.04521, 0.10113, 0.06278, 0.07586, 0.07423, 0.06175, 0.06294, 0.06478, 0.03728, 0.07427, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00043, -0.41178, 0.36626, -4.49942, 0.44195, -0.7916, -0.00646, 0.00647, -16.53509, 10.23422, -33.88713, 17.51844, -58.86781, 27.42053, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 73000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.67759, 0.00853, 0.24997, 0.0452, 0.10114, 0.06277, 0.07587, 0.07424, 0.06173, 0.06296, 0.06476, 0.03731, 0.07424, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00042, -0.40807, 0.36523, -4.49619, 0.4397, -0.78768, -0.00647, 0.00648, -16.54711, 10.23629, -33.90143, 17.51548, -58.87935, 27.40862, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 73500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.67784, 0.00851, 0.25017, 0.04519, 0.10115, 0.06276, 0.07588, 0.07426, 0.06171, 0.06298, 0.06474, 0.03734, 0.0742, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00042, -0.40439, 0.36422, -4.49297, 0.43746, -0.78377, -0.00648, 0.00649, -16.55907, 10.23836, -33.91567, 17.51254, -58.89084, 27.39676, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 74000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.67809, 0.0085, 0.25036, 0.04518, 0.10117, 0.06275, 0.07589, 0.07428, 0.0617, 0.063, 0.06471, 0.03736, 0.07417, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00041, -0.40075, 0.36321, -4.48976, 0.43523, -0.77988, -0.00649, 0.0065, -16.57098, 10.2404, -33.92985, 17.5096, -58.90227, 27.38495, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 74500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.67834, 0.00848, 0.25055, 0.04518, 0.10118, 0.06274, 0.07589, 0.07429, 0.06168, 0.06302, 0.06469, 0.03739, 0.07413, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00041, -0.39715, 0.36221, -4.48656, 0.43301, -0.776, -0.0065, 0.00651, -16.58285, 10.24244, -33.94396, 17.50668, -58.91365, 27.3732, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 75000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.67858, 0.00846, 0.25074, 0.04517, 0.10119, 0.06274, 0.0759, 0.07431, 0.06166, 0.06304, 0.06466, 0.03742, 0.0741, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0004, -0.39358, 0.36121, -4.48338, 0.4308, -0.77214, -0.0065, 0.00651, -16.59468, 10.24447, -33.95802, 17.50376, -58.92498, 27.36149, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 75500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.67882, 0.00845, 0.25093, 0.04516, 0.10121, 0.06273, 0.07591, 0.07432, 0.06164, 0.06306, 0.06464, 0.03744, 0.07406, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0004, -0.39005, 0.36022, -4.4802, 0.4286, -0.76829, -0.00651, 0.00652, -16.60645, 10.24648, -33.97202, 17.50085, -58.93626, 27.34984, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 76000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.67906, 0.00843, 0.25112, 0.04515, 0.10122, 0.06272, 0.07592, 0.07434, 0.06163, 0.06308, 0.06462, 0.03747, 0.07403, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.0004, -0.38655, 0.35924, -4.47704, 0.42641, -0.76446, -0.00652, 0.00653, -16.61818, 10.24848, -33.98595, 17.49795, -58.94749, 27.33824, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 76500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.67929, 0.00841, 0.2513, 0.04515, 0.10123, 0.06271, 0.07593, 0.07436, 0.06161, 0.0631, 0.06459, 0.0375, 0.07399, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00039, -0.38308, 0.35827, -4.47389, 0.42423, -0.76064, -0.00653, 0.00654, -16.62987, 10.25047, -33.99983, 17.49505, -58.95867, 27.32668, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 77000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.67952, 0.0084, 0.25149, 0.04514, 0.10125, 0.06271, 0.07593, 0.07437, 0.06159, 0.06312, 0.06457, 0.03753, 0.07396, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00039, -0.37965, 0.3573, -4.47075, 0.42205, -0.75683, -0.00654, 0.00654, -16.64151, 10.25244, -34.01366, 17.49217, -58.96979, 27.31518, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 77500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.67975, 0.00838, 0.25168, 0.04513, 0.10126, 0.0627, 0.07594, 0.07439, 0.06158, 0.06314, 0.06455, 0.03755, 0.07393, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00038, -0.37625, 0.35635, -4.46762, 0.41989, -0.75304, -0.00654, 0.00655, -16.65311, 10.25441, -34.02742, 17.48929, -58.98087, 27.30372, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True False False False  True False  True  True  True  True]
Iteration 78000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.67998, 0.00837, 0.25186, 0.04513, 0.10127, 0.06269, 0.07595, 0.0744, 0.06156, 0.06316, 0.06453, 0.03758, 0.07389, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00038, -0.37289, 0.35539, -4.46451, 0.41773, -0.74926, -0.00655, 0.00656, -16.66466, 10.25636, -34.04114, 17.48643, -58.9919, 27.29232, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 78500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.6802, 0.00835, 0.25204, 0.04512, 0.10128, 0.06268, 0.07596, 0.07442, 0.06155, 0.06318, 0.0645, 0.0376, 0.07386, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00037, -0.36955, 0.35445, -4.4614, 0.41559, -0.7455, -0.00656, 0.00657, -16.67618, 10.2583, -34.05479, 17.48357, -59.00288, 27.28096, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 79000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68042, 0.00833, 0.25223, 0.04511, 0.10129, 0.06268, 0.07596, 0.07443, 0.06153, 0.06319, 0.06448, 0.03763, 0.07382, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00037, -0.36625, 0.35351, -4.4583, 0.41345, -0.74175, -0.00657, 0.00658, -16.68765, 10.26023, -34.06839, 17.48072, -59.01382, 27.26964, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 79500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68064, 0.00832, 0.25241, 0.04511, 0.10131, 0.06267, 0.07597, 0.07445, 0.06151, 0.06321, 0.06446, 0.03766, 0.07379, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00037, -0.36298, 0.35257, -4.45522, 0.41132, -0.73801, -0.00657, 0.00658, -16.69907, 10.26215, -34.08194, 17.47787, -59.0247, 27.25838, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 80000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68086, 0.0083, 0.25259, 0.0451, 0.10132, 0.06266, 0.07598, 0.07447, 0.0615, 0.06323, 0.06444, 0.03768, 0.07376, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00036, -0.35974, 0.35165, -4.45215, 0.40919, -0.73429, -0.00658, 0.00659, -16.71046, 10.26406, -34.09544, 17.47504, -59.03554, 27.24715, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 80500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68107, 0.00829, 0.25277, 0.04509, 0.10133, 0.06265, 0.07599, 0.07448, 0.06148, 0.06325, 0.06441, 0.03771, 0.07372, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00036, -0.35654, 0.35073, -4.44908, 0.40708, -0.73058, -0.00659, 0.0066, -16.72181, 10.26595, -34.10888, 17.47221, -59.04634, 27.23598, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 81000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68128, 0.00827, 0.25295, 0.04509, 0.10134, 0.06265, 0.07599, 0.0745, 0.06146, 0.06327, 0.06439, 0.03774, 0.07369, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00035, -0.35336, 0.34981, -4.44603, 0.40497, -0.72688, -0.0066, 0.00661, -16.73312, 10.26784, -34.12227, 17.46939, -59.05709, 27.22484, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 81500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68149, 0.00826, 0.25313, 0.04508, 0.10135, 0.06264, 0.076, 0.07451, 0.06145, 0.06329, 0.06437, 0.03776, 0.07366, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00035, -0.35021, 0.3489, -4.44298, 0.40288, -0.7232, -0.0066, 0.00661, -16.74438, 10.26972, -34.13561, 17.46658, -59.0678, 27.21376, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 82000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.6817, 0.00824, 0.25331, 0.04507, 0.10137, 0.06263, 0.07601, 0.07453, 0.06143, 0.06331, 0.06435, 0.03779, 0.07362, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00035, -0.34709, 0.348, -4.43995, 0.40079, -0.71952, -0.00661, 0.00662, -16.75561, 10.27158, -34.1489, 17.46378, -59.07846, 27.20271, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 82500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.6819, 0.00823, 0.25348, 0.04507, 0.10138, 0.06262, 0.07602, 0.07454, 0.06142, 0.06333, 0.06432, 0.03781, 0.07359, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00034, -0.344, 0.3471, -4.43693, 0.3987, -0.71586, -0.00662, 0.00663, -16.7668, 10.27344, -34.16214, 17.46098, -59.08908, 27.19171, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 83000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.6821, 0.00821, 0.25366, 0.04506, 0.10139, 0.06262, 0.07602, 0.07456, 0.0614, 0.06334, 0.0643, 0.03784, 0.07356, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00034, -0.34094, 0.34621, -4.43391, 0.39663, -0.71222, -0.00663, 0.00664, -16.77795, 10.27528, -34.17533, 17.45819, -59.09966, 27.18075, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 83500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.6823, 0.00819, 0.25384, 0.04505, 0.1014, 0.06261, 0.07603, 0.07457, 0.06138, 0.06336, 0.06428, 0.03786, 0.07353, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00033, -0.33791, 0.34533, -4.43091, 0.39456, -0.70858, -0.00663, 0.00664, -16.78907, 10.27712, -34.18847, 17.45541, -59.11019, 27.16983, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 84000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.6825, 0.00818, 0.25401, 0.04505, 0.10141, 0.0626, 0.07604, 0.07459, 0.06137, 0.06338, 0.06426, 0.03789, 0.07349, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00033, -0.3349, 0.34445, -4.42791, 0.3925, -0.70496, -0.00664, 0.00665, -16.80014, 10.27894, -34.20157, 17.45263, -59.12068, 27.15896, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 84500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.6827, 0.00817, 0.25419, 0.04504, 0.10142, 0.06259, 0.07605, 0.0746, 0.06135, 0.0634, 0.06424, 0.03792, 0.07346, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00033, -0.33192, 0.34357, -4.42493, 0.39045, -0.70134, -0.00665, 0.00666, -16.81118, 10.28076, -34.21461, 17.44986, -59.13113, 27.14812, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 85000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68289, 0.00815, 0.25436, 0.04503, 0.10144, 0.06259, 0.07605, 0.07462, 0.06134, 0.06342, 0.06421, 0.03794, 0.07343, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00032, -0.32897, 0.3427, -4.42195, 0.3884, -0.69774, -0.00665, 0.00666, -16.82219, 10.28256, -34.22761, 17.4471, -59.14154, 27.13733, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 85500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68308, 0.00814, 0.25453, 0.04503, 0.10145, 0.06258, 0.07606, 0.07463, 0.06132, 0.06343, 0.06419, 0.03797, 0.0734, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00032, -0.32605, 0.34184, -4.41898, 0.38637, -0.69416, -0.00666, 0.00667, -16.83315, 10.28436, -34.24056, 17.44435, -59.15191, 27.12658, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 86000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68327, 0.00812, 0.25471, 0.04502, 0.10146, 0.06257, 0.07607, 0.07465, 0.06131, 0.06345, 0.06417, 0.03799, 0.07337, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00032, -0.32315, 0.34098, -4.41603, 0.38434, -0.69058, -0.00667, 0.00668, -16.84408, 10.28614, -34.25347, 17.4416, -59.16224, 27.11586, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 86500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68346, 0.00811, 0.25488, 0.04501, 0.10147, 0.06256, 0.07607, 0.07466, 0.06129, 0.06347, 0.06415, 0.03802, 0.07334, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00031, -0.32028, 0.34013, -4.41308, 0.38231, -0.68701, -0.00668, 0.00669, -16.85498, 10.28792, -34.26633, 17.43886, -59.17253, 27.10519, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 87000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68365, 0.00809, 0.25505, 0.04501, 0.10148, 0.06256, 0.07608, 0.07468, 0.06128, 0.06349, 0.06413, 0.03804, 0.0733, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00031, -0.31743, 0.33929, -4.41014, 0.3803, -0.68346, -0.00668, 0.00669, -16.86584, 10.28969, -34.27914, 17.43613, -59.18278, 27.09455, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 87500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68383, 0.00808, 0.25522, 0.045, 0.10149, 0.06255, 0.07609, 0.07469, 0.06126, 0.06351, 0.06411, 0.03807, 0.07327, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00031, -0.31462, 0.33844, -4.40721, 0.37829, -0.67992, -0.00669, 0.0067, -16.87667, 10.29144, -34.29191, 17.4334, -59.19299, 27.08395, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 88000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68401, 0.00806, 0.25539, 0.045, 0.1015, 0.06254, 0.0761, 0.07471, 0.06124, 0.06353, 0.06409, 0.03809, 0.07324, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.0003, -0.31182, 0.33761, -4.40429, 0.37628, -0.67638, -0.0067, 0.00671, -16.88746, 10.29319, -34.30464, 17.43068, -59.20317, 27.07339, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 88500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68419, 0.00805, 0.25556, 0.04499, 0.10151, 0.06254, 0.0761, 0.07472, 0.06123, 0.06354, 0.06407, 0.03812, 0.07321, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.0003, -0.30905, 0.33678, -4.40137, 0.37429, -0.67286, -0.00671, 0.00672, -16.89822, 10.29493, -34.31732, 17.42797, -59.2133, 27.06287, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 89000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68437, 0.00804, 0.25572, 0.04498, 0.10152, 0.06253, 0.07611, 0.07474, 0.06121, 0.06356, 0.06404, 0.03814, 0.07318, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.0003, -0.30631, 0.33595, -4.39847, 0.3723, -0.66935, -0.00671, 0.00672, -16.90895, 10.29666, -34.32996, 17.42526, -59.2234, 27.05239, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 89500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68455, 0.00802, 0.25589, 0.04498, 0.10153, 0.06252, 0.07612, 0.07475, 0.0612, 0.06358, 0.06402, 0.03816, 0.07315, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00029, -0.30358, 0.33513, -4.39557, 0.37031, -0.66585, -0.00672, 0.00673, -16.91964, 10.29839, -34.34256, 17.42256, -59.23346, 27.04194, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 90000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68472, 0.00801, 0.25606, 0.04497, 0.10155, 0.06252, 0.07612, 0.07476, 0.06118, 0.0636, 0.064, 0.03819, 0.07312, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00029, -0.30089, 0.33431, -4.39268, 0.36834, -0.66236, -0.00673, 0.00674, -16.9303, 10.3001, -34.35512, 17.41986, -59.24349, 27.03153, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 90500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68489, 0.00799, 0.25622, 0.04497, 0.10156, 0.06251, 0.07613, 0.07478, 0.06117, 0.06361, 0.06398, 0.03821, 0.07309, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00029, -0.29821, 0.3335, -4.38981, 0.36637, -0.65888, -0.00673, 0.00674, -16.94093, 10.3018, -34.36763, 17.41717, -59.25347, 27.02115, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 91000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68506, 0.00798, 0.25639, 0.04496, 0.10157, 0.0625, 0.07614, 0.07479, 0.06115, 0.06363, 0.06396, 0.03824, 0.07305, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00029, -0.29556, 0.3327, -4.38693, 0.3644, -0.65541, -0.00674, 0.00675, -16.95152, 10.3035, -34.3801, 17.41449, -59.26343, 27.01081, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 91500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.68523, 0.00797, 0.25655, 0.04495, 0.10158, 0.06249, 0.07615, 0.07481, 0.06114, 0.06365, 0.06394, 0.03826, 0.07302, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00028, -0.29294, 0.33189, -4.38407, 0.36245, -0.65195, -0.00675, 0.00676, -16.96209, 10.30519, -34.39253, 17.41182, -59.27334, 27.0005, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 92000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.00011, 0.6854, 0.00795, 0.25672, 0.04495, 0.10159, 0.06249, 0.07615, 0.07482, 0.06112, 0.06367, 0.06392, 0.03829, 0.07299, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00028, -0.29033, 0.3311, -4.38122, 0.3605, -0.6485, -0.00675, 0.00676, -16.97262, 10.30687, -34.40493, 17.40915, -59.28323, 26.99023, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 92500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.0001, 0.68557, 0.00794, 0.25688, 0.04494, 0.1016, 0.06248, 0.07616, 0.07484, 0.06111, 0.06368, 0.0639, 0.03831, 0.07296, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00028, -0.28775, 0.3303, -4.37837, 0.35855, -0.64507, -0.00676, 0.00677, -16.98312, 10.30854, -34.41728, 17.40648, -59.29307, 26.98, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 93000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.0001, 0.68573, 0.00793, 0.25705, 0.04494, 0.10161, 0.06247, 0.07617, 0.07485, 0.06109, 0.0637, 0.06388, 0.03833, 0.07293, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [-0.0, -0.0, 0.00027, -0.28519, 0.32952, -4.37553, 0.35661, -0.64164, -0.00677, 0.00678, -16.99359, 10.3102, -34.42959, 17.40382, -59.30289, 26.9698, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 93500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.0001, 0.68589, 0.00791, 0.25721, 0.04493, 0.10162, 0.06247, 0.07617, 0.07486, 0.06108, 0.06372, 0.06386, 0.03836, 0.0729, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00027, -0.28266, 0.32873, -4.3727, 0.35468, -0.63822, -0.00678, 0.00679, -17.00404, 10.31185, -34.44186, 17.40117, -59.31267, 26.95963, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 94000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.0001, 0.68605, 0.0079, 0.25737, 0.04492, 0.10163, 0.06246, 0.07618, 0.07488, 0.06106, 0.06374, 0.06384, 0.03838, 0.07287, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00027, -0.28014, 0.32795, -4.36988, 0.35276, -0.63481, -0.00678, 0.00679, -17.01445, 10.3135, -34.4541, 17.39853, -59.32241, 26.9495, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 94500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.0001, 0.68621, 0.00789, 0.25753, 0.04492, 0.10164, 0.06245, 0.07619, 0.07489, 0.06105, 0.06375, 0.06382, 0.03841, 0.07284, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00026, -0.27765, 0.32718, -4.36706, 0.35083, -0.63141, -0.00679, 0.0068, -17.02483, 10.31514, -34.46629, 17.39589, -59.33212, 26.93939, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 95000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.0001, 0.68637, 0.00787, 0.25769, 0.04491, 0.10165, 0.06245, 0.07619, 0.07491, 0.06103, 0.06377, 0.0638, 0.03843, 0.07281, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 1 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00026, -0.27518, 0.32641, -4.36425, 0.34892, -0.62802, -0.0068, 0.00681, -17.03518, 10.31677, -34.47845, 17.39325, -59.3418, 26.92933, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False  True  True  True  True]
Iteration 95500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.0001, 0.68652, 0.00786, 0.25785, 0.04491, 0.10166, 0.06244, 0.0762, 0.07492, 0.06102, 0.06379, 0.06378, 0.03845, 0.07278, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 0 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00026, -0.27272, 0.32564, -4.36145, 0.34701, -0.62464, -0.0068, 0.00681, -17.0455, 10.31839, -34.49057, 17.39062, -59.35145, 26.91929, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False False  True  True  True]
Iteration 96000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.0001, 0.68667, 0.00785, 0.25801, 0.0449, 0.10167, 0.06243, 0.07621, 0.07494, 0.061, 0.0638, 0.06376, 0.03848, 0.07275, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 0 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00026, -0.27029, 0.32488, -4.35866, 0.34511, -0.62127, -0.00681, 0.00682, -17.0558, 10.32, -34.50265, 17.388, -59.36107, 26.90929, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False False  True  True  True]
Iteration 96500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.0001, 0.68683, 0.00783, 0.25817, 0.0449, 0.10168, 0.06243, 0.07621, 0.07495, 0.06099, 0.06382, 0.06374, 0.0385, 0.07272, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 0 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00025, -0.26788, 0.32413, -4.35588, 0.34321, -0.6179, -0.00682, 0.00683, -17.06607, 10.32161, -34.5147, 17.38538, -59.37065, 26.89932, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False False  True  True  True]
Iteration 97000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.0001, 0.68698, 0.00782, 0.25833, 0.04489, 0.10169, 0.06242, 0.07622, 0.07496, 0.06098, 0.06384, 0.06372, 0.03853, 0.07269, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 0 1 0 0]
Discounting practice with new adv calculation: [0.0, -0.0, 0.00025, -0.26549, 0.32337, -4.3531, 0.34132, -0.61455, -0.00682, 0.00683, -17.0763, 10.32321, -34.52671, 17.38277, -59.3802, 26.88938, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False False  True  True  True]
Iteration 97500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.0001, 0.68713, 0.00781, 0.25849, 0.04489, 0.1017, 0.06241, 0.07623, 0.07498, 0.06096, 0.06386, 0.0637, 0.03855, 0.07267, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 0 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00025, -0.26313, 0.32262, -4.35033, 0.33943, -0.61121, -0.00683, 0.00684, -17.08651, 10.3248, -34.53868, 17.38016, -59.38972, 26.87947, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False False  True  True  True]
Iteration 98000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.0001, 0.68727, 0.0078, 0.25864, 0.04488, 0.10171, 0.06241, 0.07623, 0.07499, 0.06095, 0.06387, 0.06368, 0.03857, 0.07264, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 0 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00025, -0.26078, 0.32188, -4.34757, 0.33755, -0.60787, -0.00684, 0.00685, -17.0967, 10.32639, -34.55062, 17.37756, -59.39921, 26.86959, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False False  True  True  True]
Iteration 98500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.0001, 0.68742, 0.00778, 0.2588, 0.04487, 0.10172, 0.0624, 0.07624, 0.07501, 0.06093, 0.06389, 0.06366, 0.0386, 0.07261, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 0 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00024, -0.25845, 0.32114, -4.34481, 0.33568, -0.60454, -0.00684, 0.00685, -17.10685, 10.32796, -34.56252, 17.37496, -59.40867, 26.85974, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False False  True  True  True]
Iteration 99000
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.0001, 0.68756, 0.00777, 0.25896, 0.04487, 0.10173, 0.06239, 0.07625, 0.07502, 0.06092, 0.06391, 0.06364, 0.03862, 0.07258, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 0 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00024, -0.25614, 0.3204, -4.34206, 0.33381, -0.60123, -0.00685, 0.00686, -17.11698, 10.32953, -34.57439, 17.37237, -59.4181, 26.84992, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False False  True  True  True]
Iteration 99500
Discounting practice with old adv calculation: [0.00013, 0.66508, 0.0001, 0.68771, 0.00776, 0.25911, 0.04486, 0.10174, 0.06238, 0.07625, 0.07503, 0.0609, 0.06392, 0.06362, 0.03864, 0.07255, 0.06931, 0.06931, 0.06931, 0.06931]
The better action at a given state with old adv calculation: [1 1 1 1 1 0 0 1 0 0]
Discounting practice with new adv calculation: [0.0, 0.0, 0.00024, -0.25385, 0.31967, -4.33932, 0.33194, -0.59792, -0.00686, 0.00687, -17.12708, 10.3311, -34.58622, 17.36978, -59.42749, 26.84014, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False False False False  True False False  True  True  True]
Iteration 0
Discounting jh with old adv calculation: [0.0082, -0.0082, 0.00772, -0.00772, 0.00674, -0.00674, 0.00475, -0.00475, 0.00075, -0.00075, -0.00734, 0.00734, -0.02368, 0.02368, -0.0567, 0.0567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 0 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0082, -0.0082, 0.00772, -0.00772, 0.00674, -0.00674, 0.00475, -0.00475, 0.00075, -0.00075, -0.00734, 0.00734, -0.02368, 0.02368, -0.0567, 0.0567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 0 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 1000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 1500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 2000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 2500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 3000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 3500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 4000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 4500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 5000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 5500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 6000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 6500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 7000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 7500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 8000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 8500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 9000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 9500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 10000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 10500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 11000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 11500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 12000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 12500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 13000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 13500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 14000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 14500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 15000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 15500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 16000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 16500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 17000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 17500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 18000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 18500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 19000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 19500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 20000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 20500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 21000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 21500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 22000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 22500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 23000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 23500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 24000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 24500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 25000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 25500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 26000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 26500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 27000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 27500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 28000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 28500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 29000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 29500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 30000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 30500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 31000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 31500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 32000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 32500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 33000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 33500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 34000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 34500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 35000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 35500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 36000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 36500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 37000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 37500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 38000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 38500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 39000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 39500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 40000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 40500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 41000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 41500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 42000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 42500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 43000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 43500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 44000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 44500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 45000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 45500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 46000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 46500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 47000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 47500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 48000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 48500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 49000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 49500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 50000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 50500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 51000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 51500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 52000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 52500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 53000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 53500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 54000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 54500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 55000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 55500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 56000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 56500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 57000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 57500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 58000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 58500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 59000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 59500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 60000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 60500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 61000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 61500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 62000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 62500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 63000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 63500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 64000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 64500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 65000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 65500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 66000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 66500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 67000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 67500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 68000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 68500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 69000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 69500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 70000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 70500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 71000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 71500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 72000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 72500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 73000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 73500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 74000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 74500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 75000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 75500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 76000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 76500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 77000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 77500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 78000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 78500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 79000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 79500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 80000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 80500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 81000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 81500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 82000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 82500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 83000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 83500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 84000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 84500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 85000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 85500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 86000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 86500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 87000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 87500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 88000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 88500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 89000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 89500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 90000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 90500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 91000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 91500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 92000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 92500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 93000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 93500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 94000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, -0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 94500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 95000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 95500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 96000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 96500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 97000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 97500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 98000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 98500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 99000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 99500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 0
Discounting jh with old adv calculation: [0.0082, -0.0082, 0.00772, -0.00772, 0.00674, -0.00674, 0.00475, -0.00475, 0.00075, -0.00075, -0.00734, 0.00734, -0.02368, 0.02368, -0.0567, 0.0567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 0 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0082, -0.0082, 0.00772, -0.00772, 0.00674, -0.00674, 0.00475, -0.00475, 0.00075, -0.00075, -0.00734, 0.00734, -0.02368, 0.02368, -0.0567, 0.0567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 0 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 1000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 1500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 2000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 2500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 3000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 3500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 4000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 4500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 5000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 5500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 6000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 6500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 7000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 7500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 8000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 8500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 9000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 9500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 10000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 10500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 11000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 11500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 12000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 12500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 13000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 13500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 14000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 14500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 15000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 15500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 16000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 16500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 17000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 17500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 18000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 18500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 19000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 19500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 20000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 20500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 21000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 21500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 22000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 22500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 23000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 23500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 24000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 24500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 25000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 25500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 26000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 26500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 27000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 27500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 28000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 28500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 29000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 29500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 30000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 30500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 31000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 31500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 32000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 32500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 33000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 33500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 34000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 34500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 35000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 35500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 36000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 36500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 37000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 37500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 38000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 38500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 39000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 39500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 40000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 40500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 41000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 41500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 42000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 42500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 43000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 43500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 44000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 44500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 45000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 45500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 46000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 46500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 47000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 47500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 48000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 48500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 49000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 49500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False  True  True  True  True  True  True  True  True  True]
Iteration 50000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 50500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 51000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 51500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 52000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 52500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 53000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 53500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 54000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 54500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 55000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 55500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 56000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 56500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 57000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 57500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 58000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 58500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 59000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 59500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 60000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 60500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 61000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 61500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 62000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 62500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 63000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 63500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 64000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 64500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 65000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 65500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 66000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 66500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 67000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 67500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 68000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 68500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 69000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 69500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 70000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 70500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 71000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 71500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 72000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 72500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 73000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 73500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 74000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 74500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 75000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 75500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 76000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 76500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 77000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 77500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 78000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 78500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 79000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 79500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 80000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 80500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 81000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 81500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 82000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 82500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 83000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 83500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 84000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 84500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 85000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 85500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 86000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 86500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 87000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 87500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 88000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 88500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 89000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 89500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 90000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 90500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 91000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 91500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 92000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 92500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 93000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 93500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 94000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 94500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 95000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 95500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 96000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 96500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 97000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 97500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 98000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 98500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 99000
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 99500
Discounting jh with old adv calculation: [0.00771, -0.00815, 0.00716, -0.00755, 0.00601, -0.00627, 0.00355, -0.00364, -0.00159, 0.00158, -0.01206, 0.01111, -0.03226, 0.02622, -0.06793, 0.04547, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 0
Discounting jh with old adv calculation: [0.0082, -0.0082, 0.00772, -0.00772, 0.00674, -0.00674, 0.00475, -0.00475, 0.00075, -0.00075, -0.00734, 0.00734, -0.02368, 0.02368, -0.0567, 0.0567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 0 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0082, -0.0082, 0.00772, -0.00772, 0.00674, -0.00674, 0.00475, -0.00475, 0.00075, -0.00075, -0.00734, 0.00734, -0.02368, 0.02368, -0.0567, 0.0567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 0 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 500
Discounting jh with old adv calculation: [0.0, -0.0039, -0.00492, 0.05051, -0.06484, 1e-05, -0.07435, 1e-05, -0.08397, 1e-05, -0.09367, 1e-05, -0.10348, 1e-05, -0.11339, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, -0.00492, 0.05051, -138.83457, 0.01487, -160.25541, 0.01274, -182.02362, 0.01224, -204.07307, 0.01232, -226.35013, 0.01232, -248.86135, 0.01231, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 1000
Discounting jh with old adv calculation: [0.0, -0.00024, -0.00861, 0.04684, -0.06487, 0.0, -0.07438, 0.0, -0.08398, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, -0.00861, 0.04684, -2380.23384, 0.04925, -2730.65684, 0.04755, -3084.65807, 0.04699, -3442.25902, 0.04695, -3803.48193, 0.04686, -4168.36281, 0.04677, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 1500
Discounting jh with old adv calculation: [-0.0, 0.01075, -0.01972, 0.03574, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.01075, -0.01972, 0.03574, -0.0, 0.0, -1.66422, 1e-05, -3.36287, 1e-05, -5.09263, 2e-05, -6.85085, 2e-05, -8.63604, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 2000
Discounting jh with old adv calculation: [-1e-05, 0.04598, -0.0553, 0.00015, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04598, -4.60436, 0.01265, -0.0, -0.0, -1.66348, 1e-05, -3.36155, 1e-05, -5.09085, 2e-05, -6.84868, 2e-05, -8.63354, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 2500
Discounting jh with old adv calculation: [-1e-05, 0.04606, -0.05539, 7e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04606, -3.79345, 0.0046, -0.0, -0.0, -1.66274, 1e-05, -3.36024, 1e-05, -5.08907, 2e-05, -6.84651, 2e-05, -8.63105, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 3000
Discounting jh with old adv calculation: [-1e-05, 0.04608, -0.05541, 5e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04608, -3.40008, 0.00278, -0.0, -0.0, -1.66201, 1e-05, -3.35892, 1e-05, -5.08729, 2e-05, -6.84434, 2e-05, -8.62856, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 3500
Discounting jh with old adv calculation: [-1e-05, 0.04609, -0.05542, 3e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04609, -3.14037, 0.00198, 0.0, 0.0, -1.66127, 1e-05, -3.35761, 1e-05, -5.08551, 2e-05, -6.84218, 2e-05, -8.62608, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 4000
Discounting jh with old adv calculation: [-1e-05, 0.0461, -0.05543, 3e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0461, -2.94715, 0.00153, -0.0, 0.0, -1.66054, 1e-05, -3.3563, 1e-05, -5.08374, 2e-05, -6.84003, 2e-05, -8.6236, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 4500
Discounting jh with old adv calculation: [-1e-05, 0.0461, -0.05543, 2e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0461, -2.79371, 0.00124, -0.0, 0.0, -1.6598, 1e-05, -3.355, 1e-05, -5.08197, 2e-05, -6.83788, 2e-05, -8.62113, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 5000
Discounting jh with old adv calculation: [-1e-05, 0.04611, -0.05543, 2e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04611, -2.66675, 0.00104, -0.0, 0.0, -1.65907, 1e-05, -3.35369, 1e-05, -5.0802, 2e-05, -6.83573, 2e-05, -8.61867, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 5500
Discounting jh with old adv calculation: [-1e-05, 0.04611, -0.05544, 2e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04611, -2.55866, 0.0009, -0.0, 0.0, -1.65834, 1e-05, -3.35239, 1e-05, -5.07844, 2e-05, -6.83359, 2e-05, -8.61621, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 6000
Discounting jh with old adv calculation: [-1e-05, 0.04611, -0.05544, 2e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04611, -2.46468, 0.00079, -0.0, 0.0, -1.65761, 1e-05, -3.35109, 1e-05, -5.07668, 2e-05, -6.83145, 2e-05, -8.61376, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 6500
Discounting jh with old adv calculation: [-1e-05, 0.04611, -0.05544, 2e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04611, -2.38165, 0.0007, -0.0, 0.0, -1.65688, 1e-05, -3.34979, 1e-05, -5.07492, 2e-05, -6.82932, 2e-05, -8.61132, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 7000
Discounting jh with old adv calculation: [-1e-05, 0.04611, -0.05544, 2e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04611, -2.30736, 0.00063, -0.0, 0.0, -1.65615, 1e-05, -3.34849, 1e-05, -5.07317, 2e-05, -6.8272, 2e-05, -8.60888, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 7500
Discounting jh with old adv calculation: [-1e-05, 0.04611, -0.05544, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04611, -2.2402, 0.00057, -0.0, 0.0, -1.65542, 1e-05, -3.34719, 1e-05, -5.07142, 2e-05, -6.82507, 2e-05, -8.60645, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 8000
Discounting jh with old adv calculation: [-1e-05, 0.04611, -0.05544, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04611, -2.17896, 0.00052, -0.0, 0.0, -1.65469, 1e-05, -3.3459, 1e-05, -5.06967, 2e-05, -6.82296, 2e-05, -8.60402, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 8500
Discounting jh with old adv calculation: [-1e-05, 0.04611, -0.05544, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04611, -2.12273, 0.00048, -0.0, 0.0, -1.65396, 1e-05, -3.3446, 1e-05, -5.06792, 2e-05, -6.82084, 2e-05, -8.6016, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 9000
Discounting jh with old adv calculation: [-1e-05, 0.04611, -0.05544, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04611, -2.07077, 0.00045, -0.0, 0.0, -1.65323, 1e-05, -3.34331, 1e-05, -5.06618, 2e-05, -6.81873, 2e-05, -8.59919, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 9500
Discounting jh with old adv calculation: [-1e-05, 0.04611, -0.05544, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04611, -2.02251, 0.00042, -0.0, 0.0, -1.6525, 1e-05, -3.34202, 1e-05, -5.06444, 2e-05, -6.81663, 2e-05, -8.59678, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 10000
Discounting jh with old adv calculation: [-1e-05, 0.04611, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04611, -1.97747, 0.00039, -0.0, -0.0, -1.65178, 1e-05, -3.34073, 1e-05, -5.06271, 2e-05, -6.81453, 2e-05, -8.59437, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 10500
Discounting jh with old adv calculation: [-1e-05, 0.04611, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04611, -1.93526, 0.00036, 0.0, 0.0, -1.65105, 1e-05, -3.33945, 1e-05, -5.06097, 2e-05, -6.81243, 2e-05, -8.59197, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 11000
Discounting jh with old adv calculation: [-1e-05, 0.04611, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04611, -1.89557, 0.00034, -0.0, 0.0, -1.65032, 1e-05, -3.33816, 1e-05, -5.05924, 2e-05, -6.81034, 2e-05, -8.58958, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 11500
Discounting jh with old adv calculation: [-1e-05, 0.04611, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.04611, -1.85812, 0.00032, -0.0, -0.0, -1.6496, 1e-05, -3.33688, 1e-05, -5.05751, 2e-05, -6.80825, 2e-05, -8.58718, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 12000
Discounting jh with old adv calculation: [-2e-05, 0.04611, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-2e-05, 0.04611, -1.82268, 0.00031, -0.0, 0.0, -1.64887, 1e-05, -3.33559, 1e-05, -5.05579, 2e-05, -6.80616, 2e-05, -8.5848, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 12500
Discounting jh with old adv calculation: [-2e-05, 0.04611, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-2e-05, 0.04611, -1.78904, 0.00029, -0.0, -0.0, -1.64815, 1e-05, -3.33431, 1e-05, -5.05406, 2e-05, -6.80407, 2e-05, -8.58242, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 13000
Discounting jh with old adv calculation: [-2e-05, 0.04611, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-2e-05, 0.04611, -1.75705, 0.00028, -0.0, 0.0, -1.64742, 1e-05, -3.33303, 1e-05, -5.05234, 2e-05, -6.80199, 2e-05, -8.58004, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 13500
Discounting jh with old adv calculation: [-2e-05, 0.04611, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-2e-05, 0.04611, -1.72654, 0.00026, -0.0, 0.0, -1.6467, 1e-05, -3.33175, 1e-05, -5.05062, 2e-05, -6.79991, 2e-05, -8.57766, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 14000
Discounting jh with old adv calculation: [-2e-05, 0.0461, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-2e-05, 0.0461, -1.69739, 0.00025, -0.0, -0.0, -1.64597, 1e-05, -3.33046, 1e-05, -5.0489, 2e-05, -6.79784, 2e-05, -8.57529, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 14500
Discounting jh with old adv calculation: [-3e-05, 0.0461, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-3e-05, 0.0461, -1.66947, 0.00024, -0.0, 0.0, -1.64524, 1e-05, -3.32918, 1e-05, -5.04717, 2e-05, -6.79576, 2e-05, -8.57292, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 15000
Discounting jh with old adv calculation: [-3e-05, 0.0461, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-3e-05, 0.0461, -1.64268, 0.00023, -0.0, 0.0, -1.64451, 1e-05, -3.3279, 1e-05, -5.04545, 2e-05, -6.79368, 2e-05, -8.57055, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 15500
Discounting jh with old adv calculation: [-3e-05, 0.04609, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-3e-05, 0.04609, -1.61691, 0.00022, -0.0, 0.0, -1.64379, 1e-05, -3.32661, 1e-05, -5.04373, 2e-05, -6.7916, 2e-05, -8.56818, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 16000
Discounting jh with old adv calculation: [-4e-05, 0.04609, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-4e-05, 0.04609, -1.59206, 0.00021, -0.0, 0.0, -1.64305, 1e-05, -3.32533, 1e-05, -5.042, 2e-05, -6.78952, 2e-05, -8.5658, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 16500
Discounting jh with old adv calculation: [-5e-05, 0.04608, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-5e-05, 0.04608, -1.56804, 0.0002, -0.0, 0.0, -1.64232, 1e-05, -3.32403, 1e-05, -5.04027, 2e-05, -6.78743, 2e-05, -8.56342, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 17000
Discounting jh with old adv calculation: [-6e-05, 0.04607, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-6e-05, 0.04607, -1.54472, 0.0002, -0.0, 0.0, -1.64158, 1e-05, -3.32273, 1e-05, -5.03852, 2e-05, -6.78533, 2e-05, -8.56102, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 17500
Discounting jh with old adv calculation: [-9e-05, 0.04604, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-9e-05, 0.04604, -1.52192, 0.00019, -0.0, 0.0, -1.64083, 1e-05, -3.32141, 1e-05, -5.03675, 1e-05, -6.7832, 2e-05, -8.5586, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 18000
Discounting jh with old adv calculation: [-0.00014, 0.04598, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00014, 0.04598, -1.4993, 0.00018, -0.0, 0.0, -1.64005, 1e-05, -3.32005, 1e-05, -5.03494, 1e-05, -6.78102, 2e-05, -8.55611, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 18500
Discounting jh with old adv calculation: [-0.00041, 0.04572, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00041, 0.04572, -1.47548, 0.00018, -0.0, 0.0, -1.63921, 1e-05, -3.31857, 1e-05, -5.03296, 1e-05, -6.77864, 2e-05, -8.55341, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 19000
Discounting jh with old adv calculation: [-0.0461, 2e-05, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1.21506, 0.00065, -1.38763, 0.00015, -0.0, 0.0, -1.63585, 1e-05, -3.31271, 1e-05, -5.02519, 1e-05, -6.76937, 2e-05, -8.54293, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 19500
Discounting jh with old adv calculation: [-0.04611, 2e-05, -0.05545, 1e-05, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.77985, 0.00027, -1.24854, 0.00012, 0.0, 0.0, -1.62942, 1e-05, -3.30157, 1e-05, -5.01046, 1e-05, -6.75184, 2e-05, -8.52318, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 20000
Discounting jh with old adv calculation: [-0.04612, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.56337, 0.00016, -1.13736, 0.0001, 0.0, 0.0, -1.62306, 1e-05, -3.29058, 1e-05, -4.99598, 1e-05, -6.73465, 2e-05, -8.50385, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 20500
Discounting jh with old adv calculation: [-0.04612, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.42844, 0.0001, -1.04543, 8e-05, 0.0, 0.0, -1.61676, 1e-05, -3.27975, 1e-05, -4.98175, 1e-05, -6.7178, 2e-05, -8.48493, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 21000
Discounting jh with old adv calculation: [-0.04612, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.33547, 7e-05, -0.96752, 7e-05, 0.0, 0.0, -1.61052, 1e-05, -3.26907, 1e-05, -4.96776, 1e-05, -6.70127, 2e-05, -8.4664, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 21500
Discounting jh with old adv calculation: [-0.04612, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.26764, 6e-05, -0.90028, 6e-05, 0.0, 0.0, -1.60435, 1e-05, -3.25854, 1e-05, -4.954, 1e-05, -6.68504, 2e-05, -8.44825, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 22000
Discounting jh with old adv calculation: [-0.04612, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.21632, 4e-05, -0.8414, 5e-05, -0.0, 0.0, -1.59824, 1e-05, -3.24815, 1e-05, -4.94046, 1e-05, -6.66912, 2e-05, -8.43046, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 22500
Discounting jh with old adv calculation: [-0.04612, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.1765, 3e-05, -0.78925, 5e-05, -0.0, 0.0, -1.59219, 1e-05, -3.2379, 1e-05, -4.92714, 1e-05, -6.65348, 2e-05, -8.41301, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 23000
Discounting jh with old adv calculation: [-0.04612, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.14504, 3e-05, -0.74261, 4e-05, -0.0, 0.0, -1.5862, 1e-05, -3.22778, 1e-05, -4.91403, 1e-05, -6.63811, 2e-05, -8.3959, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 23500
Discounting jh with old adv calculation: [-0.04612, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.11984, 2e-05, -0.70057, 4e-05, -0.0, -0.0, -1.58026, 1e-05, -3.2178, 1e-05, -4.90113, 1e-05, -6.62301, 2e-05, -8.37912, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 24000
Discounting jh with old adv calculation: [-0.04612, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.09945, 2e-05, -0.66243, 3e-05, -0.0, 0.0, -1.57439, 1e-05, -3.20794, 1e-05, -4.88842, 1e-05, -6.60817, 1e-05, -8.36264, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 24500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.08281, 1e-05, -0.62762, 3e-05, -0.0, 0.0, -1.56857, 1e-05, -3.19822, 1e-05, -4.8759, 1e-05, -6.59358, 1e-05, -8.34646, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 25000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.06914, 1e-05, -0.59569, 3e-05, -0.0, 0.0, -1.5628, 1e-05, -3.18861, 1e-05, -4.86357, 1e-05, -6.57924, 1e-05, -8.33056, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 25500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.05786, 1e-05, -0.56628, 3e-05, -0.0, 0.0, -1.55709, 1e-05, -3.17912, 1e-05, -4.85142, 1e-05, -6.56512, 1e-05, -8.31495, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 26000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.04851, 1e-05, -0.53908, 3e-05, -0.0, 0.0, -1.55143, 1e-05, -3.16976, 1e-05, -4.83944, 1e-05, -6.55123, 1e-05, -8.2996, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 26500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.04074, 1e-05, -0.51385, 2e-05, 0.0, 0.0, -1.54582, 1e-05, -3.1605, 1e-05, -4.82764, 1e-05, -6.53756, 1e-05, -8.28452, 2e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 27000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.03425, 1e-05, -0.49036, 2e-05, -0.0, -0.0, -1.54027, 1e-05, -3.15136, 1e-05, -4.816, 1e-05, -6.5241, 1e-05, -8.26968, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 27500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.02883, 0.0, -0.46843, 2e-05, 0.0, 0.0, -1.53476, 1e-05, -3.14232, 1e-05, -4.80452, 1e-05, -6.51085, 1e-05, -8.25509, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 28000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.02428, 0.0, -0.44791, 2e-05, 0.0, 0.0, -1.52931, 1e-05, -3.13339, 1e-05, -4.7932, 1e-05, -6.49779, 1e-05, -8.24073, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 28500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.02047, 0.0, -0.42868, 2e-05, -0.0, 0.0, -1.5239, 1e-05, -3.12457, 1e-05, -4.78203, 1e-05, -6.48493, 1e-05, -8.2266, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 29000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.01727, 0.0, -0.4106, 2e-05, -0.0, -0.0, -1.51854, 1e-05, -3.11585, 1e-05, -4.77102, 1e-05, -6.47226, 1e-05, -8.21269, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 29500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.01457, 0.0, -0.39357, 2e-05, 0.0, 0.0, -1.51323, 1e-05, -3.10722, 1e-05, -4.76014, 1e-05, -6.45977, 1e-05, -8.19899, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 30000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.01231, 0.0, -0.37752, 1e-05, -0.0, 0.0, -1.50797, 1e-05, -3.0987, 1e-05, -4.74941, 1e-05, -6.44746, 1e-05, -8.1855, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 30500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0104, 0.0, -0.36235, 1e-05, 0.0, 0.0, -1.50275, 1e-05, -3.09026, 1e-05, -4.73881, 1e-05, -6.43532, 1e-05, -8.17221, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 31000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00879, 0.0, -0.348, 1e-05, 0.0, 0.0, -1.49757, 1e-05, -3.08193, 1e-05, -4.72835, 1e-05, -6.42335, 1e-05, -8.15912, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 31500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00743, 0.0, -0.33441, 1e-05, 0.0, 0.0, -1.49244, 0.0, -3.07368, 1e-05, -4.71802, 1e-05, -6.41154, 1e-05, -8.14622, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 32000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00628, 0.0, -0.32152, 1e-05, 0.0, 0.0, -1.48736, 0.0, -3.06552, 1e-05, -4.70782, 1e-05, -6.39989, 1e-05, -8.1335, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 32500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00531, 0.0, -0.30927, 1e-05, -0.0, -0.0, -1.48231, 0.0, -3.05745, 1e-05, -4.69774, 1e-05, -6.3884, 1e-05, -8.12096, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 33000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00449, 0.0, -0.29763, 1e-05, -0.0, 0.0, -1.47731, 0.0, -3.04946, 1e-05, -4.68779, 1e-05, -6.37705, 1e-05, -8.10859, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 33500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0038, 0.0, -0.28656, 1e-05, 0.0, 0.0, -1.47235, 0.0, -3.04156, 1e-05, -4.67795, 1e-05, -6.36586, 1e-05, -8.09639, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 34000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00322, 0.0, -0.276, 1e-05, 0.0, 0.0, -1.46743, 0.0, -3.03374, 1e-05, -4.66823, 1e-05, -6.35481, 1e-05, -8.08436, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 34500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00273, 0.0, -0.26594, 1e-05, 0.0, 0.0, -1.46255, 0.0, -3.02601, 1e-05, -4.65862, 1e-05, -6.34389, 1e-05, -8.07249, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 35000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00231, 0.0, -0.25634, 1e-05, 0.0, 0.0, -1.45772, 0.0, -3.01835, 1e-05, -4.64913, 1e-05, -6.33311, 1e-05, -8.06078, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 35500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00196, 0.0, -0.24717, 1e-05, 0.0, 0.0, -1.45292, 0.0, -3.01076, 1e-05, -4.63974, 1e-05, -6.32247, 1e-05, -8.04921, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 36000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00166, 0.0, -0.2384, 1e-05, -0.0, 0.0, -1.44816, 0.0, -3.00326, 1e-05, -4.63046, 1e-05, -6.31195, 1e-05, -8.0378, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 36500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0014, 0.0, -0.23002, 1e-05, -0.0, 0.0, -1.44343, 0.0, -2.99582, 1e-05, -4.62128, 1e-05, -6.30157, 1e-05, -8.02653, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 37000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00119, 0.0, -0.22199, 1e-05, -0.0, 0.0, -1.43875, 0.0, -2.98846, 1e-05, -4.6122, 1e-05, -6.2913, 1e-05, -8.0154, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 37500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00101, 0.0, -0.21431, 1e-05, 0.0, 0.0, -1.4341, 0.0, -2.98118, 1e-05, -4.60322, 1e-05, -6.28116, 1e-05, -8.00441, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 38000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00086, 0.0, -0.20694, 1e-05, -0.0, 0.0, -1.42949, 0.0, -2.97396, 1e-05, -4.59434, 1e-05, -6.27113, 1e-05, -7.99355, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 38500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00073, 0.0, -0.19988, 1e-05, -0.0, 0.0, -1.42491, 0.0, -2.96681, 1e-05, -4.58555, 1e-05, -6.26122, 1e-05, -7.98283, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 39000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00062, 0.0, -0.19311, 1e-05, 0.0, 0.0, -1.42037, 0.0, -2.95973, 1e-05, -4.57686, 1e-05, -6.25142, 1e-05, -7.97223, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 39500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00052, 0.0, -0.18661, 1e-05, 0.0, 0.0, -1.41586, 0.0, -2.95271, 1e-05, -4.56826, 1e-05, -6.24173, 1e-05, -7.96176, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 40000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00045, 0.0, -0.18037, 1e-05, -0.0, 0.0, -1.41139, 0.0, -2.94576, 1e-05, -4.55975, 1e-05, -6.23215, 1e-05, -7.95141, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 40500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00038, 0.0, -0.17437, 1e-05, -0.0, -0.0, -1.40695, 0.0, -2.93888, 1e-05, -4.55132, 1e-05, -6.22268, 1e-05, -7.94117, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 41000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00032, 0.0, -0.1686, 1e-05, -0.0, 0.0, -1.40255, 0.0, -2.93206, 1e-05, -4.54298, 1e-05, -6.2133, 1e-05, -7.93106, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 41500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00028, 0.0, -0.16306, 1e-05, -0.0, 0.0, -1.39818, 0.0, -2.9253, 1e-05, -4.53473, 1e-05, -6.20403, 1e-05, -7.92106, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 42000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05545, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00023, 0.0, -0.15773, 1e-05, -0.0, -0.0, -1.39384, 0.0, -2.9186, 1e-05, -4.52655, 1e-05, -6.19486, 1e-05, -7.91117, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 42500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0002, 0.0, -0.15259, 0.0, -0.0, 0.0, -1.38953, 0.0, -2.91196, 1e-05, -4.51846, 1e-05, -6.18578, 1e-05, -7.90139, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 43000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00017, 0.0, -0.14765, 0.0, -0.0, 0.0, -1.38526, 0.0, -2.90538, 1e-05, -4.51045, 1e-05, -6.1768, 1e-05, -7.89171, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 43500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00015, 0.0, -0.1429, 0.0, -0.0, 0.0, -1.38101, 0.0, -2.89885, 1e-05, -4.50251, 1e-05, -6.16791, 1e-05, -7.88214, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 44000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00013, 0.0, -0.13831, 0.0, 0.0, 0.0, -1.3768, 0.0, -2.89239, 1e-05, -4.49465, 1e-05, -6.15912, 1e-05, -7.87267, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 44500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.00011, 0.0, -0.1339, 0.0, -0.0, 0.0, -1.37262, 0.0, -2.88597, 1e-05, -4.48687, 1e-05, -6.15041, 1e-05, -7.86331, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 45000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-9e-05, 0.0, -0.12964, 0.0, 0.0, 0.0, -1.36847, 0.0, -2.87962, 1e-05, -4.47916, 1e-05, -6.14179, 1e-05, -7.85404, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 45500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-8e-05, 0.0, -0.12553, 0.0, -0.0, 0.0, -1.36434, 0.0, -2.87332, 1e-05, -4.47152, 1e-05, -6.13325, 1e-05, -7.84486, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 46000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-7e-05, 0.0, -0.12157, 0.0, -0.0, 0.0, -1.36025, 0.0, -2.86707, 1e-05, -4.46395, 1e-05, -6.1248, 1e-05, -7.83578, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 46500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-6e-05, 0.0, -0.11775, 0.0, -0.0, 0.0, -1.35619, 0.0, -2.86087, 1e-05, -4.45645, 1e-05, -6.11643, 1e-05, -7.82679, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 47000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-5e-05, 0.0, -0.11407, 0.0, -0.0, 0.0, -1.35215, 0.0, -2.85473, 1e-05, -4.44902, 1e-05, -6.10814, 1e-05, -7.81789, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 47500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-5e-05, 0.0, -0.11051, 0.0, -0.0, 0.0, -1.34814, 0.0, -2.84863, 1e-05, -4.44165, 1e-05, -6.09993, 1e-05, -7.80908, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 48000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-4e-05, 0.0, -0.10707, 0.0, -0.0, 0.0, -1.34416, 0.0, -2.84259, 1e-05, -4.43436, 1e-05, -6.0918, 1e-05, -7.80036, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 48500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-3e-05, 0.0, -0.10376, 0.0, 0.0, 0.0, -1.34021, 0.0, -2.83659, 1e-05, -4.42712, 1e-05, -6.08374, 1e-05, -7.79172, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 49000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-3e-05, 0.0, -0.10055, 0.0, 0.0, 0.0, -1.33628, 0.0, -2.83064, 1e-05, -4.41995, 1e-05, -6.07576, 1e-05, -7.78317, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 49500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-3e-05, 0.0, -0.09746, 0.0, 0.0, 0.0, -1.33238, 0.0, -2.82474, 1e-05, -4.41285, 1e-05, -6.06786, 1e-05, -7.7747, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 50000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-2e-05, 0.0, -0.09447, 0.0, -0.0, -0.0, -1.32851, 0.0, -2.81889, 1e-05, -4.4058, 1e-05, -6.06002, 1e-05, -7.7663, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 50500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-2e-05, 0.0, -0.09157, 0.0, 0.0, 0.0, -1.32466, 0.0, -2.81308, 1e-05, -4.39882, 1e-05, -6.05226, 1e-05, -7.75799, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 51000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-2e-05, 0.0, -0.08878, 0.0, 0.0, 0.0, -1.32084, 0.0, -2.80732, 1e-05, -4.39189, 1e-05, -6.04456, 1e-05, -7.74975, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 51500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-2e-05, 0.0, -0.08608, 0.0, -0.0, 0.0, -1.31705, 0.0, -2.8016, 1e-05, -4.38502, 1e-05, -6.03694, 1e-05, -7.74159, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 52000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-2e-05, 0.0, -0.08346, 0.0, -0.0, 0.0, -1.31328, 0.0, -2.79593, 1e-05, -4.37821, 1e-05, -6.02938, 1e-05, -7.73351, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 52500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0, -0.08094, 0.0, 0.0, 0.0, -1.30953, 0.0, -2.7903, 1e-05, -4.37146, 1e-05, -6.02188, 1e-05, -7.72549, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 53000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0, -0.07849, 0.0, 0.0, 0.0, -1.30581, 0.0, -2.78471, 1e-05, -4.36476, 1e-05, -6.01446, 1e-05, -7.71755, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 53500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0, -0.07613, 0.0, 0.0, 0.0, -1.30212, 0.0, -2.77916, 1e-05, -4.35812, 1e-05, -6.00709, 1e-05, -7.70968, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 54000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0, -0.07384, 0.0, 0.0, 0.0, -1.29844, 0.0, -2.77366, 1e-05, -4.35153, 1e-05, -5.99979, 1e-05, -7.70188, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 54500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0, -0.07162, 0.0, 0.0, 0.0, -1.2948, 0.0, -2.7682, 1e-05, -4.34499, 1e-05, -5.99255, 1e-05, -7.69415, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 55000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0, -0.06948, 0.0, 0.0, 0.0, -1.29117, 0.0, -2.76278, 1e-05, -4.33851, 1e-05, -5.98537, 1e-05, -7.68648, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 55500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0, -0.0674, 0.0, 0.0, 0.0, -1.28757, 0.0, -2.75739, 1e-05, -4.33207, 1e-05, -5.97825, 1e-05, -7.67888, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 56000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0, -0.06539, 0.0, -0.0, -0.0, -1.28399, 0.0, -2.75205, 1e-05, -4.32569, 1e-05, -5.97119, 1e-05, -7.67135, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 56500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0, -0.06345, 0.0, -0.0, 0.0, -1.28043, 0.0, -2.74674, 1e-05, -4.31936, 1e-05, -5.96419, 1e-05, -7.66387, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 57000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0, -0.06156, 0.0, 0.0, 0.0, -1.2769, 0.0, -2.74148, 1e-05, -4.31307, 1e-05, -5.95724, 1e-05, -7.65647, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 57500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0, -0.05974, 0.0, -0.0, 0.0, -1.27339, 0.0, -2.73625, 1e-05, -4.30684, 1e-05, -5.95035, 1e-05, -7.64912, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 58000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0, -0.05797, 0.0, 0.0, 0.0, -1.2699, 0.0, -2.73106, 1e-05, -4.30065, 1e-05, -5.94351, 1e-05, -7.64183, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 58500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0, -0.05626, 0.0, 0.0, 0.0, -1.26643, 0.0, -2.7259, 1e-05, -4.29451, 1e-05, -5.93673, 1e-05, -7.6346, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 59000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0, -0.0546, 0.0, 0.0, 0.0, -1.26298, 0.0, -2.72078, 1e-05, -4.28841, 1e-05, -5.93, 1e-05, -7.62743, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 59500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0, -0.05299, 0.0, 0.0, 0.0, -1.25956, 0.0, -2.7157, 0.0, -4.28236, 1e-05, -5.92333, 1e-05, -7.62032, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 60000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-1e-05, 0.0, -0.05143, 0.0, -0.0, -0.0, -1.25615, 0.0, -2.71065, 0.0, -4.27636, 1e-05, -5.9167, 1e-05, -7.61327, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 60500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.04992, 0.0, 0.0, 0.0, -1.25277, 0.0, -2.70563, 0.0, -4.2704, 1e-05, -5.91013, 1e-05, -7.60627, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 61000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.04846, 0.0, -0.0, 0.0, -1.2494, 0.0, -2.70065, 0.0, -4.26448, 1e-05, -5.90361, 1e-05, -7.59933, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 61500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.04704, 0.0, -0.0, -0.0, -1.24606, 0.0, -2.69571, 0.0, -4.25861, 1e-05, -5.89714, 1e-05, -7.59244, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 62000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.04566, 0.0, 0.0, 0.0, -1.24274, 0.0, -2.69079, 0.0, -4.25278, 1e-05, -5.89071, 1e-05, -7.5856, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 62500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.04433, 0.0, -0.0, 0.0, -1.23943, 0.0, -2.68591, 0.0, -4.24699, 1e-05, -5.88433, 1e-05, -7.57882, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 63000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.04304, 0.0, -0.0, 0.0, -1.23615, 0.0, -2.68107, 0.0, -4.24124, 1e-05, -5.878, 1e-05, -7.57208, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 63500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.04179, 0.0, -0.0, 0.0, -1.23288, 0.0, -2.67625, 0.0, -4.23553, 1e-05, -5.87172, 1e-05, -7.5654, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 64000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.04057, 0.0, -0.0, 0.0, -1.22964, 0.0, -2.67147, 0.0, -4.22986, 1e-05, -5.86548, 1e-05, -7.55877, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 64500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.03939, 0.0, 0.0, 0.0, -1.22641, 0.0, -2.66672, 0.0, -4.22423, 1e-05, -5.85929, 1e-05, -7.55219, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 65000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.03825, 0.0, 0.0, 0.0, -1.2232, 0.0, -2.66199, 0.0, -4.21864, 1e-05, -5.85314, 1e-05, -7.54566, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 65500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.03714, 0.0, 0.0, 0.0, -1.22001, 0.0, -2.6573, 0.0, -4.21309, 1e-05, -5.84704, 1e-05, -7.53917, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 66000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.03607, 0.0, -0.0, 0.0, -1.21684, 0.0, -2.65264, 0.0, -4.20758, 1e-05, -5.84098, 1e-05, -7.53273, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 66500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.03502, 0.0, -0.0, 0.0, -1.21369, 0.0, -2.64801, 0.0, -4.2021, 1e-05, -5.83496, 1e-05, -7.52634, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 67000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.03401, 0.0, -0.0, 0.0, -1.21056, 0.0, -2.64341, 0.0, -4.19666, 1e-05, -5.82899, 1e-05, -7.52, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 67500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.03303, 0.0, -0.0, 0.0, -1.20744, 0.0, -2.63883, 0.0, -4.19126, 1e-05, -5.82305, 1e-05, -7.5137, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 68000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.03208, 0.0, -0.0, 0.0, -1.20434, 0.0, -2.63429, 0.0, -4.18589, 1e-05, -5.81716, 1e-05, -7.50744, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 68500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.03115, 0.0, -0.0, 0.0, -1.20126, 0.0, -2.62977, 0.0, -4.18056, 1e-05, -5.81131, 1e-05, -7.50123, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 69000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.03026, 0.0, 0.0, 0.0, -1.1982, 0.0, -2.62529, 0.0, -4.17526, 1e-05, -5.80549, 1e-05, -7.49506, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 69500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.02939, 0.0, -0.0, 0.0, -1.19515, 0.0, -2.62083, 0.0, -4.17, 1e-05, -5.79972, 1e-05, -7.48894, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 70000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.02854, 0.0, 0.0, 0.0, -1.19212, 0.0, -2.61639, 0.0, -4.16477, 1e-05, -5.79399, 1e-05, -7.48286, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 70500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.02772, 0.0, 0.0, 0.0, -1.18911, 0.0, -2.61199, 0.0, -4.15958, 1e-05, -5.78829, 1e-05, -7.47682, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 71000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.02693, 0.0, -0.0, 0.0, -1.18611, 0.0, -2.60761, 0.0, -4.15441, 1e-05, -5.78263, 1e-05, -7.47082, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 71500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.02615, 0.0, -0.0, 0.0, -1.18313, 0.0, -2.60325, 0.0, -4.14929, 1e-05, -5.77701, 1e-05, -7.46486, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 72000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.02541, 0.0, 0.0, 0.0, -1.18016, 0.0, -2.59893, 0.0, -4.14419, 1e-05, -5.77143, 1e-05, -7.45894, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 72500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.02468, 0.0, 0.0, 0.0, -1.17722, 0.0, -2.59463, 0.0, -4.13913, 1e-05, -5.76588, 1e-05, -7.45306, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 73000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.02397, 0.0, 0.0, 0.0, -1.17428, 0.0, -2.59035, 0.0, -4.13409, 0.0, -5.76037, 1e-05, -7.44722, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 73500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.02329, 0.0, 0.0, 0.0, -1.17137, 0.0, -2.5861, 0.0, -4.12909, 0.0, -5.75489, 1e-05, -7.44141, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 74000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.02262, 0.0, 0.0, 0.0, -1.16847, 0.0, -2.58187, 0.0, -4.12412, 0.0, -5.74945, 1e-05, -7.43565, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 74500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.02198, 0.0, -0.0, 0.0, -1.16558, 0.0, -2.57767, 0.0, -4.11918, 0.0, -5.74404, 1e-05, -7.42992, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 75000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.02135, 0.0, 0.0, 0.0, -1.16271, 0.0, -2.5735, 0.0, -4.11427, 0.0, -5.73867, 1e-05, -7.42423, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 75500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.02074, 0.0, -0.0, 0.0, -1.15986, 0.0, -2.56934, 0.0, -4.10939, 0.0, -5.73333, 1e-05, -7.41858, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 76000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.02015, 0.0, -0.0, -0.0, -1.15702, 0.0, -2.56521, 0.0, -4.10454, 0.0, -5.72802, 1e-05, -7.41296, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 76500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01958, 0.0, -0.0, -0.0, -1.1542, 0.0, -2.56111, 0.0, -4.09972, 0.0, -5.72275, 1e-05, -7.40738, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 77000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01902, 0.0, -0.0, 0.0, -1.15139, 0.0, -2.55703, 0.0, -4.09493, 0.0, -5.71751, 1e-05, -7.40183, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 77500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01848, 0.0, 0.0, 0.0, -1.14859, 0.0, -2.55297, 0.0, -4.09017, 0.0, -5.7123, 1e-05, -7.39632, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 78000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01795, 0.0, -0.0, 0.0, -1.14581, 0.0, -2.54893, 0.0, -4.08543, 0.0, -5.70712, 1e-05, -7.39084, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 78500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01744, 0.0, 0.0, 0.0, -1.14305, 0.0, -2.54492, 0.0, -4.08072, 0.0, -5.70198, 1e-05, -7.3854, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 79000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01695, 0.0, 0.0, 0.0, -1.1403, 0.0, -2.54093, 0.0, -4.07604, 0.0, -5.69686, 1e-05, -7.37999, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 79500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01647, 0.0, -0.0, 0.0, -1.13756, 0.0, -2.53696, 0.0, -4.07139, 0.0, -5.69178, 1e-05, -7.37461, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 80000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.016, 0.0, -0.0, -0.0, -1.13484, 0.0, -2.53301, 0.0, -4.06676, 0.0, -5.68672, 1e-05, -7.36926, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 80500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01554, 0.0, -0.0, 0.0, -1.13213, 0.0, -2.52909, 0.0, -4.06216, 0.0, -5.6817, 1e-05, -7.36395, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 81000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.0151, 0.0, -0.0, -0.0, -1.12943, 0.0, -2.52518, 0.0, -4.05759, 0.0, -5.67671, 0.0, -7.35867, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 81500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01468, 0.0, -0.0, 0.0, -1.12675, 0.0, -2.5213, 0.0, -4.05304, 0.0, -5.67174, 0.0, -7.35342, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 82000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01426, 0.0, 0.0, 0.0, -1.12408, 0.0, -2.51744, 0.0, -4.04852, 0.0, -5.6668, 0.0, -7.3482, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 82500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01386, 0.0, -0.0, 0.0, -1.12142, 0.0, -2.51359, 0.0, -4.04402, 0.0, -5.6619, 0.0, -7.34302, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 83000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01346, 0.0, 0.0, 0.0, -1.11878, 0.0, -2.50977, 0.0, -4.03955, 0.0, -5.65702, 0.0, -7.33786, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 83500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01308, 0.0, -0.0, 0.0, -1.11615, 0.0, -2.50597, 0.0, -4.0351, 0.0, -5.65216, 0.0, -7.33273, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 84000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01271, 0.0, 0.0, 0.0, -1.11354, 0.0, -2.50219, 0.0, -4.03068, 0.0, -5.64734, 0.0, -7.32763, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 84500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01235, 0.0, 0.0, 0.0, -1.11094, 0.0, -2.49843, 0.0, -4.02628, 0.0, -5.64254, 0.0, -7.32257, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 85000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.012, 0.0, -0.0, 0.0, -1.10835, 0.0, -2.49469, 0.0, -4.0219, 0.0, -5.63777, 0.0, -7.31753, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 85500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01167, 0.0, 0.0, 0.0, -1.10577, 0.0, -2.49097, 0.0, -4.01755, 0.0, -5.63303, 0.0, -7.31252, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 86000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01134, 0.0, -0.0, 0.0, -1.1032, 0.0, -2.48727, 0.0, -4.01323, 0.0, -5.62831, 0.0, -7.30753, 1e-05, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 86500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01102, 0.0, 0.0, 0.0, -1.10065, 0.0, -2.48358, 0.0, -4.00892, 0.0, -5.62362, 0.0, -7.30258, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 87000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01071, 0.0, 0.0, 0.0, -1.09811, 0.0, -2.47992, 0.0, -4.00464, 0.0, -5.61895, 0.0, -7.29765, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 87500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.0104, 0.0, -0.0, 0.0, -1.09558, 0.0, -2.47627, 0.0, -4.00038, 0.0, -5.61431, 0.0, -7.29276, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 88000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.01011, 0.0, 0.0, 0.0, -1.09307, 0.0, -2.47265, 0.0, -3.99615, 0.0, -5.6097, 0.0, -7.28788, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 88500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00982, 0.0, -0.0, 0.0, -1.09057, 0.0, -2.46904, 0.0, -3.99193, 0.0, -5.60511, 0.0, -7.28304, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 89000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00955, 0.0, -0.0, 0.0, -1.08807, 0.0, -2.46545, 0.0, -3.98774, 0.0, -5.60054, 0.0, -7.27822, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 89500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00928, 0.0, -0.0, 0.0, -1.0856, 0.0, -2.46188, 0.0, -3.98357, 0.0, -5.596, 0.0, -7.27343, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 90000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00902, 0.0, 0.0, 0.0, -1.08313, 0.0, -2.45832, 0.0, -3.97943, 0.0, -5.59149, 0.0, -7.26866, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 90500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00876, 0.0, -0.0, 0.0, -1.08067, 0.0, -2.45479, 0.0, -3.9753, 0.0, -5.587, 0.0, -7.26392, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 91000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00852, 0.0, 0.0, 0.0, -1.07823, 0.0, -2.45127, 0.0, -3.97119, 0.0, -5.58253, 0.0, -7.25921, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 91500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00828, 0.0, 0.0, 0.0, -1.07579, 0.0, -2.44777, 0.0, -3.96711, 0.0, -5.57808, 0.0, -7.25452, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 92000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00804, 0.0, 0.0, 0.0, -1.07337, 0.0, -2.44428, 0.0, -3.96305, 0.0, -5.57366, 0.0, -7.24985, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 92500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00782, 0.0, 0.0, 0.0, -1.07096, 0.0, -2.44081, 0.0, -3.959, 0.0, -5.56926, 0.0, -7.24521, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 93000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.0076, 0.0, 0.0, 0.0, -1.06856, 0.0, -2.43736, 0.0, -3.95498, 0.0, -5.56488, 0.0, -7.2406, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 93500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00738, 0.0, -0.0, 0.0, -1.06617, 0.0, -2.43393, 0.0, -3.95098, 0.0, -5.56053, 0.0, -7.23601, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 94000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00718, 0.0, 0.0, 0.0, -1.0638, 0.0, -2.43051, 0.0, -3.947, 0.0, -5.5562, 0.0, -7.23144, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 94500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00697, 0.0, -0.0, 0.0, -1.06143, 0.0, -2.42711, 0.0, -3.94304, 0.0, -5.55189, 0.0, -7.2269, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 95000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00678, 0.0, -0.0, -0.0, -1.05908, 0.0, -2.42373, 0.0, -3.93909, 0.0, -5.5476, 0.0, -7.22237, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 95500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00659, 0.0, -0.0, -0.0, -1.05673, 0.0, -2.42036, 0.0, -3.93517, 0.0, -5.54333, 0.0, -7.21788, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 96000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.0064, 0.0, -0.0, 0.0, -1.0544, 0.0, -2.417, 0.0, -3.93127, 0.0, -5.53909, 0.0, -7.2134, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 96500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00622, 0.0, 0.0, 0.0, -1.05207, 0.0, -2.41367, 0.0, -3.92738, 0.0, -5.53486, 0.0, -7.20895, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 97000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00605, 0.0, 0.0, 0.0, -1.04976, 0.0, -2.41035, 0.0, -3.92351, 0.0, -5.53066, 0.0, -7.20452, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 97500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00588, 0.0, 0.0, 0.0, -1.04746, 0.0, -2.40704, 0.0, -3.91967, 0.0, -5.52648, 0.0, -7.20011, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 98000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00571, 0.0, 0.0, 0.0, -1.04517, 0.0, -2.40375, 0.0, -3.91584, 0.0, -5.52232, 0.0, -7.19573, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 98500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00555, 0.0, 0.0, 0.0, -1.04288, 0.0, -2.40047, 0.0, -3.91203, 0.0, -5.51817, 0.0, -7.19136, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 99000
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.0054, 0.0, -0.0, 0.0, -1.04061, 0.0, -2.39721, 0.0, -3.90823, 0.0, -5.51405, 0.0, -7.18702, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 99500
Discounting jh with old adv calculation: [-0.04613, 1e-05, -0.05546, 0.0, -0.06487, 0.0, -0.07438, 0.0, -0.08399, 0.0, -0.09369, 0.0, -0.10349, 0.0, -0.11339, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [1 1 1 1 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, -0.00525, 0.0, -0.0, -0.0, -1.03835, 0.0, -2.39397, 0.0, -3.90446, 0.0, -5.50995, 0.0, -7.1827, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 1 1 1 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 0
Discounting jh with old adv calculation: [0.0082, -0.0082, 0.00772, -0.00772, 0.00674, -0.00674, 0.00475, -0.00475, 0.00075, -0.00075, -0.00734, 0.00734, -0.02368, 0.02368, -0.0567, 0.0567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 0 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0082, -0.0082, 0.00772, -0.00772, 0.00674, -0.00674, 0.00475, -0.00475, 0.00075, -0.00075, -0.00734, 0.00734, -0.02368, 0.02368, -0.0567, 0.0567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 0 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 1000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 1500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 2000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 2500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 3000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 3500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 4000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 4500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 5000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 5500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 6000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 6500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 7000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 7500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 8000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 8500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 9000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 9500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 10000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 10500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 11000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 11500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 12000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 12500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 13000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 13500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 14000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 14500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 15000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 15500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 16000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 16500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 17000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 17500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 18000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 18500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 19000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 19500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 20000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 20500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 21000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 21500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 22000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 22500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 23000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 23500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 24000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 24500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 25000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 25500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 26000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 26500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 27000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 27500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 28000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 28500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 29000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 29500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 30000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 30500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 31000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 31500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 32000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 32500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 33000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 33500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 34000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 34500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 35000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 35500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 36000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 36500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 37000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 37500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 38000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 38500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 39000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 39500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 40000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 40500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 41000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 41500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 42000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 42500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 43000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 43500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 44000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 44500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 45000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 45500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 46000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 46500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 47000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 47500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 48000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 48500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 49000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 49500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 50000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 50500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 51000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 51500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 52000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 52500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 53000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 53500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 54000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 54500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 55000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 55500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 56000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 56500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 57000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 57500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 58000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 58500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 59000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 59500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 60000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 60500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 61000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 61500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 62000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 62500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 63000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 63500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 64000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 64500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 65000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 65500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 66000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 66500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 67000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 67500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 68000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 68500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 69000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 69500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 70000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 70500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 71000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 71500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 72000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 72500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 73000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 73500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 74000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 74500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 75000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 75500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 76000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 76500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 77000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 77500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 78000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 78500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 79000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 79500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 80000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 80500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 81000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 81500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 82000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 82500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 83000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 83500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 84000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 84500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 85000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 85500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 86000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 86500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 87000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 87500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 88000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 88500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 89000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 89500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 90000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 90500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 91000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 91500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 92000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 92500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 93000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 93500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 94000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 94500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 95000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 95500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 96000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 96500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 97000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 97500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 98000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 98500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 99000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 99500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 0
Discounting jh with old adv calculation: [0.0082, -0.0082, 0.00772, -0.00772, 0.00674, -0.00674, 0.00475, -0.00475, 0.00075, -0.00075, -0.00734, 0.00734, -0.02368, 0.02368, -0.0567, 0.0567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 0 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0082, -0.0082, 0.00772, -0.00772, 0.00674, -0.00674, 0.00475, -0.00475, 0.00075, -0.00075, -0.00734, 0.00734, -0.02368, 0.02368, -0.0567, 0.0567, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 0 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 1000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 1500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 2000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 2500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 3000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 3500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 4000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 4500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 5000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 5500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 6000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 6500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 7000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 7500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 8000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [1 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [False  True  True  True  True  True  True  True  True  True]
Iteration 8500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 9000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 9500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 10000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 10500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 11000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 11500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 12000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 12500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 13000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 13500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 14000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 14500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 15000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 15500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 16000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 16500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 17000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 17500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 18000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 18500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 19000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 19500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 20000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 20500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 21000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 21500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 22000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 22500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 23000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 23500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 24000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 24500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 25000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 25500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 26000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 26500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 27000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 27500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 28000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 28500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 29000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 29500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 30000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 30500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 31000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 31500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 32000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 32500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 33000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 33500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 34000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 34500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 35000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 35500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 36000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 36500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 37000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 37500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 38000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 38500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 39000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 39500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 40000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 40500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 41000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 41500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 42000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 42500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 43000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 43500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 44000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 44500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 45000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 45500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 46000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 46500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 47000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 47500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 48000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 48500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 49000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 49500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 50000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 50500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 51000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 51500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 52000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 52500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 53000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 53500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 54000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 54500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 55000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 55500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 56000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 56500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 57000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 57500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 58000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 58500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 59000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 59500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 60000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 60500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 61000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 61500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 62000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 62500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 63000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 63500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 64000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 64500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 65000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 65500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 66000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 66500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 67000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 67500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 68000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 68500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 69000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 69500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 70000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 70500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 71000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 71500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 72000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 72500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 73000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 73500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 74000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 74500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 75000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 75500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 76000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 76500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 77000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 77500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 78000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 78500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 79000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 79500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 80000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 80500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 81000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 81500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 82000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 82500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 83000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 83500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 84000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 84500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 85000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 85500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 86000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 86500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 87000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 87500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 88000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 88500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 89000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 89500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 90000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 90500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 91000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 91500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 92000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 92500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 93000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 93500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 94000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 94500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 95000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 95500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 96000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 96500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 97000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 97500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 98000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 98500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 99000
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
Iteration 99500
Discounting jh with old adv calculation: [0.00781, -0.00817, 0.00728, -0.00759, 0.00617, -0.00639, 0.00383, -0.00391, -0.00104, 0.00103, -0.01095, 0.01031, -0.03033, 0.0259, -0.06565, 0.04774, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with old adv calculation: [0 0 0 0 1 1 1 1 0 0]
Discounting jh with new adv calculation: [0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
The better action at a given state with new adv calculation: [0 0 0 0 1 1 1 1 0 0]
Is better action same for both adv calculation: [ True  True  True  True  True  True  True  True  True  True]
