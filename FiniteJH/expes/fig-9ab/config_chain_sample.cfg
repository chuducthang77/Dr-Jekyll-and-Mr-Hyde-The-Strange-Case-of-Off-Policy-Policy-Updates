######### MDP parameters ###########
# Environment for the experiment:
# chain: chain of n states where the maximal reward is at the end, but other actions are misleadingly guiding the gradient to end the trajectory at the beginning
# garnets: random MDP 
environment: 'garnets'

# number of states in the MDP
nb_states: 100

# number of actions in the MDP
nb_actions: 4

# discount factor
gamma: 0.99

### Used only in the chain experiment ###
# ratio of performance between the low-hanging-fruit policy and the optimal one.
vr: 0.95

# Transition stochasticty parameter
stochasticity: 1

### Used only in the garnets experiment ###
# determines how many different states are reachable after perform a given action in a given state
connectivity: 2

# determines whether we want the goal state to be the hardest state to reach
farthest: True

####################################

######### Experiment parameters ###########
# random seed (not used if non positive)
seed: -1

# setting of the experiment:
# exact = P and R are assumed to be known so that we can compute exact q and d
# sample = the estimates are to be constructed from collected samples
setting: 'sample'

# Number of trajectories (samples) or updates (exact):
max_nb_it: 100000

# Number of runs:
nb_expes: 100

############################################

########## Algorithm parameters ############
# parametrization: parametrization of the policy [softmax, direct]
# discounting: it is the density/discounting/exploration algorithm [jh, theory, practice]
# critic_type: defines how the critic is updated [sarsa, qlearning, mle]
# actor_stepsize: actor stepsize [any positive number]
# critic_stepsize: critic stepsize (useless with mle) [any positive number]
# init_q: critic initialization (useless with mle) [any positive number, negative number yields random initialization]
# hyde_param: parameter factor for the probability of selecting Hyde's policy (only used with jh) [any positive number]
# alpha: decaying power of the Hyde policy selection (only used with jh) [any positive number]
# lambada (cannot use lambda ;-)) is the parameter for entropic regularization [any positive number]
# critic_UCB_stepsize: UCB critic stepsize (useless with mle and jh) [any positive number]

algos:
    - parametrization: 'softmax'
      discounting: 'jh'
      critic_type: 'sample-replay'
      actor_stepsize: 1
      critic_stepsize: 0.1
      init_q: 0
      hyde_param: 100
      alpha: 1
      lambada: 0
      critic_UCB_stepsize: 1
      name: 'J\&H $\epsilon_t=100/t$ $d_{t}=0.5$'
      minibatch_size: 4
      offpol_param: 0.5
      offpol_alpha: 0
    - parametrization: 'softmax'
      discounting: 'jh'
      critic_type: 'sample-replay'
      actor_stepsize: 1
      critic_stepsize: 0.1
      init_q: 0
      hyde_param: 10
      alpha: 0.5
      lambada: 0
      critic_UCB_stepsize: 1
      name: 'J\&H $\epsilon_t=10/\sqrt{t}$ $d_{t}=0.5$'
      minibatch_size: 4
      offpol_param: 0.5
      offpol_alpha: 0
    - parametrization: 'softmax'
      discounting: 'jh'
      critic_type: 'sample-replay'
      actor_stepsize: 1
      critic_stepsize: 0.1
      init_q: 0
      hyde_param: 100
      alpha: 1
      lambada: 0
      critic_UCB_stepsize: 1
      name: 'J\&H $\epsilon_t=100/t$ $d_{t}=10/\sqrt{t}$'
      minibatch_size: 4
      offpol_param: 10
      offpol_alpha: 0.5
    - parametrization: 'softmax'
      discounting: 'jh'
      critic_type: 'sample-replay'
      actor_stepsize: 1
      critic_stepsize: 0.1
      init_q: 0
      hyde_param: 10
      alpha: 0.5
      lambada: 0
      critic_UCB_stepsize: 1
      name: 'J\&H $\epsilon_t=10/\sqrt{t}$ $d_{t}=10/\sqrt{t}$'
      minibatch_size: 4
      offpol_param: 10
      offpol_alpha: 0.5
    - parametrization: 'softmax'
      discounting: 'theory'
      critic_type: 'sample-replay'
      actor_stepsize: 1
      critic_stepsize: 0.1
      init_q: 0
      hyde_param: 10
      alpha: 1
      lambada: 0
      critic_UCB_stepsize: 1
      name: 'theory'
      minibatch_size: 4
      offpol_param: 0
      offpol_alpha: 0
    - parametrization: 'softmax'
      discounting: 'theory'
      critic_type: 'sample-replay'
      actor_stepsize: 1
      critic_stepsize: 0.1
      init_q: 0
      hyde_param: 10
      alpha: 1
      lambada: 0.01
      critic_UCB_stepsize: 1
      name: 'theory + entropy'
      minibatch_size: 4
      offpol_param: 0
      offpol_alpha: 0
    - parametrization: 'softmax'
      discounting: 'practice'
      critic_type: 'sample-replay'
      actor_stepsize: 1
      critic_stepsize: 0.1
      init_q: 0
      hyde_param: 10
      alpha: 1
      lambada: 0
      critic_UCB_stepsize: 1
      name: 'practice'
      minibatch_size: 4
      offpol_param: 0
      offpol_alpha: 0
    - parametrization: 'softmax'
      discounting: 'practice'
      critic_type: 'sample-replay'
      actor_stepsize: 1
      critic_stepsize: 0.1
      init_q: 0
      hyde_param: 10
      alpha: 1
      lambada: 0.01
      critic_UCB_stepsize: 1
      name: 'practice + entropy'
      minibatch_size: 4
      offpol_param: 0
      offpol_alpha: 0
